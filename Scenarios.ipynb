{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "# Normal Imports\n",
    "import pyspark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import col,struct,when, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "print(\"imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For references use - https://sparkbyexamples.com/pyspark-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark context\n",
    "conf = SparkConf().setAppName(\"RowSpark\").setMaster(\"local[*]\")\\\n",
    "            .set(\"spark.driver.allowMultipleContexts\",\"true\")\n",
    "sc= SparkContext.getOrCreate(conf = conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# spark session initialization\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "            .appName(\"Scenarios\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mother and Father Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|gender|\n",
      "+---+------+------+\n",
      "|101|  riya|     F|\n",
      "|566|  Aman|     M|\n",
      "|202|rakesh|     F|\n",
      "|875| lucky|     M|\n",
      "|202| reena|     M|\n",
      "|322| raina|     M|\n",
      "|345| Rohit|     F|\n",
      "|322| Mohit|     F|\n",
      "|345| Meena|     M|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "p = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/p.csv\")\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|cid|pid|\n",
      "+---+---+\n",
      "|101|202|\n",
      "|566|322|\n",
      "|875|345|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/r.csv\")\n",
    "r.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|cid|pid| name|\n",
      "+---+---+-----+\n",
      "|101|202| riya|\n",
      "|566|322| Aman|\n",
      "|875|345|lucky|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftjoin = r.join(p, r.cid == p.id, \"left\").select(\"cid\", \"pid\", \"name\")\n",
    "leftjoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|  name|gender|\n",
      "+---+------+------+\n",
      "|202|rakesh|     F|\n",
      "|202| reena|     M|\n",
      "|322| raina|     M|\n",
      "|345| Rohit|     F|\n",
      "|322| Mohit|     F|\n",
      "|345| Meena|     M|\n",
      "+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftantijoin = p.join(r, r.cid == p.id, \"left_anti\")\n",
    "leftantijoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|mothers|gender|\n",
      "+---+-------+------+\n",
      "|202|  reena|     M|\n",
      "|322|  raina|     M|\n",
      "|345|  Meena|     M|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mothers = leftantijoin.filter(leftantijoin.gender == \"M\").withColumnRenamed(\"name\", \"mothers\")\n",
    "mothers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|fathers|gender|\n",
      "+---+-------+------+\n",
      "|202| rakesh|     F|\n",
      "|345|  Rohit|     F|\n",
      "|322|  Mohit|     F|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fathers = leftantijoin.filter(leftantijoin.gender == \"F\").withColumnRenamed(\"name\", \"fathers\")\n",
    "fathers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+---+-------+------+---+-------+------+\n",
      "|cid|pid| name| id|mothers|gender| id|fathers|gender|\n",
      "+---+---+-----+---+-------+------+---+-------+------+\n",
      "|101|202| riya|202|  reena|     M|202| rakesh|     F|\n",
      "|566|322| Aman|322|  raina|     M|322|  Mohit|     F|\n",
      "|875|345|lucky|345|  Meena|     M|345|  Rohit|     F|\n",
      "+---+---+-----+---+-------+------+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftmothers = leftjoin.join(mothers, leftjoin.pid == mothers.id, \"left\")\n",
    "leftfathers = leftmothers.join(fathers, leftmothers.pid == fathers.id, \"left\")\n",
    "\n",
    "leftfathers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "| name|mothers|fathers|\n",
      "+-----+-------+-------+\n",
      "| riya|  reena| rakesh|\n",
      "| Aman|  raina|  Mohit|\n",
      "|lucky|  Meena|  Rohit|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf = leftfathers.select(\"name\", \"mothers\", \"fathers\")\n",
    "finaldf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 34 Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sort after _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+\n",
      "|       name|sal|\n",
      "+-----------+---+\n",
      "|sree_ramesh|100|\n",
      "| chiran_tan|200|\n",
      "|  ram_krish|300|\n",
      "|  john_stan|400|\n",
      "|   mar_jany|200|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortdata = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/sortusing.csv\")\n",
    "sortdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+--------+\n",
      "|       name|sal|lastname|\n",
      "+-----------+---+--------+\n",
      "|   mar_jany|200|   _jany|\n",
      "|  ram_krish|300|  _krish|\n",
      "|sree_ramesh|100| _ramesh|\n",
      "|  john_stan|400|   _stan|\n",
      "| chiran_tan|200|    _tan|\n",
      "+-----------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitdata = sortdata.withColumn(\"lastname\", expr(\"split(name, '_')[1]\")).sort(col(\"lastname\").asc())\n",
    "# splitdata = sortdata.withColumn(\"lastname\", expr(\"split(name, '_')[1]\")).sort(col(\"lastname\").asc()).drop(col(\"lastname\"))\n",
    "splitdata = sortdata.withColumn(\"lastname\", expr(\"concat('_', split(name, '_')[1])\")).sort(col(\"lastname\").asc())\n",
    "\n",
    "splitdata.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### olderemployeeselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+---------+\n",
      "|emp_id|dept_id|sal|hire_date|\n",
      "+------+-------+---+---------+\n",
      "|    10|     10|600|10-Jan-19|\n",
      "|    20|     10|200|10-Jun-19|\n",
      "|    30|     20|300|20-Jan-20|\n",
      "|    40|     30|400|30-Jun-20|\n",
      "+------+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortdata = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/olderemployeeselect.csv\")\n",
    "sortdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+---------+----------+\n",
      "|emp_id|dept_id|sal|hire_date|row_number|\n",
      "+------+-------+---+---------+----------+\n",
      "|    20|     10|200|10-Jun-19|         1|\n",
      "|    10|     10|600|10-Jan-19|         2|\n",
      "|    30|     20|300|20-Jan-20|         1|\n",
      "|    40|     30|400|30-Jun-20|         1|\n",
      "+------+-------+---+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowfunc = Window.partitionBy(col(\"dept_id\")).orderBy(col(\"hire_date\").desc())\n",
    "\n",
    "hiredate = sortdata.withColumn(\"row_number\", row_number().over(windowfunc))\n",
    "hiredate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---+---------+\n",
      "|emp_id|dept_id|sal|hire_date|\n",
      "+------+-------+---+---------+\n",
      "|    20|     10|200|10-Jun-19|\n",
      "|    30|     20|300|20-Jan-20|\n",
      "|    40|     30|400|30-Jun-20|\n",
      "+------+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hirefinal = hiredate.filter(hiredate.row_number == 1).select(\"emp_id\", \"dept_id\", \"sal\", \"hire_date\")\n",
    "hirefinal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reporting manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+-------+\n",
      "| id| Name|Dept|Manager|\n",
      "+---+-----+----+-------+\n",
      "|101| John|   A|   null|\n",
      "|102|  Dan|   A|    101|\n",
      "|103|James|   A|    101|\n",
      "|104|  Amy|   A|    101|\n",
      "|105| Anne|   A|    101|\n",
      "|106|  Ron|   B|    101|\n",
      "+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager_reporting = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/manager_reporting.csv\")\n",
    "manager_reporting.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sql\n",
    "dataset = manager_reporting.createOrReplaceTempView(\"managertable\")\n",
    "managerdf = spark.sql(\"\"\"select Name \n",
    "                        from managertable \n",
    "                        where id in (select Manager \n",
    "                        from managertable \n",
    "                        group by Manager \n",
    "                        having count(*)>=5)\n",
    "                        \"\"\")\n",
    "managerdf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|mgid|count|\n",
      "+----+-----+\n",
      "| 101|    5|\n",
      "|null|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DSL\n",
    "\n",
    "dsl_data = manager_reporting.groupBy(col(\"Manager\").alias(\"mgid\")).count()\n",
    "dsl_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|name|\n",
      "+----+\n",
      "|John|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtereddata = dsl_data.filter(col(\"count\") >= 5)\n",
    "filtereddata.join(manager_reporting, col(\"mgid\") == col(\"id\"), \"left\").select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manager_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|EMPID|EMNAME|MNGID|\n",
      "+-----+------+-----+\n",
      "|  101|  Mary|  102|\n",
      "|  102|  Ravi| NULL|\n",
      "|  103|   Raj|  102|\n",
      "|  104|  Pete|  103|\n",
      "|  105|Prasad|  103|\n",
      "|  106|   Ben|  103|\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager_details = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/manager_details.csv\")\n",
    "manager_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|102|\n",
      "|103|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mngid = manager_details.filter(col(\"mngid\") != \"NULL\").select(\"mngid\").withColumnRenamed(\"mngid\", \"id\").distinct()\n",
    "mngid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|EMPID|EMNAME|\n",
      "+-----+------+\n",
      "|  102|  Ravi|\n",
      "|  103|   Raj|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = mngid.join(manager_details, col(\"empid\") == col(\"id\"), \"left\").select(\"EMPID\", \"EMNAME\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace_underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+\n",
      "|Emp Id|   Emp Name|Dept Name|\n",
      "+------+-----------+---------+\n",
      "|   101|Alice james|    Sales|\n",
      "|   102|   Bob jack|    Sales|\n",
      "+------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "replace_underscore = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/replace_underscore.csv\")\n",
    "replace_underscore.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+-----------+\n",
      "|Emp Id|   Emp Name|Dept Name| Emp Name 1|\n",
      "+------+-----------+---------+-----------+\n",
      "|   101|Alice james|    Sales|Alice_james|\n",
      "|   102|   Bob jack|    Sales|   Bob_jack|\n",
      "+------+-----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the column data, space with underscore\n",
    "replace_underscore.withColumn(\"Emp Name 1\", regexp_replace(\"Emp Name\", \" \", \"_\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+---------+\n",
      "|Emp Id|   Emp Name|Dept Name|\n",
      "+------+-----------+---------+\n",
      "|   101|Alice james|    Sales|\n",
      "|   102|   Bob jack|    Sales|\n",
      "+------+-----------+---------+\n",
      "\n",
      "+------+-----------+---------+\n",
      "|Emp_Id|   Emp_Name|Dept_Name|\n",
      "+------+-----------+---------+\n",
      "|   101|Alice james|    Sales|\n",
      "|   102|   Bob jack|    Sales|\n",
      "+------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace the column header space with underscore\n",
    "replace_underscore1 = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/replace_underscore.csv\")\n",
    "replace_underscore1.show()\n",
    "\n",
    "replace_underscore1.withColumnRenamed(\"Emp Id\", \"Emp_Id\")\\\n",
    "                    .withColumnRenamed(\"Emp Name\", \"Emp_Name\")\\\n",
    "                    .withColumnRenamed(\"Dept Name\", \"Dept_Name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Range|Number|\n",
      "+-----+------+\n",
      "|   90|     2|\n",
      "|   60|     3|\n",
      "|   70|     5|\n",
      "|   80|     1|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "addscore = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/addscore.csv\")\n",
    "addscore.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+\n",
      "|Range|Number|lead|\n",
      "+-----+------+----+\n",
      "|   90|     2|null|\n",
      "|   80|     1|   2|\n",
      "|   70|     5|   1|\n",
      "|   60|     3|   5|\n",
      "+-----+------+----+\n",
      "\n",
      "+-----+------+----+\n",
      "|Range|Number|lead|\n",
      "+-----+------+----+\n",
      "|   90|     2|null|\n",
      "|   80|     3|   2|\n",
      "|   70|     6|   1|\n",
      "|   60|     8|   5|\n",
      "+-----+------+----+\n",
      "\n",
      "+-----+------+\n",
      "|Range|Number|\n",
      "+-----+------+\n",
      "|   90|     2|\n",
      "|   80|     3|\n",
      "|   70|     8|\n",
      "|   60|    11|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowspec = Window.orderBy(addscore.Range.desc())\n",
    "leadcol = addscore.withColumn(\"lead\", lag(col(\"Number\"),1).over(windowspec))\n",
    "leadcol.show()\n",
    "\n",
    "leadcol.withColumn(\"Number\", expr(\"case when lead is null then number else number+lead end\").cast(\"Integer\")).show()\n",
    "addscore.withColumn(\"Number\", expr(\"sum(Number) over (order by Range desc rows unbounded preceding)\").cast(\"Integer\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list split\n",
    "Input -> my_list = ['abc', 'for', 'abc', 'like','geek1','nerdy', 'xyz', 'love','questions','words', 'life']\n",
    " \n",
    "Output -> [['abc', 'for', 'abc', 'like', 'geek1'], ['nerdy', 'xyz', 'love', 'questions', 'words'], ['life']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['abc', 'for', 'abc', 'like', 'geek1'], ['nerdy', 'xyz', 'love', 'questions', 'words'], ['life']]\n"
     ]
    }
   ],
   "source": [
    "my_list = ['abc', 'for', 'abc', 'like','geek1','nerdy', 'xyz', 'love','questions','words', 'life']\n",
    "\n",
    "# using python\n",
    "output = []\n",
    "length = 0\n",
    "for i in range((len(my_list)//5)+1):\n",
    "    temp = []\n",
    "#     print(i, end=\" \")s\n",
    "    if not length >len(my_list):\n",
    "        output.append(my_list[length : length+5])\n",
    "        length = length+5\n",
    "        \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Highest salary in department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|department|assetValue|\n",
      "+----------+----------+\n",
      "|     DEPT1|      1000|\n",
      "|     DEPT1|       500|\n",
      "|     DEPT1|       700|\n",
      "|     DEPT2|       400|\n",
      "|     DEPT2|       200|\n",
      "|     DEPT3|       500|\n",
      "|     DEPT3|       200|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = [\"department\", \"assetValue\"]\n",
    "dept = [(\"DEPT1\", 1000), (\"DEPT1\", 500), (\"DEPT1\", 700), (\"DEPT2\", 400), (\"DEPT2\", 200),  (\"DEPT3\", 500), (\"DEPT3\", 200)]\n",
    "# rdd = spark.sparkContext.parallelize(dept)\n",
    "# rddf = rdd.toDF(df).show()\n",
    "\n",
    "rdd = spark.createDataFrame(dept, schema=df)\n",
    "rdd.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|department|assetValue|col3|\n",
      "+----------+----------+----+\n",
      "|     DEPT1|      1000|   1|\n",
      "|     DEPT1|       700|   2|\n",
      "|     DEPT1|       500|   3|\n",
      "|     DEPT2|       400|   1|\n",
      "|     DEPT2|       200|   2|\n",
      "|     DEPT3|       500|   1|\n",
      "|     DEPT3|       200|   2|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "win = Window.partitionBy(col(\"department\")).orderBy(col(\"assetValue\").desc())\n",
    "windowfunc = rdd.withColumn(\"col3\", dense_rank().over(win))\n",
    "windowfunc.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|department|assetValue|col3|\n",
      "+----------+----------+----+\n",
      "|     DEPT1|       700|   2|\n",
      "|     DEPT2|       200|   2|\n",
      "|     DEPT3|       200|   2|\n",
      "+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowfunc.filter(col(\"col3\") == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List\n",
    "A1 = [1,2,3]\n",
    "\n",
    "A2 = [2,3,4]\n",
    "\n",
    "output = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n",
      "[1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "A1 = [1,2,3]\n",
    "A2 = [2,3,4]\n",
    "\n",
    "output = []\n",
    "\n",
    "# using list extend method\n",
    "A1.extend(A2)\n",
    "for i in A1:\n",
    "    if i not in output:\n",
    "        output.append(i)\n",
    "        \n",
    "print(output)\n",
    "\n",
    "# using set method\n",
    "\n",
    "output = list(set(A1))\n",
    "print(output)\n",
    "\n",
    "\n",
    "# using zip() function\n",
    "# both list should be same size, when we are using zip function\n",
    "for i, j in zip(A1, A2):\n",
    "#     print(i, j)\n",
    "    if i not in output:\n",
    "        output.append(i)\n",
    "        \n",
    "    if j not in output:\n",
    "        output.append(j)\n",
    "        \n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### name table, and Salary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Henry|\n",
      "|  2|Smith|\n",
      "|  3| Hall|\n",
      "+---+-----+\n",
      "\n",
      "+---+------+\n",
      "|sid|salary|\n",
      "+---+------+\n",
      "|  1|   100|\n",
      "|  2|   500|\n",
      "|  4|  1000|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nametable = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/nametable.csv\")\n",
    "nametable.show()\n",
    "\n",
    "salarytable = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/salarytable.csv\").selectExpr(\"id as sid\", \"salary\")\n",
    "salarytable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|Henry|   100|\n",
      "|  2|Smith|   500|\n",
      "|  3| Hall|  null|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftjoin = nametable.join(salarytable, col(\"id\") == col(\"sid\"), \"left\").drop(col(\"sid\"))\n",
    "leftjoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|salary|\n",
      "+---+-----+------+\n",
      "|  1|Henry|   100|\n",
      "|  2|Smith|   500|\n",
      "|  3| Hall|     0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaltable = leftjoin.withColumn(\"salary\", expr(\"case when salary is not null then salary else 0 End\"))\n",
    "finaltable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### array_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|  Spark|\n",
      "|  1|  Scala|\n",
      "|  1|   Hive|\n",
      "|  2|  Scala|\n",
      "|  3|  Spark|\n",
      "|  3|  Scala|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_merge = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/array_merge.csv\")\n",
    "array_merge.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------+\n",
      "| id|collect_list(subject)|\n",
      "+---+---------------------+\n",
      "|  1| [Spark, Scala, Hive]|\n",
      "|  2|              [Scala]|\n",
      "|  3|       [Spark, Scala]|\n",
      "+---+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use collect_list to make array along with group_by\n",
    "\n",
    "# array_merge.groupby(\"id\").agg(collect_list(\"subject\")).sort(col(\"id\").desc()).show()\n",
    "# array_merge.groupby(\"id\").agg(collect_list(\"subject\")).show()\n",
    "madelist = array_merge.groupby(\"id\").agg(collect_list(\"subject\")).sort(col(\"id\"))\n",
    "madelist.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pivot marks table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| Id|subject|marks|\n",
      "+---+-------+-----+\n",
      "|101|    Eng|   90|\n",
      "|101|    Sci|   80|\n",
      "|101|    Mat|   95|\n",
      "|102|    Eng|   75|\n",
      "|102|    Sci|   85|\n",
      "|102|    Mat|   90|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_table = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/pivot_table.csv\")\n",
    "pivot_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| id|Eng|Mat|Sci|\n",
      "+---+---+---+---+\n",
      "|101| 90| 95| 80|\n",
      "|102| 75| 90| 85|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_table = pivot_table.withColumn(\"marks\", col(\"marks\").cast(\"Integer\"))\n",
    "pivot_sub = pivot_table.groupBy(\"id\").pivot(\"subject\").sum(\"marks\")\n",
    "pivot_sub.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take number only to list Scala code\n",
    "\n",
    "val mylist = List(10,5,24,'Hi',90,12,'Hello')\n",
    "\n",
    "mylist.collect{\n",
    "\n",
    "    case num : Int => println(num)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 5 24 90 12 "
     ]
    }
   ],
   "source": [
    "# Take number only to list python code\n",
    "\n",
    "aplhalist = [10,5,24,'Hi',90,12,'Hello']\n",
    "\n",
    "for i in aplhalist:\n",
    "    if isinstance(i, int):\n",
    "        print(i, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### email_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------------------+\n",
      "| id|  name|               email|\n",
      "+---+------+--------------------+\n",
      "|  1| Henry|   henry12@gmail.com|\n",
      "|  2| Smith|     smith@yahoo.com|\n",
      "|  3|Martin|martin221@hotmail...|\n",
      "+---+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_split = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/email_split.csv\")\n",
    "email_split.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----------+\n",
      "| id|  name|     domain|\n",
      "+---+------+-----------+\n",
      "|  1| Henry|  gmail.com|\n",
      "|  2| Smith|  yahoo.com|\n",
      "|  3|Martin|hotmail.com|\n",
      "+---+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_df = email_split.withColumn(\"domain\", split(\"email\", \"@\")[1])\n",
    "email_df.select(\"id\", \"name\", \"domain\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### student table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-----+\n",
      "|student_name|subject|marks|\n",
      "+------------+-------+-----+\n",
      "|           a|english|   99|\n",
      "|           b|english|   99|\n",
      "|           b|  maths|   45|\n",
      "|           c|science|   35|\n",
      "|           c|english|   98|\n",
      "+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_table = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/student table.csv\")\n",
    "student_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.2\n",
      "+------------+-------+-----+\n",
      "|student_name|subject|marks|\n",
      "+------------+-------+-----+\n",
      "|           a|english|   99|\n",
      "|           b|english|   99|\n",
      "|           c|english|   98|\n",
      "+------------+-------+-----+\n",
      "\n",
      "+------------+-------+-----+\n",
      "|student_name|subject|marks|\n",
      "+------------+-------+-----+\n",
      "|           a|english|   99|\n",
      "|           b|english|   99|\n",
      "|           c|english|   98|\n",
      "+------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using collect method\n",
    "averagemarks = student_table.selectExpr(\"avg(marks)\").collect()[0][0]\n",
    "print(averagemarks)\n",
    "student_table.filter(student_table.marks > averagemarks).show()\n",
    "\n",
    "# using the sql\n",
    "student_table.createOrReplaceTempView(\"stu\")\n",
    "sp = spark.sql(\"select * from stu where marks > (select avg(marks) from stu)\")\n",
    "sp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmployeeDetails, EmployeeSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---------+-------------+----------+\n",
      "|EmpId|    FullName|ManagerId|DateOfJoining|      City|\n",
      "+-----+------------+---------+-------------+----------+\n",
      "|  121|   John Snow|      321|   01/31/2014|   Toronto|\n",
      "|  321|Walter White|      986|   01/30/2015|California|\n",
      "|  421|Kuldeep Rana|      876|   27/11/2016| New Delhi|\n",
      "+-----+------------+---------+-------------+----------+\n",
      "\n",
      "root\n",
      " |-- EmpId: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- ManagerId: string (nullable = true)\n",
      " |-- DateOfJoining: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      "\n",
      "+-----+-------+------+--------+\n",
      "|EmpId|Project|Salary|Variable|\n",
      "+-----+-------+------+--------+\n",
      "|  121|     P1|  8000|     500|\n",
      "|  321|     P2| 10000|    1000|\n",
      "|  421|     P1| 12000|       0|\n",
      "+-----+-------+------+--------+\n",
      "\n",
      "root\n",
      " |-- EmpId: string (nullable = true)\n",
      " |-- Project: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- Variable: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EmployeeDetails = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/EmployeeDetails.csv\")\n",
    "EmployeeDetails.show()\n",
    "# EmployeeDetails.printSchema()\n",
    "\n",
    "EmployeeSalary = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/EmployeeSalary.csv\")\n",
    "EmployeeSalary.show()\n",
    "# EmployeeSalary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+\n",
      "|EmpId|Project|Salary|Variable|\n",
      "+-----+-------+------+--------+\n",
      "|  121|     P1|  8000|     500|\n",
      "|  321|     P2| 10000|    1000|\n",
      "|  421|     P1|  8000|       0|\n",
      "+-----+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# update the salary of employee ID 421 to 8000\n",
    "\n",
    "updateslary = EmployeeSalary.withColumn(\"Salary\", expr(\"case when Empid='421' then '8000' else Salary end\"))\n",
    "\n",
    "# updateslary = EmployeeSalary.withColumn(\"Salary\", when(EmployeeSalary.EmpId == \"421\", \"8000\").otherwise(EmployeeSalary.Salary))\n",
    "updateslary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| id|finalsalary|\n",
      "+---+-----------+\n",
      "|121|     8500.0|\n",
      "|321|    11000.0|\n",
      "|421|     8000.0|\n",
      "+---+-----------+\n",
      "\n",
      "+------------+-----------+\n",
      "|    FullName|finalsalary|\n",
      "+------------+-----------+\n",
      "|   John Snow|     8500.0|\n",
      "|Walter White|    11000.0|\n",
      "|Kuldeep Rana|     8000.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a final dataframe with two columns, where finaldf = employe names and finalsalary(finalsalary = variable + salary)\n",
    "\n",
    "finalSalary = updateslary.withColumn(\"finalsalary\", updateslary.Salary + updateslary.Variable).selectExpr(\"EmpId as id\", \"finalsalary\")\n",
    "finalSalary.show()\n",
    "\n",
    "finalSelect = finalSalary.join(EmployeeDetails, col(\"id\") == col(\"EmpId\"), \"left\").select(\"FullName\", \"finalsalary\")\n",
    "finalSelect.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create Dataframe and explode the array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                dept|\n",
      "+---+--------------------+\n",
      "|  1|            [IT, HR]|\n",
      "|  2|[MR, Sales, Finance]|\n",
      "+---+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- dept: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a sc.dataframe using array\n",
    "\n",
    "# 1, (IT, HR)\n",
    "# 2 , (MR, Sales, Finance)\n",
    "\n",
    "df = [\"id\", \"dept\"]\n",
    "dept = [(1, [\"IT\", \"HR\"]), (2 , [\"MR\", \"Sales\", \"Finance\"])]\n",
    "# rdd = spark.sparkContext.parallelize(dept)\n",
    "# rddf = rdd.toDF(df).show()\n",
    "\n",
    "rdd = spark.createDataFrame(dept, schema=df)\n",
    "rdd.show()\n",
    "rdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   dept|\n",
      "+---+-------+\n",
      "|  1|     IT|\n",
      "|  1|     HR|\n",
      "|  2|     MR|\n",
      "|  2|  Sales|\n",
      "|  2|Finance|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodeddf = rdd.withColumn(\"dept\", explode(col(\"dept\")))\n",
    "explodeddf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voting_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------------+---------+\n",
      "|Name|Age|Gender|Constituency|Voting_id|\n",
      "+----+---+------+------------+---------+\n",
      "| aaa| 22|     M|          TN|   RAZ000|\n",
      "| bbb| 27|     M|          MH|   RAZ009|\n",
      "| ccc| 35|     F|          KA|   RAZ007|\n",
      "| ddd| 16|     F|          TN|   RAZ004|\n",
      "| eee| 46|     M|          AP|   RAZ002|\n",
      "+----+---+------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.Read the csv and create a dataframe\n",
    "voting_data = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/voting_data.csv\")\n",
    "voting_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------------+---------+---------+\n",
      "|Name|Age|Gender|Constituency|Voting_id|age_check|\n",
      "+----+---+------+------------+---------+---------+\n",
      "| aaa| 22|     M|          TN|   RAZ000|     true|\n",
      "| bbb| 27|     M|          MH|   RAZ009|     true|\n",
      "| ccc| 35|     F|          KA|   RAZ007|     true|\n",
      "| ddd| 16|     F|          TN|   RAZ004|    false|\n",
      "| eee| 46|     M|          AP|   RAZ002|     true|\n",
      "+----+---+------+------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.Add a new column age_Check to df with condition age > 18 then true else false \n",
    "ageRest = voting_data.withColumn(\"age_check\", expr(\"case when age > 18 then true else false end\"))\n",
    "ageRest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+------------+---------+---------+\n",
      "|Name|Age|Gender|Constituency|Voting_id|age_check|\n",
      "+----+---+------+------------+---------+---------+\n",
      "| aaa| 22|     M|          TN|   RAZ000|     true|\n",
      "| bbb| 27|     M|          MH|   RAZ009|     true|\n",
      "| eee| 46|     M|          AP|   RAZ002|     true|\n",
      "+----+---+------+------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.Create a table from dataframe and retrieve contents from it where gender is male\n",
    "\n",
    "maletable = ageRest.filter(col(\"Gender\") == \"M\")\n",
    "maletable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Plan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+---+\n",
      "|PatientName|PlanId|Amt|\n",
      "+-----------+------+---+\n",
      "|          X|     1|100|\n",
      "|          X|     1|200|\n",
      "|          Y|     2|200|\n",
      "|          Y|     2|300|\n",
      "|          Z|     3|400|\n",
      "|          B|     3|400|\n",
      "+-----------+------+---+\n",
      "\n",
      "+------+--------+-----+\n",
      "|PlanID|PlanName|Limit|\n",
      "+------+--------+-----+\n",
      "|     1|  Plan A|  150|\n",
      "|     2|  Plan B|  250|\n",
      "|     3|  Plan C|  350|\n",
      "+------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Patientdata = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Patientdata.csv\")\n",
    "Patientdata.show()\n",
    "\n",
    "Plan = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Plan.csv\")\n",
    "Plan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---+\n",
      "|PatientName|pid|Amt|\n",
      "+-----------+---+---+\n",
      "|          B|  3|400|\n",
      "|          X|  1|100|\n",
      "|          Y|  2|200|\n",
      "|          Z|  3|400|\n",
      "+-----------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dispatient = Patientdata.dropDuplicates([\"PatientName\"]).withColumnRenamed(\"PlanId\", \"pid\")\n",
    "dispatient.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|PatientName|PlanName|\n",
      "+-----------+--------+\n",
      "|          B|  Plan C|\n",
      "|          X|  Plan A|\n",
      "|          Y|  Plan B|\n",
      "|          Z|  Plan C|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dis_plan = dispatient.join(Plan, col(\"pid\") == col(\"PlanID\"), \"left\").select(\"PatientName\", \"PlanName\")\n",
    "# dis_plan.sort(col(\"PlanName\")).show()\n",
    "dis_plan.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 35 Qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------+\n",
      "|store|entries                     |\n",
      "+-----+----------------------------+\n",
      "|1    |[p1, p2, p3, p4]            |\n",
      "|2    |[p1]                        |\n",
      "|3    |[p1, p2]                    |\n",
      "|4    |[p1, p2, p3, p4, p5, p6, p7]|\n",
      "+-----+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"store\",\"entries\"]\n",
    "data =  [(1,[\"p1\",\"p2\",\"p3\",\"p4\"]),\n",
    "(2,[\"p1\"]),\n",
    "(3,[\"p1\",\"p2\"]),\n",
    "(4,['p1',\"p2\",\"p3\",'p4',\"p5\",\"p6\",\"p7\"])]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "store_ = df.select(\"store\").distinct().count()\n",
    "print(store_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|store|entries|\n",
      "+-----+-------+\n",
      "|    1|     p1|\n",
      "|    1|     p2|\n",
      "|    1|     p3|\n",
      "|    1|     p4|\n",
      "|    2|     p1|\n",
      "|    3|     p1|\n",
      "|    3|     p2|\n",
      "|    4|     p1|\n",
      "|    4|     p2|\n",
      "|    4|     p3|\n",
      "|    4|     p4|\n",
      "|    4|     p5|\n",
      "|    4|     p6|\n",
      "|    4|     p7|\n",
      "+-----+-------+\n",
      "\n",
      "root\n",
      " |-- store: long (nullable = true)\n",
      " |-- entries: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enteries_ = df.withColumn(\"entries\", explode(col(\"entries\")))\n",
    "enteries_.show()\n",
    "enteries_.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "count_ent = enteries_.select(\"entries\").distinct().count()\n",
    "print(count_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|store|entries|\n",
      "+-----+-------+\n",
      "|    4|      7|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame([(store_, count_ent)], schema=[\"store\" , \"entries\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stu_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----+---+------+\n",
      "|RollNo|  name|tamil|eng|math|sci|social|\n",
      "+------+------+-----+---+----+---+------+\n",
      "|203040|Rajesh|   10| 20|  30| 40|    50|\n",
      "+------+------+-----+---+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stu_table = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/stu_table.csv\")\n",
    "stu_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+---+----+---+------+-----+\n",
      "|RollNo|  name|tamil|eng|math|sci|social|total|\n",
      "+------+------+-----+---+----+---+------+-----+\n",
      "|203040|Rajesh|   10| 20|  30| 40|    50|  150|\n",
      "+------+------+-----+---+----+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stu_table.withColumn(\"total\" , (col(\"tamil\")+col(\"eng\")+col(\"math\")+col(\"sci\")+col(\"social\")).cast(\"Integer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### email_phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Mail|       mob|\n",
      "+--------------------+----------+\n",
      "|Renuka1992@gmail.com|9856765434|\n",
      "|anbu.arasu@gmail.com|9844567788|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "email_phone = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/email_phone.csv\")\n",
    "email_phone.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|                Mail|       mob|\n",
      "+--------------------+----------+\n",
      "|R********2@gmail.com|9********4|\n",
      "|a********u@gmail.com|9********8|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_replce_data = email_phone.withColumn(\"Mail\", regexp_replace(col(\"Mail\"),\"(?<!^).(?=.+@)\",\"*\")) \\\n",
    "                                .withColumn(\"mob\", regexp_replace(col(\"mob\"),\"(?<!^).(?!$)\",\"*\"))\n",
    "    \n",
    "regex_replce_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------------------------+\n",
      "|StockId|PredictedPrice                          |\n",
      "+-------+----------------------------------------+\n",
      "|RIL    |[1000, 1005, 1090, 1200, 1000, 900, 890]|\n",
      "|HDFC   |[890, 940, 810, 730, 735, 960, 980]     |\n",
      "|INFY   |[1001, 902, 1000, 990, 1230, 1100, 1200]|\n",
      "+-------+----------------------------------------+\n",
      "\n",
      "root\n",
      " |-- StockId: string (nullable = true)\n",
      " |-- PredictedPrice: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = [\"StockId\",\"PredictedPrice\"]\n",
    "data = [(\"RIL\", [1000,1005,1090,1200,1000,900,890]),\n",
    "(\"HDFC\", [890,940,810,730,735,960,980]),\n",
    "(\"INFY\", [1001,902,1000,990,1230,1100,1200])]\n",
    "\n",
    "stock_data = spark.createDataFrame(data, schema=col)\n",
    "stock_data.show(truncate=False)\n",
    "stock_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|StockId|PredictedPrice|\n",
      "+-------+--------------+\n",
      "|    RIL|          1000|\n",
      "|    RIL|          1005|\n",
      "|    RIL|          1090|\n",
      "|    RIL|          1200|\n",
      "|    RIL|          1000|\n",
      "|    RIL|           900|\n",
      "|    RIL|           890|\n",
      "|   HDFC|           890|\n",
      "|   HDFC|           940|\n",
      "|   HDFC|           810|\n",
      "|   HDFC|           730|\n",
      "|   HDFC|           735|\n",
      "|   HDFC|           960|\n",
      "|   HDFC|           980|\n",
      "|   INFY|          1001|\n",
      "|   INFY|           902|\n",
      "|   INFY|          1000|\n",
      "|   INFY|           990|\n",
      "|   INFY|          1230|\n",
      "|   INFY|          1100|\n",
      "+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_stock_data = stock_data.withColumn(\"PredictedPrice\", explode(stock_data.PredictedPrice))\n",
    "explode_stock_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|StockId|SellPrice|\n",
      "+-------+---------+\n",
      "|    RIL|     1200|\n",
      "|   HDFC|      980|\n",
      "|   INFY|     1230|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_stock_data = explode_stock_data.groupBy(\"StockId\").max(\"PredictedPrice\").withColumnRenamed(\"max(PredictedPrice)\", \"SellPrice\")\n",
    "max_stock_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|StockId1|BuyPrice|\n",
      "+--------+--------+\n",
      "|     RIL|     890|\n",
      "|    HDFC|     730|\n",
      "|    INFY|     902|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_stock_data = explode_stock_data.groupBy(\"StockId\").min(\"PredictedPrice\").withColumnRenamed(\"StockId\",\"StockId1\").withColumnRenamed(\"min(PredictedPrice)\", \"BuyPrice\")\n",
    "min_stock_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+--------+\n",
      "|StockId|SellPrice|StockId1|BuyPrice|\n",
      "+-------+---------+--------+--------+\n",
      "|    RIL|     1200|     RIL|     890|\n",
      "|   HDFC|      980|    HDFC|     730|\n",
      "|   INFY|     1230|    INFY|     902|\n",
      "+-------+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_stock1 = max_stock_data.join(min_stock_data, max_stock_data.StockId == min_stock_data.StockId1, \"left\")\n",
    "final_stock1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+------+\n",
      "|StockId|SellPrice|BuyPrice|Profit|\n",
      "+-------+---------+--------+------+\n",
      "|    RIL|     1200|     890|   310|\n",
      "|   HDFC|      980|     730|   250|\n",
      "|   INFY|     1230|     902|   328|\n",
      "+-------+---------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_stock = final_stock1.withColumn(\"Profit\", final_stock1.SellPrice - final_stock1.BuyPrice) \\\n",
    "                            .select(\"StockId\", \"SellPrice\", \"BuyPrice\", \"Profit\")\n",
    "final_stock.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### travel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+\n",
      "|   name|     travel_location|age|\n",
      "+-------+--------------------+---+\n",
      "|   ravi|[pune, delhi, che...| 32|\n",
      "|gautham|    [delhi, chennai]| 30|\n",
      "|   mary|       [noida, pune]| 35|\n",
      "| thomas|       [delhi, pune]| 31|\n",
      "|shankar|    [chennai, noida]| 30|\n",
      "+-------+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = ['name','travel_location','age']\n",
    "data = [('ravi',['pune','delhi','chennai','noida'],32),\n",
    "('gautham',['delhi','chennai'],30),\n",
    "('mary',['noida','pune'],35),\n",
    "('thomas',['delhi','pune'],31),\n",
    "('shankar',['chennai','noida'],30)]\n",
    " \n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+---+\n",
      "|   name|travel_location|age|\n",
      "+-------+---------------+---+\n",
      "|   ravi|           pune| 32|\n",
      "|   ravi|          delhi| 32|\n",
      "|   ravi|        chennai| 32|\n",
      "|   ravi|          noida| 32|\n",
      "|gautham|          delhi| 30|\n",
      "|gautham|        chennai| 30|\n",
      "|   mary|          noida| 35|\n",
      "|   mary|           pune| 35|\n",
      "| thomas|          delhi| 31|\n",
      "| thomas|           pune| 31|\n",
      "|shankar|        chennai| 30|\n",
      "|shankar|          noida| 30|\n",
      "+-------+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_df = df.withColumn(\"travel_location\", explode(df.travel_location))\n",
    "explode_df.show()\n",
    "\n",
    "# explode_df.groupBy(\"name\").agg(count(col(\"travel_location\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   name|total|\n",
      "+-------+-----+\n",
      "|   ravi|    4|\n",
      "|gautham|    2|\n",
      "|   mary|    2|\n",
      "| thomas|    2|\n",
      "|shankar|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total_travel = explode_df.groupBy(\"name\").agg(count(explode_df.travel_location)).show()\n",
    "total_travel = explode_df.groupBy(\"name\").agg(count(\"travel_location\")).withColumnRenamed(\"count(travel_location)\", \"total\")\n",
    "total_travel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|total|\n",
      "+----+-----+\n",
      "|ravi|    4|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_travel.orderBy(desc(\"total\")).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I have three columns deptno,deptname and place using scala read a text file and display only deptno and deptname. no rdds and dataframes used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_dipt = sc.textFile(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/split_dipt.txt\")\n",
    "split_dipt.foreach(print)\n",
    "\n",
    "splitted_dipt = split_dipt.map(lambda x : x.split(\",\"))\n",
    "splitted_dipt.foreach(print)\n",
    "\n",
    "splitted_dipt.map(lambda x : (x[0], x[1])).foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second_highest_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----+\n",
      "|EmpId|EmpName|Dept| Sal|\n",
      "+-----+-------+----+----+\n",
      "|    1|Charles|   A|1000|\n",
      "|    2|Richard|   A|2000|\n",
      "|    3|   John|   A|2000|\n",
      "|    4| Alisha|   B| 400|\n",
      "|    5|  Robin|   B| 500|\n",
      "|    6|   Kara|   C| 700|\n",
      "|    7|Natalie|   D| 900|\n",
      "|    8|  Harry|   C| 600|\n",
      "|    9|Charles|   D| 500|\n",
      "|   10|   Kate|   A|1000|\n",
      "+-----+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "second_highest_salary = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/second_highest_salary.csv\")\n",
    "second_highest_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+----+----+\n",
      "|EmpId|EmpName|Dept| sal|rank|\n",
      "+-----+-------+----+----+----+\n",
      "|    2|Richard|   A|2000|   1|\n",
      "|    3|   John|   A|2000|   1|\n",
      "|    1|Charles|   A|1000|   2|\n",
      "|   10|   Kate|   A|1000|   2|\n",
      "|    7|Natalie|   D| 900|   3|\n",
      "|    6|   Kara|   C| 700|   4|\n",
      "|    8|  Harry|   C| 600|   5|\n",
      "|    5|  Robin|   B| 500|   6|\n",
      "|    9|Charles|   D| 500|   6|\n",
      "|    4| Alisha|   B| 400|   7|\n",
      "+-----+-------+----+----+----+\n",
      "\n",
      "+----+\n",
      "| sal|\n",
      "+----+\n",
      "|1000|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get 2nd highest salary using window functions\n",
    "casted_data = second_highest_salary.withColumn(\"sal\", second_highest_salary.Sal.cast(\"Integer\"))\n",
    "windowfunc = Window.orderBy(desc(\"Sal\"))\n",
    "\n",
    "final_sal = casted_data.withColumn(\"rank\", dense_rank().over(windowfunc))\n",
    "final_sal.show()\n",
    "\n",
    "final1 = final_sal.filter(final_sal.rank == 2).limit(1)\n",
    "final1.select(\"sal\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### makeToList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-------+\n",
      "|CustId|   CustName|Address|\n",
      "+------+-----------+-------+\n",
      "|     1|   Mark Ray|     AB|\n",
      "|     2|Peter Smith|     CD|\n",
      "|     1|   Mark Ray|     EF|\n",
      "|     2|Peter Smith|     GH|\n",
      "|     2|Peter Smith|     CD|\n",
      "|     3|       Kate|     IJ|\n",
      "+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "makeToList = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/makeToList.csv\")\n",
    "makeToList.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+\n",
      "|   CustName|collect_list(Address)|\n",
      "+-----------+---------------------+\n",
      "|       Kate|                 [IJ]|\n",
      "|Peter Smith|         [CD, GH, CD]|\n",
      "|   Mark Ray|             [AB, EF]|\n",
      "+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "makeToList.groupBy(\"CustName\").agg(collect_list(\"Address\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custTable, pincodeTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+-------+\n",
      "|CustId|CustName|Address|PinCode|\n",
      "+------+--------+-------+-------+\n",
      "|     1|    Mark|     AB|    123|\n",
      "|     2|   Peter|     CD|    456|\n",
      "|     3|    Kate|     EF|    789|\n",
      "+------+--------+-------+-------+\n",
      "\n",
      "+--------+-----+\n",
      "|PinCode1|State|\n",
      "+--------+-----+\n",
      "|     123|   MH|\n",
      "|     456|   WB|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custTable = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/custTable.csv\")\n",
    "custTable.show()\n",
    "\n",
    "pincodeTable = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/pincodeTable.csv\").withColumnRenamed(\"PinCode\", \"PinCode1\")\n",
    "pincodeTable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+-------+-----+\n",
      "|CustId|CustName|Address|PinCode|State|\n",
      "+------+--------+-------+-------+-----+\n",
      "|     1|    Mark|     AB|    123|   MH|\n",
      "|     2|   Peter|     CD|    456|   WB|\n",
      "+------+--------+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalCust = custTable.join(pincodeTable, custTable.PinCode == pincodeTable.PinCode1, \"inner\")\n",
    "finalCust.select(\"CustId\",\"CustName\",\"Address\",\"PinCode\", \"State\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explode_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 emp|\n",
      "+--------------------+\n",
      "|[{[{hyderabad, in...|\n",
      "+--------------------+\n",
      "\n",
      "root\n",
      " |-- emp: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- address: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- line1: string (nullable = true)\n",
      " |    |    |    |    |-- line2: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explode_json_data = spark.read.format(\"json\").option(\"multiline\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/explode_json_data.json\")\n",
    "explode_json_data.show()\n",
    "explode_json_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 emp|\n",
      "+--------------------+\n",
      "|{[{hyderabad, ind...|\n",
      "|{[{bng, ind}, {bn...|\n",
      "|{[{bng, ind}, {bn...|\n",
      "+--------------------+\n",
      "\n",
      "root\n",
      " |-- emp: struct (nullable = true)\n",
      " |    |-- address: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- line1: string (nullable = true)\n",
      " |    |    |    |-- line2: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = explode_json_data.withColumn(\"emp\", explode(\"emp\"))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------------+\n",
      "| id|name|             address|\n",
      "+---+----+--------------------+\n",
      "|123| xyz|[{hyderabad, ind}...|\n",
      "|124| abc|[{bng, ind}, {bng...|\n",
      "|125|jhon|[{bng, ind}, {bng...|\n",
      "+---+----+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- line1: string (nullable = true)\n",
      " |    |    |-- line2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.select(\"emp.id\" , \"emp.name\", \"emp.address\")\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------------+\n",
      "| id|name|          address|\n",
      "+---+----+-----------------+\n",
      "|123| xyz| {hyderabad, ind}|\n",
      "|123| xyz|{hyderabad, null}|\n",
      "|123| xyz|      {null, ind}|\n",
      "|124| abc|       {bng, ind}|\n",
      "|124| abc|      {bng, null}|\n",
      "|124| abc|     {null, null}|\n",
      "|125|jhon|       {bng, ind}|\n",
      "|125|jhon|      {bng, null}|\n",
      "|125|jhon|     {null, null}|\n",
      "+---+----+-----------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: struct (nullable = true)\n",
      " |    |-- line1: string (nullable = true)\n",
      " |    |-- line2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 =df2.withColumn(\"address\", explode(\"address\"))\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---------+-----+\n",
      "| id|name|    line1|line2|\n",
      "+---+----+---------+-----+\n",
      "|123| xyz|hyderabad|  ind|\n",
      "|123| xyz|hyderabad| null|\n",
      "|123| xyz|     null|  ind|\n",
      "|124| abc|      bng|  ind|\n",
      "|124| abc|      bng| null|\n",
      "|124| abc|     null| null|\n",
      "|125|jhon|      bng|  ind|\n",
      "|125|jhon|      bng| null|\n",
      "|125|jhon|     null| null|\n",
      "+---+----+---------+-----+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- line1: string (nullable = true)\n",
      " |-- line2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.select(\"id\", \"name\", \"address.*\")\n",
    "df4.show()\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate_Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+---------+\n",
      "|Empid|empname|salary|managerid|\n",
      "+-----+-------+------+---------+\n",
      "|    1|    xyz| 10000|     NULL|\n",
      "|    2|    abc| 20000|        1|\n",
      "|    3|   fgkb| 30000|        2|\n",
      "|    4|   gfkj| 50000|        2|\n",
      "+-----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Populate_Manager = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Populate_Manager.csv\")\n",
    "Populate_Manager.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+---------+\n",
      "|Empid|empname|salary|managerid|\n",
      "+-----+-------+------+---------+\n",
      "|    1|    xyz| 10000|     NULL|\n",
      "|    2|    abc| 20000|        1|\n",
      "+-----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql \n",
    "Populate_Manager.createOrReplaceTempView(\"managertable\")\n",
    "df2 = spark.sql(\"select Empid, empname, salary, managerid from managertable where empid in (select distinct managerid from managertable)\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mgid|\n",
      "+----+\n",
      "|   1|\n",
      "|   2|\n",
      "|NULL|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using dsl\n",
    "mgid = Populate_Manager.select(\"managerid\").distinct().withColumnRenamed(\"managerid\", \"mgid\")\n",
    "mgid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+---------+\n",
      "|Empid|empname|salary|managerid|\n",
      "+-----+-------+------+---------+\n",
      "|    1|    xyz| 10000|     NULL|\n",
      "|    2|    abc| 20000|        1|\n",
      "+-----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mgid.join(Populate_Manager, mgid.mgid == Populate_Manager.Empid, \"inner\").drop(\"mgid\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parent_child_grand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|child|parent|\n",
      "+-----+------+\n",
      "|    A|    AA|\n",
      "|    B|    BB|\n",
      "|    C|    CC|\n",
      "|   AA|   AAA|\n",
      "|   BB|   BBB|\n",
      "|   CC|   CCC|\n",
      "+-----+------+\n",
      "\n",
      "+------+-------+\n",
      "|child1|parent1|\n",
      "+------+-------+\n",
      "|     A|     AA|\n",
      "|     B|     BB|\n",
      "|     C|     CC|\n",
      "|    AA|    AAA|\n",
      "|    BB|    BBB|\n",
      "|    CC|    CCC|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parent_child_grand = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/parent_child_grand.csv\")\n",
    "parent_child_grand.show()\n",
    "\n",
    "parent_child_grand1 = parent_child_grand.withColumnRenamed(\"child\", \"child1\").withColumnRenamed(\"parent\",\"parent1\")\n",
    "parent_child_grand1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-------+\n",
      "|child|parent|child1|parent1|\n",
      "+-----+------+------+-------+\n",
      "|    A|    AA|    AA|    AAA|\n",
      "|    B|    BB|    BB|    BBB|\n",
      "|    C|    CC|    CC|    CCC|\n",
      "+-----+------+------+-------+\n",
      "\n",
      "+-----+------+-----------+\n",
      "|child|parent|Grandparent|\n",
      "+-----+------+-----------+\n",
      "|    A|    AA|        AAA|\n",
      "|    B|    BB|        BBB|\n",
      "|    C|    CC|        CCC|\n",
      "+-----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grandParent = parent_child_grand.join(parent_child_grand1, parent_child_grand1.child1 == parent_child_grand.parent , \"inner\")\n",
    "grandParent.show()\n",
    "\n",
    "grandParent.withColumnRenamed(\"parent1\", \"Grandparent\").drop(\"child1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employee_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------+----------+\n",
      "| id|         name|salary|manager_id|\n",
      "+---+-------------+------+----------+\n",
      "|  1|   John Smith| 10000|         3|\n",
      "|  2|Jane Anderson| 12000|         3|\n",
      "|  3|    Tom Lanon| 15000|         4|\n",
      "|  4|  Anne Connor| 20000|      NULL|\n",
      "|  5|  Jeremy York|  9000|         1|\n",
      "+---+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employee_manager = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Employee_manager.csv\")\n",
    "Employee_manager.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mgid|\n",
      "+----+\n",
      "|   3|\n",
      "|   1|\n",
      "|   4|\n",
      "|NULL|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manager_ids = Employee_manager.selectExpr(\"manager_id as mgid\").distinct()\n",
    "manager_ids.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+------+----------+\n",
      "| id|       name|salary|manager_id|\n",
      "+---+-----------+------+----------+\n",
      "|  1| John Smith| 10000|         3|\n",
      "|  3|  Tom Lanon| 15000|         4|\n",
      "|  4|Anne Connor| 20000|      NULL|\n",
      "+---+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Employee_manager.join(manager_ids, Employee_manager.id == manager_ids.mgid, \"inner\").drop(\"mgid\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### salary_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+\n",
      "|employee_id|salary|year|\n",
      "+-----------+------+----+\n",
      "|          1| 80000|2020|\n",
      "|          1| 70000|2019|\n",
      "|          1| 60000|2018|\n",
      "|          2| 65000|2020|\n",
      "|          2| 65000|2019|\n",
      "|          2| 60000|2018|\n",
      "|          3| 65000|2019|\n",
      "|          3| 60000|2018|\n",
      "+-----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_inc = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/salary_inc.csv\")\n",
    "salary_inc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+-------+\n",
      "|employee_id|salary|year|nextSal|\n",
      "+-----------+------+----+-------+\n",
      "|          1| 60000|2018|  70000|\n",
      "|          1| 70000|2019|  80000|\n",
      "|          1| 80000|2020|   null|\n",
      "|          2| 60000|2018|  65000|\n",
      "|          2| 65000|2019|  65000|\n",
      "|          2| 65000|2020|   null|\n",
      "|          3| 60000|2018|  65000|\n",
      "|          3| 65000|2019|   null|\n",
      "+-----------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowFunc = Window.partitionBy(\"employee_id\").orderBy(\"year\")\n",
    "\n",
    "nextsal = salary_inc.withColumn(\"nextSal\", lead(\"salary\", 1).over(windowFunc))\n",
    "nextsal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+----+-------+-----+\n",
      "|employee_id|salary|year|nextSal| diff|\n",
      "+-----------+------+----+-------+-----+\n",
      "|          1| 60000|2018|  70000|10000|\n",
      "|          1| 70000|2019|  80000|10000|\n",
      "|          1| 80000|2020|   null| null|\n",
      "|          2| 60000|2018|  65000| 5000|\n",
      "|          2| 65000|2019|  65000|    0|\n",
      "|          2| 65000|2020|   null| null|\n",
      "|          3| 60000|2018|  65000| 5000|\n",
      "|          3| 65000|2019|   null| null|\n",
      "+-----------+------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nextsal.withColumn(\"diff\", expr(\"case when nextsal is null then 0 else abs(nextSal-salary) end\").cast(\"Integer\")).show()\n",
    "nextsal.withColumn(\"diff\", expr(\"abs(nextSal-salary)\").cast(\"Integer\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customers_split.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Customers_split.txt\")\n",
    "data.foreach(print)\n",
    "# see output in console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.map(lambda x : x.replace(\"|^|\", \",\"))\n",
    "data1.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data1.map(lambda x : tuple(x.split(\",\")))\n",
    "data2.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### values_lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|sensorid| timestamp|values|\n",
      "+--------+----------+------+\n",
      "|   11111|2021-01-15|    10|\n",
      "|   11111|2021-01-16|    15|\n",
      "|   11111|2021-01-17|    30|\n",
      "|   11112|2021-01-15|    10|\n",
      "|   11112|2021-01-16|    20|\n",
      "|   11112|2021-01-17|    30|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values_lead = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/values_lead.csv\")\n",
    "values_lead.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-------+\n",
      "|sensorid| timestamp|values|values1|\n",
      "+--------+----------+------+-------+\n",
      "|   11111|2021-01-15|    10|     15|\n",
      "|   11111|2021-01-16|    15|     30|\n",
      "|   11111|2021-01-17|    30|   null|\n",
      "|   11112|2021-01-15|    10|     20|\n",
      "|   11112|2021-01-16|    20|     30|\n",
      "|   11112|2021-01-17|    30|   null|\n",
      "+--------+----------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowfunc = Window.partitionBy(\"sensorid\").orderBy(\"timestamp\")\n",
    "\n",
    "value_lead_data = values_lead.withColumn(\"values1\", lead(\"values\", 1).over(windowfunc))\n",
    "value_lead_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|sensorid| timestamp|values|\n",
      "+--------+----------+------+\n",
      "|   11111|2021-01-15|     5|\n",
      "|   11111|2021-01-16|    15|\n",
      "|   11111|2021-01-17|  null|\n",
      "|   11112|2021-01-15|    10|\n",
      "|   11112|2021-01-16|    10|\n",
      "|   11112|2021-01-17|  null|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = value_lead_data.withColumn(\"values\", expr(\"abs(values1-values)\").cast(\"Int\")).drop(\"values1\")\n",
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|sensorid| timestamp|values|\n",
      "+--------+----------+------+\n",
      "|   11111|2021-01-15|     5|\n",
      "|   11111|2021-01-16|    15|\n",
      "|   11112|2021-01-15|    10|\n",
      "|   11112|2021-01-16|    10|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "findata = data1.filter(\"values is not null\")\n",
    "findata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average_Salary_Under_Manager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----------+\n",
      "|Emp_Id|Emp_name|Salary|Manager_Id|\n",
      "+------+--------+------+----------+\n",
      "|    10|    Anil| 50000|        18|\n",
      "|    11|   Vikas| 75000|        16|\n",
      "|    12|   Nisha| 40000|        18|\n",
      "|    13|   Nidhi| 60000|        17|\n",
      "|    14|   Priya| 80000|        18|\n",
      "|    15|   Mohit| 45000|        18|\n",
      "|    16|  Rajesh| 90000|        16|\n",
      "|    17|   Raman| 55000|        16|\n",
      "|    18| Santosh| 65000|        17|\n",
      "+------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Average_Salary_Under_Manager = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Average_Salary_Under_Manager.csv\")\n",
    "Average_Salary_Under_Manager.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|mid|\n",
      "+---+\n",
      "| 16|\n",
      "| 18|\n",
      "| 17|\n",
      "+---+\n",
      "\n",
      "+------+--------+------+----------+---+\n",
      "|Emp_Id|Emp_name|Salary|Manager_Id|mid|\n",
      "+------+--------+------+----------+---+\n",
      "|    16|  Rajesh| 90000|        16| 16|\n",
      "|    17|   Raman| 55000|        16| 17|\n",
      "|    18| Santosh| 65000|        17| 18|\n",
      "+------+--------+------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dismanger = Average_Salary_Under_Manager.select(\"Manager_Id\").withColumnRenamed(\"Manager_Id\", \"mid\").distinct()\n",
    "dismanger.show()\n",
    "Average_Salary_Under_Manager.join(dismanger, Average_Salary_Under_Manager.Emp_Id == dismanger.mid, \"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+\n",
      "|mid|Average_Salary_Under_Manager|\n",
      "+---+----------------------------+\n",
      "| 16|                       73333|\n",
      "| 18|                       53750|\n",
      "| 17|                       62500|\n",
      "+---+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intdata = Average_Salary_Under_Manager.withColumn(\"Salary\", Average_Salary_Under_Manager.Salary.cast(\"Integer\"))\n",
    "grouped_data = intdata.groupBy(\"Manager_Id\").agg(avg(\"Salary\").cast(\"Integer\").alias(\"Average_Salary_Under_Manager\")).withColumnRenamed(\"Manager_Id\",\"mid\")\n",
    "grouped_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----------+---+----------------------------+\n",
      "|Emp_Id|Emp_name|Salary|Manager_Id|mid|Average_Salary_Under_Manager|\n",
      "+------+--------+------+----------+---+----------------------------+\n",
      "|    16|  Rajesh| 90000|        16| 16|                       73333|\n",
      "|    17|   Raman| 55000|        16| 17|                       62500|\n",
      "|    18| Santosh| 65000|        17| 18|                       53750|\n",
      "+------+--------+------+----------+---+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final1 = Average_Salary_Under_Manager.join(grouped_data, Average_Salary_Under_Manager.Emp_Id == grouped_data.mid, \"inner\")\n",
    "final1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------------------------+\n",
      "|Manager_Id|Manager|Average_Salary_Under_Manager|\n",
      "+----------+-------+----------------------------+\n",
      "|        16| Rajesh|                       73333|\n",
      "|        18|Santosh|                       53750|\n",
      "|        17|  Raman|                       62500|\n",
      "+----------+-------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = final1.selectExpr(\"Emp_Id as Manager_Id\", \"Emp_name as Manager\", \"Average_Salary_Under_Manager\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+-------------------+----------+\n",
      "|WORKER_ID|FIRST_NAME|LAST_NAME|SALARY|       JOINING_DATE|DEPARTMENT|\n",
      "+---------+----------+---------+------+-------------------+----------+\n",
      "|      001|    Monika|    Arora|100000|2014-02-20 09:00:00|        HR|\n",
      "|      002|  Niharika|    Verma|300000|2014-06-11 09:00:00|     Admin|\n",
      "|      003|    Vishal|  Singhal|300000|2014-02-20 09:00:00|        HR|\n",
      "|      004|   Amitabh|    Singh|500000|2014-02-20 09:00:00|     Admin|\n",
      "|      005|     Vivek|    Bhati|500000|2014-06-11 09:00:00|     Admin|\n",
      "+---------+----------+---------+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equal_salary = spark.read.format(\"csv\").option(\"header\", \"true\")\\\n",
    "    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/equal_salary.csv\")\n",
    "equal_salary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python Rjust, ljust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001\n",
      "00001\n",
      "00011\n",
      "00111\n",
      "01111\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# 01\n",
    "# 011\n",
    "# 0111\n",
    "# 01111\n",
    "\n",
    "list = [\"1\", \"01\", \"011\", \"0111\", \"01111\"]\n",
    "for i in list:\n",
    "    print(i.rjust(5, \"0\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### python rpad, lpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|input|\n",
      "+-----+\n",
      "|    1|\n",
      "|   01|\n",
      "|  011|\n",
      "| 0111|\n",
      "|01111|\n",
      "+-----+\n",
      "\n",
      "+-----+\n",
      "|input|\n",
      "+-----+\n",
      "|00001|\n",
      "|00001|\n",
      "|00011|\n",
      "|00111|\n",
      "|01111|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# 01\n",
    "# 011\n",
    "# 0111\n",
    "# 01111\n",
    "\n",
    "data=spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "                    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/pattern.txt\")\n",
    "data.show()\n",
    "data.withColumn(\"input\", lpad(data.input, 5, \"0\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### equal_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+------+-------------------+----------+\n",
      "|WORKER_ID|FIRST_NAME|LAST_NAME|SALARY|       JOINING_DATE|DEPARTMENT|\n",
      "+---------+----------+---------+------+-------------------+----------+\n",
      "|      001|    Monika|    Arora|100000|2014-02-20 09:00:00|        HR|\n",
      "|      002|  Niharika|    Verma|300000|2014-06-11 09:00:00|     Admin|\n",
      "|      003|    Vishal|  Singhal|300000|2014-02-20 09:00:00|        HR|\n",
      "|      004|   Amitabh|    Singh|500000|2014-02-20 09:00:00|     Admin|\n",
      "|      005|     Vivek|    Bhati|500000|2014-06-11 09:00:00|     Admin|\n",
      "+---------+----------+---------+------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equal_salary=spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "                    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/equal_salary.csv\")\n",
    "equal_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|SALARY|        SameSalary|\n",
      "+------+------------------+\n",
      "|300000|[Niharika, Vishal]|\n",
      "|100000|          [Monika]|\n",
      "|500000|  [Amitabh, Vivek]|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equal_salary.groupBy(\"SALARY\").agg(collect_list(\"FIRST_NAME\").alias(\"SameSalary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordered_dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|Order_id|status_date|    status|\n",
      "+--------+-----------+----------+\n",
      "|       1|      1-Jan|   Ordered|\n",
      "|       1|      2-Jan|dispatched|\n",
      "|       1|      3-Jan|dispatched|\n",
      "|       1|      4-Jan|   Shipped|\n",
      "|       1|      5-Jan|   Shipped|\n",
      "|       1|      6-Jan| Delivered|\n",
      "|       2|      1-Jan|   Ordered|\n",
      "|       2|      2-Jan|dispatched|\n",
      "|       2|      3-Jan|   shipped|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ordered_dispatch=spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "                    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/Ordered_dispatch.csv\")\n",
    "Ordered_dispatch.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|Order_id|status_date|    status|\n",
      "+--------+-----------+----------+\n",
      "|       1|      2-Jan|dispatched|\n",
      "|       1|      3-Jan|dispatched|\n",
      "|       2|      2-Jan|dispatched|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Ordered_dispatch.filter(Ordered_dispatch.status == \"dispatched\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input: aabbccabca\n",
    "### output: a2b2c2a1b1c1a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2b2c2a1b1c1\n"
     ]
    }
   ],
   "source": [
    "# this code is not entirely correct\n",
    "# input = \"aabbccabca\"\n",
    "# output = a2b2c2a1b1c1a1\n",
    "\n",
    "input = \"aabbccabcaa\"\n",
    "num = 1\n",
    "output = \"\"\n",
    "\n",
    "for i in range(1, len(input)):\n",
    "    if input[i-1] == input[i]:\n",
    "        num = num+1\n",
    "    else:\n",
    "#         print(output)\n",
    "        output = output + input[i-1] + str(num)\n",
    "        num = 1\n",
    "        \n",
    "print(output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3b2c2a3b3a1\n"
     ]
    }
   ],
   "source": [
    "# this code is correct\n",
    "input1 = \"aaabbccaaabbba\"\n",
    "output = \"\"\n",
    "i = 0\n",
    "\n",
    "while i < len(input1):\n",
    "    sum = 1\n",
    "    while i + 1 < len(input1) and input1[i] == input1[i+1] :\n",
    "        sum = sum + 1\n",
    "        i += 1\n",
    "\n",
    "    output = output + input1[i] + str(sum)\n",
    "    i += 1\n",
    "     \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scala code\n",
    "\n",
    "# def func(s: String): String = {\n",
    "#   var result = \"\"\n",
    "#   var count = 1\n",
    "\n",
    "#   for (i <- 1 until s.length) {\n",
    "#     if (s(i) == s(i - 1)) {\n",
    "#       count += 1\n",
    "#     } else {\n",
    "#       result += s(i - 1) + count.toString\n",
    "#       count = 1\n",
    "#     }\n",
    "#   }\n",
    "\n",
    "#   result\n",
    "# }\n",
    "\n",
    "# val input = \"aabbccabca\"\n",
    "# val result = func(input)\n",
    "# println(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### teamName_match "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| Id|   TeamName|\n",
      "+---+-----------+\n",
      "|  1|      India|\n",
      "|  2|  Australia|\n",
      "|  3|    England|\n",
      "|  4|New Zealand|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teamName_match =spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "                    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/teamName_match.csv\")\n",
    "teamName_match.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|id1|        tm1|\n",
      "+---+-----------+\n",
      "|  1|      India|\n",
      "|  2|  Australia|\n",
      "|  3|    England|\n",
      "|  4|New Zealand|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teamName_match1 = teamName_match.selectExpr(\"id as id1\", \"TeamName as tm1\")\n",
    "teamName_match1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              output|\n",
      "+--------------------+\n",
      "|India vs New Zealand|\n",
      "|  Australia vs India|\n",
      "|Australia vs England|\n",
      "|Australia vs New ...|\n",
      "|    England vs India|\n",
      "|England vs New Ze...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql\n",
    "teamName_match.createOrReplaceTempView(\"df\")\n",
    "spark.sql(\"select concat(t1.TeamName,' vs ',t2.Teamname) as output from df t1 join df t2 on t1.TeamName < t2.TeamName\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-----------+\n",
      "| Id| TeamName|id1|        tm1|\n",
      "+---+---------+---+-----------+\n",
      "|  1|    India|  4|New Zealand|\n",
      "|  2|Australia|  1|      India|\n",
      "|  2|Australia|  3|    England|\n",
      "|  2|Australia|  4|New Zealand|\n",
      "|  3|  England|  1|      India|\n",
      "|  3|  England|  4|New Zealand|\n",
      "+---+---------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # using DSL\n",
    "joinedtable = teamName_match.join(teamName_match1, teamName_match.TeamName < teamName_match1.tm1, \"inner\")\n",
    "joinedtable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              output|\n",
      "+--------------------+\n",
      "|India vs New Zealand|\n",
      "|  Australia vs India|\n",
      "|Australia vs England|\n",
      "|Australia vs New ...|\n",
      "|    England vs India|\n",
      "|England vs New Ze...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedtable.selectExpr(\"concat(TeamName, ' vs ' , tm1) as output\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### in python\n",
    "### list1= [\"India\", \"Australia\", \"England\", \"New Zealand\"]\n",
    "#### output should be same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India vs Australia\n",
      "India vs England\n",
      "India vs New Zealand\n",
      "Australia vs England\n",
      "Australia vs New Zealand\n",
      "England vs New Zealand\n"
     ]
    }
   ],
   "source": [
    "list1= [\"India\", \"Australia\", \"England\", \"New Zealand\"]\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    for j in range(len(list1)):\n",
    "        if i < j:\n",
    "            print(list1[i] + \" vs \" + list1[j])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flight source and destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "### self join example\n",
    "\n",
    "# empDF.as(\"emp1\").join(empDF.as(\"emp2\"), col(\"emp1.superior_emp_id\") === col(\"emp2.emp_id\"),\"inner\")\n",
    "# .select(col(\"emp1.emp_id\"),col(\"emp1.name\"),col(\"emp2.emp_id\").as(\"superior_emp_id\"),col(\"emp2.name\").as(\"superior_emp_name\"))\n",
    "# .show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+--------+\n",
      "| id|   airway|     src|    dest|\n",
      "+---+---------+--------+--------+\n",
      "|  1|   Indigo|   India|  Bhutan|\n",
      "|  2| Air Asia|     Aus|   India|\n",
      "|  3|   Indigo|   Nepal|SriLanka|\n",
      "|  4|spice jet|SriLanka|  Bhutan|\n",
      "|  5|   Indigo|  Bhutan|SriLanka|\n",
      "|  6| Air Asia|   India|   Japan|\n",
      "|  7|spice jet|  Bhutan|   Nepal|\n",
      "+---+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"id\", \"airway\", \"src\", \"dest\"]\n",
    "Values = [(1, 'Indigo', 'India', 'Bhutan'),\n",
    "(2, 'Air Asia', 'Aus', 'India'),\n",
    "(3, 'Indigo', 'Nepal', 'SriLanka'),\n",
    "(4, 'spice jet', 'SriLanka', 'Bhutan'),\n",
    "(5, 'Indigo', 'Bhutan', 'SriLanka'),\n",
    "(6, 'Air Asia', 'India', 'Japan'),\n",
    "(7, 'spice jet', 'Bhutan', 'Nepal')]\n",
    "\n",
    "dataframe = spark.createDataFrame(Values, schema=schema)\n",
    "dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+--------+\n",
      "|id1|  airway1|    src1|   dest1|\n",
      "+---+---------+--------+--------+\n",
      "|  1|   Indigo|   India|  Bhutan|\n",
      "|  2| Air Asia|     Aus|   India|\n",
      "|  3|   Indigo|   Nepal|SriLanka|\n",
      "|  4|spice jet|SriLanka|  Bhutan|\n",
      "|  5|   Indigo|  Bhutan|SriLanka|\n",
      "|  6| Air Asia|   India|   Japan|\n",
      "|  7|spice jet|  Bhutan|   Nepal|\n",
      "+---+---------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe1 = dataframe.selectExpr(\"airway as airway1\", \"src as src1\", \"dest as dest1\")\n",
    "dataframe1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+------+---+---------+------+--------+\n",
      "| id|   airway|     src|  dest|id1|  airway1|  src1|   dest1|\n",
      "+---+---------+--------+------+---+---------+------+--------+\n",
      "|  2| Air Asia|     Aus| India|  6| Air Asia| India|   Japan|\n",
      "|  1|   Indigo|   India|Bhutan|  5|   Indigo|Bhutan|SriLanka|\n",
      "|  4|spice jet|SriLanka|Bhutan|  7|spice jet|Bhutan|   Nepal|\n",
      "+---+---------+--------+------+---+---------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joindf = dataframe.join(dataframe1, dataframe.dest == dataframe1.src1, \"left\").filter(dataframe.airway == dataframe1.airway1)\n",
    "joindf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----------+\n",
      "|   Flight|  Source|Destination|\n",
      "+---------+--------+-----------+\n",
      "| Air Asia|     Aus|      Japan|\n",
      "|   Indigo|   India|   SriLanka|\n",
      "|spice jet|SriLanka|      Nepal|\n",
      "+---------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joindf.selectExpr(\"airway as Flight\", \"src as Source\", \" dest1 as Destination\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list with wrong index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['a', 'b', 'c', 'd', 'e']\n"
     ]
    }
   ],
   "source": [
    "list = ['a', 'b', 'c', 'd', 'e']\n",
    "print(list[9:]) \n",
    "print(list[-9:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlroW olleH\n"
     ]
    }
   ],
   "source": [
    "# INPUT  ==> (\"Hello World\")\n",
    "# OUTPUT ==> (\"olleH dlroW\")\n",
    "\n",
    "input = \"Hello World\"\n",
    "output = \"\"\n",
    "\n",
    "for i in input:\n",
    "    output = i + output\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scala code\n",
    "\n",
    "# val input=\"Hello World\"\n",
    "# val split_word= input.split(\" \")\n",
    "# val reverse=split_word.map(x=>x.reverse) \n",
    "# val result=reverse.reduce((a,b)=>a+\" \"+b)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+\n",
      "|Token|      Date|      Time|\n",
      "+-----+----------+----------+\n",
      "|12345|2023-05-17|12:29:47AM|\n",
      "|12345|2023-05-17|12:29:49AM|\n",
      "|12345|2023-05-17|12:29:54AM|\n",
      "|12346|2023-05-17|12:29:46AM|\n",
      "|12346|2023-05-17|12:29:48AM|\n",
      "|12346|2023-05-17|12:29:50AM|\n",
      "|12346|2023-05-17|12:29:51AM|\n",
      "+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_date =spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    "                    .load(\"file:///D:/Data Analytics applications/Eclipse Projects/scenarios_data/token_date.csv\")\n",
    "token_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+------+\n",
      "|Token|      Date|      Time|visits|\n",
      "+-----+----------+----------+------+\n",
      "|12345|2023-05-17|12:29:47AM|     1|\n",
      "|12345|2023-05-17|12:29:49AM|     2|\n",
      "|12345|2023-05-17|12:29:54AM|     3|\n",
      "|12346|2023-05-17|12:29:46AM|     1|\n",
      "|12346|2023-05-17|12:29:48AM|     2|\n",
      "|12346|2023-05-17|12:29:50AM|     3|\n",
      "|12346|2023-05-17|12:29:51AM|     4|\n",
      "+-----+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowfunc = Window.partitionBy(\"Token\").orderBy(\"Time\")\n",
    "\n",
    "ordered_date = token_date.withColumn(\"visits\", dense_rank().over(windowfunc))\n",
    "ordered_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+------+\n",
      "|Token|      Date|      Time|visits|\n",
      "+-----+----------+----------+------+\n",
      "|12345|2023-05-17|12:29:47AM|     1|\n",
      "|12345|2023-05-17|12:29:49AM|     0|\n",
      "|12345|2023-05-17|12:29:54AM|     0|\n",
      "|12346|2023-05-17|12:29:46AM|     1|\n",
      "|12346|2023-05-17|12:29:48AM|     0|\n",
      "|12346|2023-05-17|12:29:50AM|     0|\n",
      "|12346|2023-05-17|12:29:51AM|     0|\n",
      "+-----+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordered_date.withColumn(\"visits\", expr(\"case when visits = 1 then 1 else 0 end\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Scala program to compute the sum of the two given integer values. \n",
    "### If the two values are the same, then return triples their sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "sum = 0\n",
    "\n",
    "if a == b:\n",
    "    sum = (a + b) * 3\n",
    "    \n",
    "else:\n",
    "    sum = a + b\n",
    "    \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
