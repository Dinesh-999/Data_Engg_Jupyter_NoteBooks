{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae39025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/14 20:15:52 WARN Utils: Your hostname, DINESHs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.100 instead (on interface en0)\n",
      "24/05/14 20:15:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/14 20:15:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported All\n"
     ]
    }
   ],
   "source": [
    "# Normal Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import col,struct,when, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "print(\"imported\")\n",
    "# Agg\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set, sum, avg, max, countDistinct, count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance, var_samp, var_pop\n",
    "\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.types import MapType, StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "print(\"imported\")\n",
    "# spark context\n",
    "conf = SparkConf().setAppName(\"Spark\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Usage of config()\n",
    "# spark = SparkSession.builder \\\n",
    "#       .master(\"local[1]\") \\\n",
    "#       .appName(\"LearnSpark\") \\\n",
    "#       .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "#       .getOrCreate()\n",
    "\n",
    "print(\"imported All\")\n",
    "\n",
    "Location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Datasets/\"\n",
    "print(Location)\n",
    "\n",
    "image_location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Notebook images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9645880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Datasets/\n"
     ]
    }
   ],
   "source": [
    "Location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Datasets/\"\n",
    "print(Location)\n",
    "\n",
    "image_location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Notebook images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b38d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|       Employee_Name|EmpID|DeptID|Salary|\n",
      "+--------------------+-----+------+------+\n",
      "| Adinolfi, Wilson  K|10026|     5| 62506|\n",
      "|Ait Sidi, Karthik...|10084|     3|104437|\n",
      "|   Akinkuolie, Sarah|10196|     5| 64955|\n",
      "|        Alagbe,Trina|10088|     5| 64991|\n",
      "|    Anderson, Carol |10069|     5| 50825|\n",
      "|   Anderson, Linda  |10002|     5| 57568|\n",
      "|     Andreola, Colby|10194|     4| 95660|\n",
      "|         Athwal, Sam|10062|     5| 59365|\n",
      "|    Bachiochi, Linda|10114|     5| 47837|\n",
      "|  Bacong, Alejandro |10250|     3| 50178|\n",
      "|Baczenski, Rachael  |10252|     5| 54670|\n",
      "|     Barbara, Thomas|10242|     5| 47211|\n",
      "|    Barbossa, Hector|10012|     3| 92328|\n",
      "|Barone, Francesco  A|10265|     5| 58709|\n",
      "|       Barton, Nader|10066|     5| 52505|\n",
      "|       Bates, Norman|10061|     5| 57834|\n",
      "|    Beak, Kimberly  |10023|     5| 70131|\n",
      "| Beatrice, Courtney |10055|     5| 59026|\n",
      "|       Becker, Renee|10245|     3|110000|\n",
      "|       Becker, Scott|10277|     5| 53250|\n",
      "+--------------------+-----+------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Employee_Name: string (nullable = true)\n",
      " |-- EmpID: integer (nullable = true)\n",
      " |-- DeptID: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110963608590708737/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110963608590708737%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "#Write_a_spark_code_to_evaluate_dept_wise_10th_highest_salary_and\n",
    "        #top_6_salary using_pyspark_dataframe?\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n",
    "                                .load(Location + \"HRDataset_v14.csv\")\n",
    "# df.printSchema()\n",
    "\n",
    "df = df.select( \"Employee_Name\", \"EmpID\", \"DeptID\", \"Salary\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba898c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+----+\n",
      "|       Employee_Name|EmpID|DeptID|Salary|rank|\n",
      "+--------------------+-----+------+------+----+\n",
      "|     Howard, Estelle|10182|     1| 49920|   1|\n",
      "|         Singh, Nan |10039|     1| 51920|   2|\n",
      "|    Smith, Leigh Ann|10153|     1| 55000|   3|\n",
      "|          Brown, Mia|10238|     1| 63000|   4|\n",
      "|    Steans, Tyrone  |10147|     1| 63003|   5|\n",
      "|LaRotonda, William  |10038|     1| 64520|   6|\n",
      "|         Quinn, Sean|10131|     1| 83363|   7|\n",
      "| LeBlanc, Brandon  R|10134|     1| 93046|   8|\n",
      "|   Foster-Baker, Amy|10080|     1| 99351|   9|\n",
      "|   Boutwell, Bonalyn|10081|     1|106367|  10|\n",
      "|         King, Janet|10089|     2|250000|   1|\n",
      "|  Bacong, Alejandro |10250|     3| 50178|   1|\n",
      "|     Shepard, Anita |10179|     3| 50750|   2|\n",
      "|   Lindsay, Leonara |10008|     3| 51777|   3|\n",
      "|       Morway, Tanya|10151|     3| 52599|   4|\n",
      "|          Fett, Boba|10309|     3| 53366|   5|\n",
      "|     Gonzalez, Maria|10101|     3| 61242|   6|\n",
      "|         Galia, Lisa|10273|     3| 65707|   7|\n",
      "|        Soto, Julia |10119|     3| 66593|   8|\n",
      "|       Clayton, Rick|10220|     3| 68678|   9|\n",
      "+--------------------+-----+------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"deptid\").orderBy(\"salary\")\n",
    "\n",
    "employeeRank = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "employeeRank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c362961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+------+------+----+\n",
      "|    Employee_Name|EmpID|DeptID|Salary|rank|\n",
      "+-----------------+-----+------+------+----+\n",
      "|Boutwell, Bonalyn|10081|     1|106367|  10|\n",
      "|     Dolan, Linda|10133|     3| 70621|  10|\n",
      "| Patronick, Lucas|10005|     4|108987|  10|\n",
      "| Purinton, Janine|10262|     5| 46430|  10|\n",
      "|    Delarge, Alex|10306|     6| 61568|  10|\n",
      "+-----------------+-----+------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for 10th highest salary\n",
    "tenth_highest_salary = employeeRank.filter(col(\"rank\") == 10)\n",
    "tenth_highest_salary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12564229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+----+\n",
      "|       Employee_Name|EmpID|DeptID|Salary|rank|\n",
      "+--------------------+-----+------+------+----+\n",
      "|     Howard, Estelle|10182|     1| 49920|   1|\n",
      "|         Singh, Nan |10039|     1| 51920|   2|\n",
      "|    Smith, Leigh Ann|10153|     1| 55000|   3|\n",
      "|          Brown, Mia|10238|     1| 63000|   4|\n",
      "|    Steans, Tyrone  |10147|     1| 63003|   5|\n",
      "|LaRotonda, William  |10038|     1| 64520|   6|\n",
      "|         King, Janet|10089|     2|250000|   1|\n",
      "|  Bacong, Alejandro |10250|     3| 50178|   1|\n",
      "|     Shepard, Anita |10179|     3| 50750|   2|\n",
      "|   Lindsay, Leonara |10008|     3| 51777|   3|\n",
      "|       Morway, Tanya|10151|     3| 52599|   4|\n",
      "|          Fett, Boba|10309|     3| 53366|   5|\n",
      "|     Gonzalez, Maria|10101|     3| 61242|   6|\n",
      "|          Cady, Max |10150|     4| 77692|   1|\n",
      "|        Saada, Adell|10126|     4| 86214|   2|\n",
      "|       Szabo, Andrew|10024|     4| 92989|   3|\n",
      "|    Carabbio, Judith|10085|     4| 93396|   4|\n",
      "|     Andreola, Colby|10194|     4| 95660|   5|\n",
      "|      Exantus, Susan|10290|     4| 99280|   6|\n",
      "|       Zima, Colleen|10271|     5| 45046|   1|\n",
      "+--------------------+-----+------+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter for top 6 salaries\n",
    "top_six_salaries = employeeRank.filter(col(\"rank\").between(1, 6))\n",
    "top_six_salaries.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8377e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300de235",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7106478906605535232/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7106478906605535232%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "# How to choose number of executors required for our cluster ?\n",
    "\n",
    "😇 My Data Engineer Interview Question🌟\n",
    "========================================\n",
    "Let's Begin ...... 🏄‍♂️\n",
    "\n",
    "Q) : How to choose number of executors required for our cluster ?\n",
    "\n",
    "👉 To answer this question we need cluster size.\n",
    "Let's consider it is a \"20 Node Cluster\"\n",
    "\n",
    "Each Node -\n",
    "30 Cores\n",
    "128GB RAM\n",
    "\n",
    "➡ For good throughput let's assign 5 CORES per EXECUTOR\n",
    "--executor-cores = 5\n",
    "\n",
    "➡Should leave 1 core for Background activity (Hadoop/Yarn daemons) Number of cores available = 30 -1 = 29\n",
    "\n",
    "➡Total available of cores in cluster = 29*20 =580\n",
    "\n",
    "➡ Number of executors in cluster\n",
    "= total cores per cluster / no.of cores per executors\n",
    "= 580/5 = 116\n",
    "\n",
    "➡Should leave 1 executor for Yarn (Application Manager)\n",
    "Now available executors in cluster = 116 - 1 = 115\n",
    "\n",
    "➡Number of executors per node = 116/20 = 6 (approx)\n",
    "\n",
    "➡Memory per executor = 128GB /6 = 21 GB\n",
    "7% of 21 GB approximately 1.5 GB will be allocated to heap overhead.\n",
    "\n",
    "➡Now actual --executor-memory = 21GB - 2GB = 19 GB per executor.\n",
    "\n",
    "So, recommended\n",
    "💁‍♂️ 116 Executors (19 GB Memory, 5 cores) for \"20 Node Cluster\".\n",
    "\n",
    "\n",
    "![alternatvie text](https://media.licdn.com/dms/image/D5622AQECB7ldyRe0wg/feedshare-shrink_800/0/1694290459006?e=1706745600&v=beta&t=OzBdu0ffSmkRpcglXyfRJW9dZXPsM0P5LI_LYSohkac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e94de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {
    "1694290459006.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "d22925cc",
   "metadata": {},
   "source": [
    "<img src=\"https://media.licdn.com/dms/image/D5622AQECB7ldyRe0wg/feedshare-shrink_800/0/1694290459006?e=1706745600&v=beta&t=OzBdu0ffSmkRpcglXyfRJW9dZXPsM0P5LI_LYSohkac\" alt=\"Alternative text\" />\n",
    "\n",
    "or\n",
    "\n",
    "![1694290459006.jpeg](attachment:1694290459006.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648774e9",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/posts/mahaboob-pathan_snowflake-snowsql-sql-activity-7132574458640228352-AqzH\n",
    "\n",
    "Here are different syntax variations which can be used with the COUNT() function on Snowflake.\n",
    "\n",
    "1️⃣ COUNT(*) ➡️ will return a total count of rows in the table\n",
    "\n",
    "2️⃣ COUNT(<col_name>) ➡️ will return a count of rows with a non-NULL value in that column\n",
    "\n",
    "3️⃣ COUNT(DISTINCT <col_name>) ➡️ will return a count of distinct rows in that column (excluding non-NULL value)\n",
    "\n",
    "4️⃣ COUNT(<col_A, col_B>) ➡️ only counts the rows that have non-NULL values in either the A or B column\n",
    "\n",
    "5️⃣ COUNT(DISTINCT <col_A, col_B>) ➡️ only counts the distinct rows that have non-NULL values in either the A or B column\n",
    "\n",
    "6️⃣ COUNT(<alias>.*) ➡️ will return the count of all the rows containing non-NULL columns\n",
    "\n",
    "7️⃣ COUNT(DISTINCT <alias>.*) ➡️ will return the count of all the distinct rows containing non-NULL columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b033ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdabbecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|      A|\n",
      "|  2|      D|\n",
      "|  3|      E|\n",
      "|  4|      G|\n",
      "|  5|      J|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/mahaboob-pathan_namastesql-sqlquery-sqlchallenges-activity-7113771061086744576-xfnO\n",
    "\n",
    "schema_ = [\"id\", \"student\"]\n",
    "\n",
    "data = [(1, 'A'),\n",
    "(2, 'D'),\n",
    "(3, 'E'),\n",
    "(4, 'G'),\n",
    "(5, 'J')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema= schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1f9223d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+---+-------+\n",
      "|res|student|\n",
      "+---+-------+\n",
      "|  1|      D|\n",
      "|  2|      A|\n",
      "|  3|      G|\n",
      "|  4|      E|\n",
      "|  5|      J|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without using lead or lag\n",
    "\n",
    "max_el = lead_lag.selectExpr(\"max(id)\").collect()[0][0]\n",
    "print(max_el)\n",
    "\n",
    "finaldf = df.withColumn(\"res\", \n",
    "        when(col('id') % 2 == 0, col(\"id\")-1)\\\n",
    "        .when(col(\"id\")%2 != 0, when(col('id') == max_el , col(\"id\"))\\\n",
    "              .otherwise(col(\"id\")+1)))\n",
    "\n",
    "finaldf.select(\"res\", \"student\").orderBy(\"res\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e6d7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----+\n",
      "| id|student|lead| lag|\n",
      "+---+-------+----+----+\n",
      "|  1|      A|   D|null|\n",
      "|  2|      D|   E|   A|\n",
      "|  3|      E|   G|   D|\n",
      "|  4|      G|   J|   E|\n",
      "|  5|      J|null|   G|\n",
      "+---+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with using lead and lag functions\n",
    "\n",
    "lead_fun = Window.orderBy(asc(\"id\"))\n",
    "\n",
    "lead_lag = df.withColumn(\"lead\", lead(\"student\").over(lead_fun))\\\n",
    "        .withColumn(\"lag\", lag(\"student\").over(lead_fun))\n",
    "lead_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef9d9523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----+----+\n",
      "| id|student|lead| lag| res|\n",
      "+---+-------+----+----+----+\n",
      "|  1|      A|   D|null|   D|\n",
      "|  2|      D|   E|   A|   A|\n",
      "|  3|      E|   G|   D|   G|\n",
      "|  4|      G|   J|   E|   E|\n",
      "|  5|      J|null|   G|null|\n",
      "+---+-------+----+----+----+\n",
      "\n",
      "5\n",
      "+---+-------+----+----+---+\n",
      "| id|student|lead| lag|res|\n",
      "+---+-------+----+----+---+\n",
      "|  1|      A|   D|null|  D|\n",
      "|  2|      D|   E|   A|  A|\n",
      "|  3|      E|   G|   D|  G|\n",
      "|  4|      G|   J|   E|  E|\n",
      "|  5|      J|null|   G|  J|\n",
      "+---+-------+----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lead_lag.withColumn(\"res\", \n",
    "        when(col('id') % 2 == 0, col(\"lag\"))\\\n",
    "        .when(col(\"id\")%2 != 0, when(col('lead') == \"null\", col(\"student\"))\\\n",
    "              .otherwise(col(\"lead\")))\\\n",
    "                   ).show()\n",
    "\n",
    "max_el = lead_lag.selectExpr(\"max(id)\").collect()[0][0]\n",
    "print(max_el)\n",
    "\n",
    "lead_lag.withColumn(\"res\", \n",
    "        when(col('id') % 2 == 0, col(\"lag\"))\\\n",
    "        .when(col(\"id\")%2 != 0, when(col('id') == max_el , col(\"student\"))\\\n",
    "              .otherwise(col(\"lead\")))\\\n",
    "                   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e74bc638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|STUDENT|\n",
      "+---+-------+\n",
      "|  1|      D|\n",
      "|  2|      A|\n",
      "|  3|      G|\n",
      "|  4|      E|\n",
      "|  5|      J|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using sql\n",
    "\n",
    "df.createOrReplaceTempView(\"seats_tbl\")\n",
    "spark.sql(\"select \\\n",
    "            case when mod(ID,2) = 0 then ID-1 \\\n",
    "                  when mod(ID,2) <> 0 and ID <> (select Max(ID) \\\n",
    "                from seats_tbl) then ID+1 else ID end as ID, \\\n",
    "                  STUDENT from seats_tbl order by 1;\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "905f5fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------+\n",
      "|name|trans_id|           date_time|\n",
      "+----+--------+--------------------+\n",
      "|   D|    8888|2023-01-01 08:22:...|\n",
      "|   A|      55|2023-01-02 16:12:...|\n",
      "|   D|      22|2023-01-03 14:02:...|\n",
      "|   R|      77|2023-01-04 20:22:...|\n",
      "|   H|      33|2023-01-02 19:30:...|\n",
      "|   H|     789|2023-01-02 10:22:...|\n",
      "|   I|     654|2023-01-03 00:12:...|\n",
      "|   P|    4489|2023-01-04 00:22:...|\n",
      "|   A|    2145|2023-01-02 15:22:...|\n",
      "+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/mahaboob-pathan_namastesql-sqlquery-sqlchallenges-activity-7113004314930933760-gXk8\n",
    "\n",
    "# Write a query to get 𝒕𝒉𝒆 𝒏𝒐 𝒐𝒇 𝒖𝒏𝒊𝒒𝒖𝒆 𝒏𝒂𝒎𝒆𝒔, 𝒏𝒐 𝒐𝒇 𝒕𝒓𝒂𝒏𝒔𝒂𝒄𝒕𝒊𝒐𝒏𝒔, 𝒂𝒏𝒅 𝒕𝒉𝒆 𝒅𝒊𝒇𝒇𝒆𝒓𝒆𝒏𝒄𝒆 𝒃𝒆𝒕𝒘𝒆𝒆𝒏 𝒕𝒉𝒆 𝒇𝒊𝒓𝒔𝒕 & \n",
    "# 𝒍𝒂𝒔𝒕 𝒕𝒓𝒂𝒏𝒔𝒂𝒄𝒕𝒊𝒐𝒏 𝒐𝒄𝒄𝒖𝒓𝒆𝒅 𝒐𝒏 02-01-2023.\n",
    "\n",
    "\n",
    "schema_= [\"name\", \"trans_id\", \"date_time\"]\n",
    "\n",
    "data = [\n",
    "('D', 8888, '2023-01-01 08:22:13.053'),\n",
    "('A', 55, '2023-01-02 16:12:18.023'),\n",
    "('D', 22, '2023-01-03 14:02:13.053'),\n",
    "('R', 77, '2023-01-04 20:22:33.053'),\n",
    "('H', 33, '2023-01-02 19:30:10.015'),\n",
    "('H', 789, '2023-01-02 10:22:13.053'),\n",
    "('I', 654, '2023-01-03 00:12:13.023'),\n",
    "('P', 4489, '2023-01-04 00:22:15.013'),\n",
    "('A', 2145, '2023-01-02 15:22:13.053')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e6a1400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------+\n",
      "|name|trans_id|           date_time|\n",
      "+----+--------+--------------------+\n",
      "|   D|    8888|2023-01-01 08:22:...|\n",
      "|   A|      55|2023-01-02 16:12:...|\n",
      "|   D|      22|2023-01-03 14:02:...|\n",
      "|   R|      77|2023-01-04 20:22:...|\n",
      "|   H|      33|2023-01-02 19:30:...|\n",
      "|   H|     789|2023-01-02 10:22:...|\n",
      "|   I|     654|2023-01-03 00:12:...|\n",
      "|   P|    4489|2023-01-04 00:22:...|\n",
      "|   A|    2145|2023-01-02 15:22:...|\n",
      "+----+--------+--------------------+\n",
      "\n",
      "+----+--------+--------------------+\n",
      "|name|trans_id|           date_time|\n",
      "+----+--------+--------------------+\n",
      "|   D|    8888|2023-01-01 08:22:...|\n",
      "|   A|      55|2023-01-02 16:12:...|\n",
      "|   D|      22|2023-01-03 14:02:...|\n",
      "|   R|      77|2023-01-04 20:22:...|\n",
      "|   H|      33|2023-01-02 19:30:...|\n",
      "|   H|     789|2023-01-02 10:22:...|\n",
      "|   I|     654|2023-01-03 00:12:...|\n",
      "|   P|    4489|2023-01-04 00:22:...|\n",
      "|   A|    2145|2023-01-02 15:22:...|\n",
      "+----+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11 = df.withColumn(\"date\", to_date(col(\"date_time\")))\n",
    "df.show()\n",
    "\n",
    "df12 = df11.filter(col(\"date\") == \"2023-01-02\").drop(\"date\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "192d1d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------------------+\n",
      "|name|trans_id|date_time              |\n",
      "+----+--------+-----------------------+\n",
      "|H   |789     |2023-01-02 10:22:13.053|\n",
      "|A   |2145    |2023-01-02 15:22:13.053|\n",
      "|A   |55      |2023-01-02 16:12:18.023|\n",
      "|H   |33      |2023-01-02 19:30:10.015|\n",
      "+----+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df12.orderBy(\"date_time\")\n",
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f70b8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------------------+-------+\n",
      "|name|trans_id|           date_time|minutes|\n",
      "+----+--------+--------------------+-------+\n",
      "|   H|     789|2023-01-02 10:22:...|    622|\n",
      "|   A|    2145|2023-01-02 15:22:...|    922|\n",
      "|   A|      55|2023-01-02 16:12:...|    972|\n",
      "|   H|      33|2023-01-02 19:30:...|   1170|\n",
      "+----+--------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"minutes\", ((hour(\"date_time\")*60) + minute(\"date_time\") + (second(\"date_time\")/60)).cast(\"Integer\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e754646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+---------+\n",
      "|unique_names|no_of_transactions|mins_diff|\n",
      "+------------+------------------+---------+\n",
      "|           2|                 4|      548|\n",
      "+------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.selectExpr(\"count(distinct name) as unique_names\", \"count(distinct trans_id) as no_of_transactions\", \"max(minutes) - min(minutes) as mins_diff\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9992379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+---------+\n",
      "|unique_names|no_of_transactions|mins_diff|\n",
      "+------------+------------------+---------+\n",
      "|           2|                 4|      547|\n",
      "+------------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using SQL\n",
    "\n",
    "df.createOrReplaceTempView(\"time_diff\")\n",
    "sql_data = spark.sql(\"select count(distinct name) as unique_names,count(trans_id) as no_of_transactions,\\\n",
    "                datediff(minute,min(date_time),max(date_time)) as mins_diff\\\n",
    "                from time_diff where date(date_time)='2023-01-02';\")\n",
    "\n",
    "sql_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0896cf",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7114235881678417920/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114235881678417920%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "Topic: Normalization vs. Denormalization\n",
    "\n",
    "In the SQL world, we often find ourselves at the crossroads of data optimization. \n",
    "\n",
    "🚦 Let's demystify two key strategies: Normalization and Denormalization! 📊\n",
    "\n",
    "<hr>\n",
    "\n",
    "🧩 Normalization 🧩\n",
    "Normalization is like tidying up your data. It's about breaking down complex tables into smaller, related tables to minimize data redundancy. \n",
    "\n",
    "Think of it as Marie Kondo-ing your database. 🧹✨\n",
    "\n",
    "<hr>\n",
    "\n",
    "Pros:\n",
    "Reduces data duplication 📉\n",
    "Ensures data integrity 🔐\n",
    "Simplifies updates and maintenance 🛠️\n",
    "\n",
    "Cons:\n",
    "May require more complex queries 😓\n",
    "Can lead to increased JOIN operations ⚙️\n",
    "\n",
    "<hr>\n",
    "\n",
    "🔥 Denormalization 🔥\n",
    "Denormalization is like packing your suitcase efficiently. It involves adding redundancy to your data to improve query performance. It's like prepping for a quick getaway! 🧳💨\n",
    "\n",
    "<hr>\n",
    "\n",
    "Pros:\n",
    "Speeds up query retrieval ⚡\n",
    "Simplifies complex queries 🤓\n",
    "Reduces JOIN operations 🚀\n",
    "\n",
    "Cons:\n",
    "Increases data storage 📦\n",
    "Risks data inconsistency 🤯\n",
    "May require more effort to maintain 🧹\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D4D22AQF0V035D1KSSQ/feedshare-shrink_2048_1536/0/1696166009252?e=1707350400&v=beta&t=giafRUhlKYecte5YnvK2vhPk9_pUJ7zTYKFPrFMYrIo\" alt=\"Alternative text\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5dd83",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7114530651709554688/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114530651709554688%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "Handling late-arriving Data using PySpark:\n",
    "\n",
    "**Streaming Pipeline - Handling Late-Arriving Data:**\n",
    "\n",
    "In a streaming pipeline, handling late data often involves setting watermarks and windowing to accommodate data with delayed arrival.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LateDataStreaming\").getOrCreate()\n",
    "\n",
    "# Define a schema for the streaming data\n",
    "schema = StructType([\n",
    "  StructField(\"id\", IntegerType()),\n",
    "  StructField(\"value\", StringType()),\n",
    "  StructField(\"timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "# Read data from a streaming source\n",
    "streaming_df = (spark.readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "  .option(\"subscribe\", \"my_topic\")\n",
    "  .load()\n",
    "  .selectExpr(\"CAST(value AS STRING) as value\")\n",
    "  .select(from_json(\"value\", schema).alias(\"data\"))\n",
    "  .select(\"data.*\")\n",
    "  .withWatermark(\"timestamp\", \"30 seconds\") # Set a watermark for late data handling\n",
    ")\n",
    "\n",
    "# Perform windowed aggregation on the streaming data\n",
    "result = streaming_df.groupBy(window(\"timestamp\", \"10 minutes\"), \"id\").count()\n",
    "\n",
    "# Start the streaming query\n",
    "query = (result.writeStream\n",
    "  .outputMode(\"complete\")\n",
    "  .format(\"console\")\n",
    "  .start())\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Here, we set a watermark to specify that data arriving more than 30 seconds late should be considered late and excluded from processing.\n",
    "\n",
    "**Batch Pipeline - Handling Late-Arriving Data:**\n",
    "\n",
    "In a batch-processing pipeline, handling late data typically involves comparing timestamps and joining data from different sources. Here's a batch processing example using PySpark's DataFrame API:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"LateDataBatch\").getOrCreate()\n",
    "\n",
    "# Load current and late-arriving data from different sources\n",
    "current_data = spark.read.csv(\"current_data.csv\", header=True)\n",
    "late_arriving_data = spark.read.csv(\"late_data.csv\", header=True)\n",
    "\n",
    "# Cast timestamp columns to TimestampType\n",
    "current_data = current_data.withColumn(\"timestamp\", current_data[\"timestamp\"].cast(\"timestamp\"))\n",
    "late_arriving_data = late_arriving_data.withColumn(\"timestamp\", late_arriving_data[\"timestamp\"].cast(\"timestamp\"))\n",
    "\n",
    "# Handle late data by joining and selecting the latest version\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "joined_data = (current_data\n",
    "  .union(late_arriving_data) # Combine current and late-arriving data\n",
    "  .groupBy(\"id\")\n",
    "  .agg(\n",
    "    col(\"id\"),\n",
    "    col(\"value\"),\n",
    "    col(\"timestamp\"),\n",
    "    col(\"source\"),\n",
    "    col(\"processing_date\"),\n",
    "    col(\"source\").alias(\"latest_source\")\n",
    "  )\n",
    "  .orderBy(\"timestamp\", ascending=False)\n",
    "  .dropDuplicates([\"id\"])\n",
    "  .select(\"id\", \"value\", \"timestamp\", \"latest_source\")\n",
    ")\n",
    "\n",
    "# Show the final data\n",
    "joined_data.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1171599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd9611d",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7110481458405785600/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110481458405785600%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "𝐇𝐨𝐰 𝐒𝐩𝐚𝐫𝐤 𝐝𝐞𝐜𝐢𝐝𝐞𝐬 𝐨𝐧 𝐭𝐡𝐞 𝐉𝐨𝐢𝐧 𝐒𝐭𝐫𝐚𝐭𝐞𝐠𝐲?\n",
    "\n",
    "Spark uses a cost-based optimizer to choose the most efficient join strategy based on the size of the datasets, the type of join, the join condition, the distribution of the data, and the availability of resources.\n",
    "\n",
    "The three most common join strategies in Spark are:\n",
    "1. Broadcast join\n",
    "2. Shuffle hash join\n",
    "3. Sort merge join\n",
    "\n",
    "Read the blog to learn more about how Apache Spark decides on the join strategy and how to use join hints effectively: https://lnkd.in/g3EvZZfC\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D5622AQEXURa3p83uJg/feedshare-shrink_800/0/1695270885416?e=1707350400&v=beta&t=6kdv6dlKTVKxsttL7y0hx6eXZdJX40szMd7Q8y7AsVY\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8e3c6",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7114818781520048128/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114818781520048128%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "Different output modes in PySpark structured streaming, along with justifications for their use cases:\n",
    "\n",
    "**1. Output Mode: `append`**\n",
    "  - Justification: Use this mode when you want to capture and store only the new rows added to the result table. This is suitable for scenarios where you are processing data that consists of unique records, and you are interested in preserving only the new data.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AppendOutputModeExample\").getOrCreate()\n",
    "\n",
    "# Sample streaming source (replace with your actual source)\n",
    "stream_data = spark.readStream.format(\"CSV\").load(\"path_to_source\")\n",
    "\n",
    "# Transformation and aggregation operations (e.g., grouping, filtering)\n",
    "result_df = stream_data.groupBy(\"key\").count()\n",
    "\n",
    "query = result_df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**2. Output Mode: `complete`**\n",
    "  - Justification: Use this mode when you want to write the entire result table to the output every time there is a trigger. This is suitable for scenarios where you need the complete result set, including all historic and new data.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CompleteOutputModeExample\").getOrCreate()\n",
    "\n",
    "# Sample streaming source (replace with your actual source)\n",
    "stream_data = spark.readStream.format(\"CSV\").load(\"path_to_source\")\n",
    "\n",
    "# Transformation and aggregation operations (e.g., grouping, filtering)\n",
    "result_df = stream_data.groupBy(\"key\").count()\n",
    "\n",
    "query = result_df.writeStream.outputMode(\"complete\").format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**3. Output Mode: `update`**\n",
    "  - Justification: Use this mode when you want to capture and store only the rows that have been updated in the result table. It's suitable for scenarios where you want to track changes to existing records in the result table.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"UpdateOutputModeExample\").getOrCreate()\n",
    "\n",
    "# Sample streaming source (replace with your actual source)\n",
    "stream_data = spark.readStream.format(\"CSV\").load(\"path_to_source\")\n",
    "\n",
    "# Transformation and aggregation operations (e.g., grouping, filtering)\n",
    "result_df = stream_data.groupBy(\"key\").count()\n",
    "\n",
    "query = result_df.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "In each of the examples above, the `format(\"console\")` is used to print the results to the console for demonstration purposes. You can replace it with other output formats such as \"parquet,\" \"delta,\" or \"memory\" for further processing or storage. The choice of output mode depends on your specific use case and what kind of data you want to capture and store as part of your streaming application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd83ea3",
   "metadata": {},
   "source": [
    "Resources to Prepare:\n",
    "1. DataLemur (Really good questions from product based companies but edge cases not covered). [https://datalemur.com/]\n",
    "2. LeetCode (the classic). [https://leetcode.com/]\n",
    "3. 8 week SQL challenge. [https://lnkd.in/g9KxBFem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29aa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "67a34153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "None\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115546144084037633/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115546144084037633%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# write a code in python for finding the first non repeating character in string.\n",
    "\n",
    "# Method 1\n",
    "\n",
    "def first_non_repeating_char(str):\n",
    "    char_order = []\n",
    "    char_dict = {}\n",
    "    \n",
    "    for c in str:\n",
    "        if c in char_dict:\n",
    "            char_dict[c] += 1\n",
    "        else:\n",
    "            char_dict[c] = 1\n",
    "            \n",
    "        char_order.append(c)\n",
    "        \n",
    "    for c in char_order:\n",
    "        if char_dict[c] == 1:\n",
    "            return c\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "# Test cases\n",
    "str1 = \"abcabcbbd\"\n",
    "print(first_non_repeating_char(str1))\n",
    "\n",
    "str1 = \"aabbcc\"\n",
    "print(first_non_repeating_char(str1))\n",
    "\n",
    "str1 = \"abc\"\n",
    "print(first_non_repeating_char(str1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "10bd8042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n",
      "None\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# Method 2\n",
    "\n",
    "def first_non_repeating_char(str):\n",
    " for letter in str:\n",
    "  if str.count(letter)==1:\n",
    "   return letter\n",
    "\n",
    "\n",
    "# Test cases\n",
    "str1 = \"abcabcbbd\"\n",
    "print(first_non_repeating_char(str1))\n",
    "str.\n",
    "\n",
    "str1 = \"aabbcc\"\n",
    "print(first_non_repeating_char(str1))\n",
    "\n",
    "str1 = \"abc\"\n",
    "print(first_non_repeating_char(str1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c346dbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+-------------------+\n",
      "|userId|productId|quantity|       purchaseDate|\n",
      "+------+---------+--------+-------------------+\n",
      "|   827|     2452|      45|04-09-2022 00:00:00|\n",
      "|   333|     1122|       9|06-02-2022 01:00:00|\n",
      "|   333|     1122|      10|06-02-2022 02:00:00|\n",
      "|   536|     3223|       6|01-11-2022 12:33:44|\n",
      "|   827|     3585|      35|02-20-2022 14:05:26|\n",
      "|   536|     3223|       5|03-02-2022 09:33:28|\n",
      "|   536|     1435|      10|03-02-2022 08:40:00|\n",
      "+------+---------+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115588607486156801/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115588607486156801%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Find the users who purchased 2 or more products on different dates.\n",
    "\n",
    "schema_ = [\"userId\", \"productId\", \"quantity\", \"purchaseDate\"]\n",
    "data = [(827, 2452, 45, \"04-09-2022 00:00:00\"),\n",
    "(333, 1122, 9, \"06-02-2022 01:00:00\"),\n",
    "(333, 1122, 10, \"06-02-2022 02:00:00\"),\n",
    "(536, 3223, 6, \"01-11-2022 12:33:44\"),\n",
    "(827, 3585, 35, \"02-20-2022 14:05:26\"),\n",
    "(536, 3223, 5, \"03-02-2022 09:33:28\"),\n",
    "(536, 1435, 10, \"03-02-2022 08:40:00\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3341bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+\n",
      "|userId|productId|count|\n",
      "+------+---------+-----+\n",
      "|   827|     2452|    1|\n",
      "|   333|     1122|    2|\n",
      "|   536|     1435|    1|\n",
      "|   827|     3585|    1|\n",
      "|   536|     3223|    2|\n",
      "+------+---------+-----+\n",
      "\n",
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|   333|\n",
      "|   536|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupData = df.groupBy(\"userId\", \"productId\").agg(count_distinct(col(\"purchaseDate\")).alias(\"count\"))\n",
    "groupData.show()\n",
    "\n",
    "filterdata = groupData.filter(col(\"count\") > 1).select(\"userId\")\n",
    "filterdata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0ce2a",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7108623025624809472/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7108623025624809472%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%2\n",
    "\n",
    "Question : use of StructType and StructField classes in PySpark\n",
    "\n",
    "1. StructType: Defining Data Structure\n",
    "StructType acts as a blueprint for creating structured data. It allows us to define a schema by specifying a sequence of StructField objects. Each StructField represents a column with a name, data type, and an optional flag indicating nullability.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructTypeDemo\").getOrCreate()\n",
    "\n",
    "# Define schema using StructType and StructFields\n",
    "schema = StructType([\n",
    " StructField(\"id\", IntegerType(), False),\n",
    " StructField(\"name\", StringType(), True),\n",
    " StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "##### Create an empty DataFrame with defined schema\n",
    "df = spark.createDataFrame([], schema)\n",
    "\n",
    "\n",
    "2. StructField: Column Specification\n",
    "StructField helps us specify the characteristics of each column. Here's how we can use it to define a custom data structure.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "##### Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StructFieldDemo\").getOrCreate()\n",
    "\n",
    "##### Define a single StructField representing a person's name\n",
    "name_field = StructField(\"name\", StringType(), True)\n",
    "\n",
    "##### Create a schema with the defined StructField\n",
    "schema = StructType([name_field])\n",
    "\n",
    "##### Create an empty DataFrame with the defined schema\n",
    "df = spark.createDataFrame([], schema)\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa2d590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|description                        |\n",
      "+-----------------------------------+\n",
      "|Product A: $19.99!                 |\n",
      "|Special Offer on Product B - $29.95|\n",
      "|Product C (Limited Stock)          |\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115644296111783936/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115644296111783936%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data = [(\"Product A: $19.99!\",),\n",
    "  (\"Special Offer on Product B - $29.95\",),\n",
    "  (\"Product C (Limited Stock)\",)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"description\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d60474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------------------------------+\n",
      "|description                        |cleaned_description             |\n",
      "+-----------------------------------+--------------------------------+\n",
      "|Product A: $19.99!                 |Product A 1999                  |\n",
      "|Special Offer on Product B - $29.95|Special Offer on Product B  2995|\n",
      "|Product C (Limited Stock)          |Product C Limited Stock         |\n",
      "+-----------------------------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean and preprocess the descriptions using regex_replace\n",
    "cleaned_df = df.withColumn(\"cleaned_description\",\n",
    "       regexp_replace(col(\"description\"), r'[^a-zA-Z0-9\\s]', ''))\n",
    "cleaned_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a78213",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7115697833059491840/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115697833059491840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "## STREAMING DATA PIPELINE USING PYSPARK:\n",
    "Here, we use Spark's structured streaming mechanism to load data in real-time by pulling data from a cloud/delta lake source and applying the necessary transformation, CRUD logic with a merge operation.\n",
    "\n",
    "#### import required libraries\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_date, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "```\n",
    "\n",
    "#### Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"StreamingLoad\").getOrCreate()\n",
    "\n",
    "#### Define the source and target directories as Delta tables\n",
    "```\n",
    "source_path = \"hdfs://path/to/source\"\n",
    "target_path = \"hdfs://path/to/target\"\n",
    "checkpoint_path = \"hdfs://path/to/checkpoint\" # Define a checkpoint directory\n",
    "```\n",
    "\n",
    "#### Load the existing data as a Delta table\n",
    "```\n",
    "existing_data = DeltaTable.forPath(spark, target_path)\n",
    "```\n",
    "\n",
    "#### Define the watermark date (e.g., 1 day before the current date)\n",
    "```\n",
    "watermark_date = current_date() - timedelta(days=1)\n",
    "```\n",
    "\n",
    "#### Function to process and merge streaming data\n",
    "```\n",
    "def process_streaming_data(existing_data, batch_df):\n",
    "  existing_data.alias(\"existing\").merge(\n",
    "    batch_df.alias(\"new\"), \n",
    "    (f\"existing.date_column = new.date_column and new.date_column >= '{watermark_date}'\")\n",
    "  ).whenMatchedUpdateAll().whenNotMatchedInsertAll().whenNotMatchedDelete().execute()\n",
    "```\n",
    "\n",
    "#### Define a streaming query to continuously read and process new data\n",
    "```\n",
    "streaming_query = spark.readStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .schema(existing_data.toDF().schema) \\\n",
    "  .option(\"path\", source_path) \\\n",
    "  .load() \\\n",
    "  .filter(col(\"date_column\") > watermark_date) \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .foreachBatch(lambda batch_df, batch_id: process_streaming_data(existing_data, batch_df)) \\\n",
    "  .option(\"checkpointLocation\", checkpoint_path) # Specify checkpoint directory\n",
    "  .start()\n",
    "  ```\n",
    "\n",
    "#### Await the termination of the streaming query\n",
    "```\n",
    "streaming_query.awaitTermination()\n",
    "```\n",
    "\n",
    "#### Stop the Spark session\n",
    "```\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef9754",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7115373323517145088/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115373323517145088%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "PYSPARK REST API INCREMENTAL DATA PIPELINE:\n",
    "Please note that this is a complex task, and you might need to adapt it to your specific requirements.\n",
    "\n",
    "Here's a step-by-step guide:\n",
    "\n",
    "1. **Import Required Libraries**\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import DeltaTable\n",
    "import requests\n",
    "```\n",
    "\n",
    "2. **Initialize Spark Session**\n",
    "```\n",
    "spark = SparkSession.builder.appName(\"IncrementalDataLoad\").getOrCreate()\n",
    "```\n",
    "\n",
    "3. **Define Your API Endpoint and Parameters**\n",
    "\n",
    "```\n",
    "api_url = \"https://your.api.endpoint\"\n",
    "api_params = {\n",
    " \"start_date\": \"2023-01-01\", # Define your start date\n",
    " \"end_date\": \"2023-01-31\", # Define your end date\n",
    "}\n",
    "```\n",
    "\n",
    "4. **Make API Request and Load Data**\n",
    "\n",
    "```\n",
    "response = requests.get(api_url, params=api_params)\n",
    "if response.status_code == 200:\n",
    " api_data = response.json()\n",
    " df = spark.createDataFrame(api_data)\n",
    "else:\n",
    " print(\"Failed to fetch data from the API.\")\n",
    "```\n",
    "\n",
    "5. **Initialize Delta Lake Table**\n",
    "\n",
    "```\n",
    "delta_table = DeltaTable.forPath(spark, \"your/delta/table/path\")\n",
    "```\n",
    "6. **Check for Existing Data in Delta Table**\n",
    "\n",
    "```\n",
    "if delta_table is not None:\n",
    " existing_data = delta_table.toDF()\n",
    "else:\n",
    " existing_data = spark.createDataFrame([])\n",
    "```\n",
    "\n",
    "7. **Schema Evolution and Merge Data**\n",
    "Assuming that schema evolution can occur over time, you can use Delta Lake's schema evolution feature to handle changes in the schema.\n",
    "\n",
    "```python\n",
    "\n",
    "from delta import DeltaMergeBuilder\n",
    "\n",
    "# Define the merge condition (e.g., using primary keys)\n",
    "\n",
    "merge_condition = \"source_data.id = existing_data.id\"\n",
    "```\n",
    "\n",
    "#### Merge the data using schema evolution\n",
    "```\n",
    "delta_merge = DeltaMergeBuilder() \\\n",
    " .on(matched=merge_condition) \\\n",
    " .whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute(\n",
    "  target=delta_table,\n",
    "  source=df.alias(\"source_data\"),\n",
    "  condition=merge_condition\n",
    " )\n",
    " ```\n",
    "\n",
    "8. **Optimize the Delta Table (Optional)**\n",
    "You can run an optimization to improve query performance, especially if your table gets large over time.\n",
    "```\n",
    "delta_table.vacuum(0.001)\n",
    "```\n",
    "\n",
    "9. **Stop the Spark Session**\n",
    "```\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "10. **Schedule the Script**\n",
    "You can schedule this script to run at regular intervals to fetch incremental data and update the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115546153542172672/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115546153542172672%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "To create an incremental data pipeline with Delta Lake using PySpark, you'll need to follow these general steps:\n",
    "\n",
    "1. **Initialize the SparkSession**:\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "\n",
    "  spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Lake Incremental Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "  ```\n",
    "\n",
    "2. **Load Existing Delta Table**:\n",
    "  Load the existing Delta table if it already exists, or create a new one if it doesn't.\n",
    "  ```python\n",
    "  existing_data = spark.read.format(\"delta\").table(\"existing_delta_table\")\n",
    "  ```\n",
    "\n",
    "3. **Load New Data**:\n",
    "  Load new data that you want to append to the existing Delta table.\n",
    "  ```python\n",
    "  new_data = spark.read.format(\"parquet\").load(\"path_to_new_data\")\n",
    "  ```\n",
    "\n",
    "4. **Identify Changes**:\n",
    "  Identify the changes (updates or inserts) between the existing Delta table and the new data.\n",
    "  ```python\n",
    "  changes = new_data.subtract(existing_data)\n",
    "  ```\n",
    "\n",
    "5. **Apply Changes**:\n",
    "  Apply the changes to the existing Delta table. You can use `union` or `merge` operations depending on your use case.\n",
    "  \n",
    "  For example, using `union` to append new data:\n",
    "  ```python\n",
    "  combined_data = existing_data.union(changes)\n",
    "  combined_data.write.format(\"delta\").mode(\"overwrite\").save(\"path_to_existing_delta_table\")\n",
    "  ```\n",
    "\n",
    "  Or using `merge` for more complex scenarios (like updating existing records):\n",
    "  ```python\n",
    "  from delta.tables import DeltaTable\n",
    "\n",
    "  delta_table = DeltaTable.forPath(spark, \"path_to_existing_delta_table\")\n",
    "\n",
    "  delta_table.alias(\"oldData\") \\\n",
    "    .merge(\n",
    "      source=changes.alias(\"newData\"),\n",
    "      condition=\"oldData.primary_key = newData.primary_key\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "  ```\n",
    "\n",
    "6. **Optimize Delta Table (Optional)**:\n",
    "  Delta Lake allows for optimizations like Z-Ordering, optimizing layout, and vacuuming old files. These steps can help improve performance and reduce storage.\n",
    "  \n",
    "  ```python\n",
    "  delta_table.vacuum()\n",
    "  delta_table.optimize()\n",
    "  ```\n",
    "\n",
    "7. **Close SparkSession**:\n",
    "  After finishing the pipeline, it's good practice to stop the SparkSession.\n",
    "  ```python\n",
    "  spark.stop()\n",
    "  ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f763a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+\n",
      "|employee_id|entry_details|   time_stamp_detail|\n",
      "+-----------+-------------+--------------------+\n",
      "|       1000|        login|2023-06-16 01:00:...|\n",
      "|       1000|        login|2023-06-16 02:00:...|\n",
      "|       1000|        login|2023-06-16 03:00:...|\n",
      "|       1000|       logout|2023-06-16 12:00:...|\n",
      "|       1001|        login|2023-06-16 01:00:...|\n",
      "|       1001|        login|2023-06-16 02:00:...|\n",
      "|       1001|        login|2023-06-16 03:00:...|\n",
      "|       1001|       logout|2023-06-16 12:00:...|\n",
      "+-----------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7108420432977940480/\n",
    "\n",
    "data = [(1000, 'login', '2023-06-16 01:00:15.34'),\n",
    "    (1000, 'login', '2023-06-16 02:00:15.34'),\n",
    "    (1000, 'login', '2023-06-16 03:00:15.34'),\n",
    "    (1000, 'logout', '2023-06-16 12:00:15.34'),\n",
    "    (1001, 'login', '2023-06-16 01:00:15.34'),\n",
    "    (1001, 'login', '2023-06-16 02:00:15.34'),\n",
    "    (1001, 'login', '2023-06-16 03:00:15.34'),\n",
    "    (1001, 'logout', '2023-06-16 12:00:15.34')]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"employee_id\",\"entry_details\",\"time_stamp_detail\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adfdd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------------------+----------+\n",
      "|employee_id|entry_details|   time_stamp_detail|      date|\n",
      "+-----------+-------------+--------------------+----------+\n",
      "|       1000|        login|2023-06-16 01:00:...|2023-06-16|\n",
      "|       1000|        login|2023-06-16 02:00:...|2023-06-16|\n",
      "|       1000|        login|2023-06-16 03:00:...|2023-06-16|\n",
      "|       1000|       logout|2023-06-16 12:00:...|2023-06-16|\n",
      "|       1001|        login|2023-06-16 01:00:...|2023-06-16|\n",
      "|       1001|        login|2023-06-16 02:00:...|2023-06-16|\n",
      "|       1001|        login|2023-06-16 03:00:...|2023-06-16|\n",
      "|       1001|       logout|2023-06-16 12:00:...|2023-06-16|\n",
      "+-----------+-------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"date\", to_date(\"time_stamp_detail\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89e36b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+----------+----------+----------+--------+---------+----------+---------+\n",
      "|date      |months_between|add_months|sub_months|date_add  |date_sub  |datediff|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+--------------+----------+----------+----------+----------+--------+---------+----------+---------+\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "|2023-06-16|6.5483871     |2023-11-16|2023-01-16|2023-06-19|2023-06-13|200     |6        |16        |167      |\n",
      "+----------+--------------+----------+----------+----------+----------+--------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.select(col(\"date\"),months_between(current_date(),col(\"date\")).alias(\"months_between\"),\\\n",
    "         add_months(col(\"date\"),5).alias(\"add_months\"),\\\n",
    "         add_months(col(\"date\"),-5).alias(\"sub_months\"),\\\n",
    "         date_add(col(\"date\"),3).alias(\"date_add\"),\\\n",
    "         date_sub(col(\"date\"),3).alias(\"date_sub\"),\\\n",
    "         datediff(current_date(),col(\"date\")).alias(\"datediff\"),\\\n",
    "         dayofweek(col(\"date\")).alias(\"dayofweek\"),\\\n",
    "         dayofmonth(col(\"date\")).alias(\"dayofmonth\"),\\\n",
    "         dayofyear(col(\"date\")).alias(\"dayofyear\"))\n",
    "\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e4c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+----------+\n",
      "| ID|COUNTRY|   STATE|AMOUNT|TRANS_DATE|\n",
      "+---+-------+--------+------+----------+\n",
      "|121|     US|approved|  1000|2018-12-18|\n",
      "|122|     US|declined|  2000|2018-12-19|\n",
      "|123|     US|approved|  2000|2019-01-01|\n",
      "|124|     DE|approved|  2000|2019-01-07|\n",
      "+---+-------+--------+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7116106222910550016/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116106222910550016%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"ID\", \"COUNTRY\", \"STATE\", \"AMOUNT\", \"TRANS_DATE\"]\n",
    "\n",
    "data = [(121, 'US', 'approved', 1000, '2018-12-18'),\n",
    "  (122, 'US', 'declined', 2000, '2018-12-19'),\n",
    "  (123, 'US', 'approved', 2000, '2019-01-01'),\n",
    "  (124, 'DE', 'approved', 2000, '2019-01-07')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05b14e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+------+----------+----------+\n",
      "| ID|COUNTRY|   STATE|AMOUNT|TRANS_DATE|Year_month|\n",
      "+---+-------+--------+------+----------+----------+\n",
      "|121|     US|approved|  1000|2018-12-18|   2018-12|\n",
      "|122|     US|declined|  2000|2018-12-19|   2018-12|\n",
      "|123|     US|approved|  2000|2019-01-01|   2019-01|\n",
      "|124|     DE|approved|  2000|2019-01-07|   2019-01|\n",
      "+---+-------+--------+------+----------+----------+\n",
      "\n",
      "+---+-------+--------+------+----------+----------+--------+--------+---------------+\n",
      "| ID|COUNTRY|   STATE|AMOUNT|TRANS_DATE|Year_month|Approved|Declined|approved_amount|\n",
      "+---+-------+--------+------+----------+----------+--------+--------+---------------+\n",
      "|121|     US|approved|  1000|2018-12-18|   2018-12|       1|       0|           1000|\n",
      "|122|     US|declined|  2000|2018-12-19|   2018-12|       0|       1|              0|\n",
      "|123|     US|approved|  2000|2019-01-01|   2019-01|       1|       0|           2000|\n",
      "|124|     DE|approved|  2000|2019-01-07|   2019-01|       1|       0|           2000|\n",
      "+---+-------+--------+------+----------+----------+--------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"Year_month\", date_format(col(\"TRANS_DATE\"), \"yyyy-MM\"))\n",
    "df1.show()\n",
    "\n",
    "df2 = df1.withColumn(\"Approved\", expr(\"case when state = 'approved' then 1 else 0 end\"))\\\n",
    "            .withColumn(\"Declined\", expr(\"case when state = 'declined' then 1 else 0 end\"))\\\n",
    "            .withColumn(\"approved_amount\", col(\"Approved\") * col(\"AMOUNT\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d681846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----------+--------------+--------------+------------------+---------------------+\n",
      "|Year_month|Country|trans_count|Approved_count|Declined_count|trans_total_amount|approved_Total_amount|\n",
      "+----------+-------+-----------+--------------+--------------+------------------+---------------------+\n",
      "|   2018-12|     US|          2|             1|             1|              3000|                 1000|\n",
      "|   2019-01|     US|          1|             1|             0|              2000|                 2000|\n",
      "|   2019-01|     DE|          1|             1|             0|              2000|                 2000|\n",
      "+----------+-------+-----------+--------------+--------------+------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.groupBy(\"Year_month\", \"Country\").agg(count(col(\"year_month\")).alias(\"trans_count\"),\n",
    "                                          sum(col(\"Approved\")).alias(\"Approved_count\"),\n",
    "                                        sum(col(\"Declined\")).alias(\"Declined_count\"),\n",
    "                                        sum(col(\"AMOUNT\")).alias(\"trans_total_amount\"),\n",
    "                                        sum(col(\"approved_amount\")).alias(\"approved_Total_amount\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00307dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SQL\n",
    "\n",
    "# df.createOrReplaceTempView(\"transactions\")\n",
    "# spark.sql(\"select FORMAT(TRANS_DATE, 'yyyy-MM') AS YearMonth, \\\n",
    "#         country,\\\n",
    "#         COUNT(ID) as transaction_count, \\\n",
    "#         sum(case when state = 'approved' then 1 else 0 end) as approved_count,\\\n",
    "#         sum(case when state = 'declined' then 1 else 0 end) as declined_count,\\\n",
    "#         SUM(amount) as Transaction_total_amount,\\\n",
    "#         SUM(case when state = 'approved' then amount else 0 end) as Approved_total_amount\\\n",
    "#         from transactions\\\n",
    "#         group by FORMAT(TRANS_DATE, 'yyyy-MM'), country\\\n",
    "#         order by YearMonth\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a94ca54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bd224fe",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7116006679380443136/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116006679380443136%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "## Topic : The 3 Modes for Handling Corrupt Data in PySpark! 🛡️💡\n",
    "\n",
    "In the world of data, quality reigns supreme. To harness the true potential of your data, PySpark offers three essential modes for gracefully managing corrupt records. Let's dive in!\n",
    "\n",
    "#### 1. Permissive Mode: 🛡️\n",
    "```\n",
    "When data gets messy, Permissive Mode is your knight in shining armor! 🚀\n",
    "Set it to True, and PySpark will read corrupt records while storing them as null values in the DataFrame. \n",
    "This lenient approach salvages valid data, ensuring your analysis continues smoothly.\n",
    "Ideal for scenarios where a few corrupted records won't stand in the way of your insights.\n",
    "\n",
    "spark.read.option(\"mode\", \"permissive\").csv(\"sample.csv\")\n",
    "```\n",
    "\n",
    "\n",
    "#### 2. Drop Malformed Mode: 🗑️\n",
    "```\n",
    "For precision data handling, say hello to Drop Malformed Mode! 🚮\n",
    "Enable it, and PySpark will drop rows with malformed records during data reading. \n",
    "Perfect for situations that demand strict data quality without any room for corruption.\n",
    "\n",
    "spark.read.option(\"mode\", \"dropMalformed\").json(\"sample.json\")\n",
    "```\n",
    "\n",
    "#### 3. Fail Fast Mode: ❌\n",
    "```\n",
    "When errors are simply not an option, Fail Fast Mode becomes your trusty companion! 🚫\n",
    "This mode is a must-have when you need to swiftly identify and rectify corrupt data right from the start of your data journey.\n",
    "\n",
    "spark.read.option(\"mode\", \"FAILFAST\").parquet(\"sample.parquet\")\n",
    "```\n",
    "\n",
    "📌 Note: By default, PySpark's mode is set to permissive, but now you're equipped to choose the right mode for your specific data needs.\n",
    "Data quality is the bedrock of insightful analysis. With these modes at your disposal, you're well on your way to extracting accurate and meaningful insights from your data! 📊🔍\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D5622AQE30Jy0PuD1yA/feedshare-shrink_800/0/1696140625893?e=1707350400&v=beta&t=bQbhBc0FqW64E15AIgnVF--V7AetX6rUMb-eMKA8yJs\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78d91b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+---------------+--------------+---------------+-------------------+-------------------+\n",
      "|productname| category|quantitysold|revenuegenrated|customerrating|customerReviews|socialmediamentions|reviewarrivalTiming|\n",
      "+-----------+---------+------------+---------------+--------------+---------------+-------------------+-------------------+\n",
      "|  Product A|CategoryA|         100|        $1000.0|           4.5|             50|                200|04-09-2022 01:12:23|\n",
      "|  Product B|CategoryA|         150|        $1200.0|           4.8|             80|                300|06-02-2022 02:20:12|\n",
      "|  Product C|CategoryB|          80|         $800.0|           4.2|             30|                150|01-11-2022 12:33:44|\n",
      "|  Product D|CategoryB|         120|        $1500.0|           4.6|             60|                250|02-20-2022 14:05:26|\n",
      "|  Product E|CategoryC|          70|         $700.0|           4.0|             20|                100|03-02-2022 09:33:28|\n",
      "|  Product F|CategoryC|          90|         $900.0|           4.4|             40|                180|03-02-2022 08:40:42|\n",
      "+-----------+---------+------------+---------------+--------------+---------------+-------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- productname: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantitysold: integer (nullable = true)\n",
      " |-- revenuegenrated: string (nullable = true)\n",
      " |-- customerrating: double (nullable = true)\n",
      " |-- customerReviews: integer (nullable = true)\n",
      " |-- socialmediamentions: integer (nullable = true)\n",
      " |-- reviewarrivalTiming: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115897323930460160/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115897323930460160%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# by explicitly going through each field and type casting by hand.\n",
    "\n",
    "schema_ = StructType([\n",
    "        StructField(\"productname\", StringType(), True),\\\n",
    "        StructField(\"category\", StringType(), True),\\\n",
    "        StructField(\"quantitysold\", IntegerType(), True),\\\n",
    "        StructField(\"revenuegenrated\", StringType(), True),\\\n",
    "        StructField(\"customerrating\", DoubleType(), True),\\\n",
    "        StructField(\"customerReviews\", IntegerType(), True),\\\n",
    "        StructField(\"socialmediamentions\", IntegerType(), True),\\\n",
    "    StructField(\"reviewarrivalTiming\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "data = [(\"Product A\", \"CategoryA\", 100, \"$1000.0\", 4.5, 50, 200, \"04-09-2022 01:12:23\"),\n",
    "(\"Product B\", \"CategoryA\", 150, \"$1200.0\", 4.8, 80, 300, \"06-02-2022 02:20:12\"),\n",
    "(\"Product C\", \"CategoryB\", 80, \"$800.0\", 4.2, 30, 150, \"01-11-2022 12:33:44\"),\n",
    "(\"Product D\", \"CategoryB\", 120, \"$1500.0\", 4.6, 60, 250, \"02-20-2022 14:05:26\"),\n",
    "(\"Product E\", \"CategoryC\", 70, \"$700.0\", 4.0, 20, 100, \"03-02-2022 09:33:28\"),\n",
    "(\"Product F\", \"CategoryC\", 90, \"$900.0\", 4.4, 40, 180, \"03-02-2022 08:40:42\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa285f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Product A,CategoryA,100,$1000.0,4.5,50,200,04/09/2022 01:12:23\n",
      "Product B,CategoryA,150,$1200.0,4.8,80,300,06/02/2022 02:20:12\n",
      "Product C,CategoryB,80,$800.0,4.2,30,150,01/11/2022 12:33:44\n",
      "Product D,CategoryB,120,$1500.0,4.6,60,250,02/20/2022 14:05:26\n",
      "Product E,CategoryC,70,$700.0,4.0,20,100,03/02/2022 09:33:28\n",
      "Product F,CategoryC,90,$900.0,4.4,40,180,03/02/2022 08:40:42\n"
     ]
    }
   ],
   "source": [
    "# raw schema without explicitly going through each field and type casting by hand.\n",
    "\n",
    "rawtext = sc.textFile(Location + \"Product_Category.txt\")\n",
    "rawtext.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d338469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "['Product D', 'CategoryB', '120', '$1500.0', '4.6', '60', '250', '02/20/2022 14:05:26']\n",
      "['Product E', 'CategoryC', '70', '$700.0', '4.0', '20', '100', '03/02/2022 09:33:28']\n",
      "['Product F', 'CategoryC', '90', '$900.0', '4.4', '40', '180', '03/02/2022 08:40:42']\n",
      "['Product A', 'CategoryA', '100', '$1000.0', '4.5', '50', '200', '04/09/2022 01:12:23']\n",
      "['Product B', 'CategoryA', '150', '$1200.0', '4.8', '80', '300', '06/02/2022 02:20:12']\n",
      "['Product C', 'CategoryB', '80', '$800.0', '4.2', '30', '150', '01/11/2022 12:33:44']\n"
     ]
    }
   ],
   "source": [
    "splitData = rawtext.map(lambda x : x.split(\",\"))\n",
    "splitData.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b795423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+---------------+--------------+---------------+-------------------+-------------------+\n",
      "|productname| category|quantitysold|revenuegenrated|customerrating|customerReviews|socialmediamentions|reviewarrivalTiming|\n",
      "+-----------+---------+------------+---------------+--------------+---------------+-------------------+-------------------+\n",
      "|  Product A|CategoryA|         100|        $1000.0|           4.5|             50|                200|04/09/2022 01:12:23|\n",
      "|  Product B|CategoryA|         150|        $1200.0|           4.8|             80|                300|06/02/2022 02:20:12|\n",
      "|  Product C|CategoryB|          80|         $800.0|           4.2|             30|                150|01/11/2022 12:33:44|\n",
      "|  Product D|CategoryB|         120|        $1500.0|           4.6|             60|                250|02/20/2022 14:05:26|\n",
      "|  Product E|CategoryC|          70|         $700.0|           4.0|             20|                100|03/02/2022 09:33:28|\n",
      "|  Product F|CategoryC|          90|         $900.0|           4.4|             40|                180|03/02/2022 08:40:42|\n",
      "+-----------+---------+------------+---------------+--------------+---------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<Row('Product D', 'CategoryB', '120', '$1500.0', '4.6', '60', '250', '02/20/2022 14:05:26')>\n",
      "<Row('Product E', 'CategoryC', '70', '$700.0', '4.0', '20', '100', '03/02/2022 09:33:28')>\n",
      "<Row('Product F', 'CategoryC', '90', '$900.0', '4.4', '40', '180', '03/02/2022 08:40:42')>\n",
      "<Row('Product A', 'CategoryA', '100', '$1000.0', '4.5', '50', '200', '04/09/2022 01:12:23')>\n",
      "<Row('Product B', 'CategoryA', '150', '$1200.0', '4.8', '80', '300', '06/02/2022 02:20:12')>\n",
      "<Row('Product C', 'CategoryB', '80', '$800.0', '4.2', '30', '150', '01/11/2022 12:33:44')>\n"
     ]
    }
   ],
   "source": [
    "RowData = splitData.map(lambda x : Row(x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7]))\n",
    "RowData.foreach(print)\n",
    "\n",
    "schema_ = StructType([\n",
    "        StructField(\"productname\", StringType(), True),\\\n",
    "        StructField(\"category\", StringType(), True),\\\n",
    "        StructField(\"quantitysold\", StringType(), True),\\\n",
    "        StructField(\"revenuegenrated\", StringType(), True),\\\n",
    "        StructField(\"customerrating\", StringType(), True),\\\n",
    "        StructField(\"customerReviews\", StringType(), True),\\\n",
    "        StructField(\"socialmediamentions\", StringType(), True),\\\n",
    "    StructField(\"reviewarrivalTiming\", StringType(), True)\n",
    "])\n",
    "\n",
    "RowDf = spark.createDataFrame(data=RowData, schema=schema_)\n",
    "RowDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f4a5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productname: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- quantitysold: integer (nullable = true)\n",
      " |-- revenuegenrated: string (nullable = true)\n",
      " |-- customerrating: string (nullable = true)\n",
      " |-- customerReviews: string (nullable = true)\n",
      " |-- socialmediamentions: string (nullable = true)\n",
      " |-- reviewarrivalTiming: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df33 = RowDf.withColumn(\"quantitysold\", col(\"quantitysold\").cast(\"Integer\"))\n",
    "df33.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10cb4e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|employee_id|team_id|\n",
      "+-----------+-------+\n",
      "|          1|      8|\n",
      "|          2|      8|\n",
      "|          3|      8|\n",
      "|          4|      7|\n",
      "|          5|      9|\n",
      "|          6|      9|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117297691230957568/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117297691230957568%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"employee_id\",\"team_id\"]\n",
    "\n",
    "data = [(1,8),\n",
    "(2,8),\n",
    "(3,8),\n",
    "(4,7),\n",
    "(5,9),\n",
    "(6,9)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f495c7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|team_id|team_size|\n",
      "+-------+---------+\n",
      "|      8|        3|\n",
      "|      7|        1|\n",
      "|      9|        2|\n",
      "+-------+---------+\n",
      "\n",
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          4|        1|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupData = df.groupBy(\"team_id\").agg(count(\"employee_id\").alias(\"team_size\"))\n",
    "groupData.show()\n",
    "\n",
    "FinalDf = df.join(groupData, on=\"team_id\", how=\"inner\")\n",
    "# FinalDf.show()\n",
    "\n",
    "FinalDf.select(\"employee_id\", \"team_size\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a7d381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|name          |cleaned_name|\n",
      "+--------------+------------+\n",
      "|R@hul K#mar   |Rhul Kmar   |\n",
      "|S@m!rtha P@tel|Smrtha Ptel |\n",
      "|M!dhavi S#ngh |Mdhavi Sngh |\n",
      "+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117038244063510528/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117038244063510528%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample data with names containing special characters\n",
    "data = [\n",
    "  {\"name\": \"R@hul K#mar\"},\n",
    "  {\"name\": \"S@m!rtha P@tel\"},\n",
    "  {\"name\": \"M!dhavi S#ngh\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Remove special characters from names using regex\n",
    "df_cleaned = df.withColumn(\"cleaned_name\", regexp_replace(col(\"name\"), \"[^a-zA-Z ]\", \"\"))\n",
    "df_cleaned.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e95428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|customerID|            dateList|\n",
      "+----------+--------------------+\n",
      "| customer1|[2023-08-07, 2023...|\n",
      "| customer2|[2023-08-07, 2023...|\n",
      "| customer3|[2023-08-07, 2023...|\n",
      "| customer4|[2023-08-07, 2023...|\n",
      "| customer5|[2023-08-07, 2023...|\n",
      "+----------+--------------------+\n",
      "\n",
      "root\n",
      " |-- customerID: string (nullable = true)\n",
      " |-- dateList: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7116973047328206848/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116973047328206848%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"customerID\", \"dateList\"]\n",
    "\n",
    "data = [(\"customer1\", [\"2023-08-07\", \"2023-08-14\", \"2023-08-11\", \"2023-08-03\", \"2023-08-04\", \"2023-08-01\", \"2023-08-08\", \"2023-08-02\", \"2023-08-12\", \"2023-08-09\", \"2023-08-13\", \"2023-08-10\"]),\n",
    "(\"customer2\", [\"2023-08-07\", \"2023-08-14\", \"2023-08-03\", \"2023-08-08\", \"2023-08-02\", \"2023-08-12\", \"2023-08-09\", \"2023-08-06\", \"2023-08-13\", \"2023-08-10\"]),\n",
    "(\"customer3\", [\"2023-08-07\", \"2023-08-14\", \"2023-08-11\", \"2023-08-03\", \"2023-08-04\", \"2023-08-01\", \"2023-08-02\", \"2023-08-12\", \"2023-08-13\", \"2023-08-10\"]),\n",
    "(\"customer4\", [\"2023-08-07\", \"2023-08-11\", \"2023-08-03\", \"2023-08-04\", \"2023-08-01\", \"2023-08-05\", \"2023-08-02\", \"2023-08-09\", \"2023-08-06\", \"2023-08-13\", \"2023-08-10\"]),\n",
    "(\"customer5\", [\"2023-08-07\", \"2023-08-14\", \"2023-08-11\", \"2023-08-03\", \"2023-08-04\", \"2023-08-01\", \"2023-08-02\", \"2023-08-06\", \"2023-08-13\"])]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d42989b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|customerID|  dateList|\n",
      "+----------+----------+\n",
      "| customer1|2023-08-07|\n",
      "| customer1|2023-08-14|\n",
      "| customer1|2023-08-11|\n",
      "| customer1|2023-08-03|\n",
      "| customer1|2023-08-04|\n",
      "| customer1|2023-08-01|\n",
      "| customer1|2023-08-08|\n",
      "| customer1|2023-08-02|\n",
      "| customer1|2023-08-12|\n",
      "| customer1|2023-08-09|\n",
      "| customer1|2023-08-13|\n",
      "| customer1|2023-08-10|\n",
      "| customer2|2023-08-07|\n",
      "| customer2|2023-08-14|\n",
      "| customer2|2023-08-03|\n",
      "| customer2|2023-08-08|\n",
      "| customer2|2023-08-02|\n",
      "| customer2|2023-08-12|\n",
      "| customer2|2023-08-09|\n",
      "| customer2|2023-08-06|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+-------------+\n",
      "|customerID|            DateList|LongestStreak|\n",
      "+----------+--------------------+-------------+\n",
      "| customer1|[2023-08-09, 2023...|           12|\n",
      "| customer2|[2023-08-09, 2023...|           10|\n",
      "| customer3|[2023-08-01, 2023...|           10|\n",
      "| customer4|[2023-08-09, 2023...|           11|\n",
      "| customer5|[2023-08-01, 2023...|            9|\n",
      "+----------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodeDf = df.withColumn(\"dateList\", explode(col(\"dateList\")))\n",
    "explodeDf.show()\n",
    "\n",
    "expl = explodeDf.groupBy(\"customerID\").agg(collect_set(col(\"dateList\")).alias(\"DateList\"), count(col(\"customerID\")).alias(\"LongestStreak\"))\n",
    "expl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a26aef56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "| name|product_id|units|\n",
      "+-----+----------+-----+\n",
      "|Room1|         1|    1|\n",
      "|Room1|         2|   10|\n",
      "|Room1|         3|    5|\n",
      "|Room2|         1|    2|\n",
      "|Room2|         2|    2|\n",
      "|Room3|         4|    1|\n",
      "+-----+----------+-----+\n",
      "\n",
      "+----------+---------------+-----+------+------+\n",
      "|product_id|   product_name|Width|Length|Height|\n",
      "+----------+---------------+-----+------+------+\n",
      "|         1|  Mencollection|    5|    50|    40|\n",
      "|         2|Girlscollection|    5|     5|     5|\n",
      "|         3|Childcollection|    2|    10|    10|\n",
      "|         4|Womencollection|    4|    10|    20|\n",
      "+----------+---------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7116830873060069376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116830873060069376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Storagehouse Table:\n",
    "\n",
    "schema1 = [\"name\" , \"product_id\" , \"units\" ]\n",
    "\n",
    "data1 = [\n",
    "(\"Room1\" , 1 , 1 ),\n",
    "(\"Room1\" , 2 , 10),\n",
    "(\"Room1\" , 3 , 5 ),\n",
    "(\"Room2\" , 1 , 2 ),\n",
    "(\"Room2\" , 2 , 2 ),\n",
    "(\"Room3\" , 4 , 1 )]\n",
    "\n",
    "# Products Table:\n",
    "\n",
    "schema2 = [\"product_id\" , \"product_name\" , \"Width\" , \"Length\" , \"Height\" ]\n",
    "\n",
    "data2 = [\n",
    "(1 , \"Mencollection\" , 5 , 50 , 40) ,\n",
    "(2 , \"Girlscollection\" , 5 , 5 , 5) ,\n",
    "(3 , \"Childcollection\" , 2 , 10 , 10) ,\n",
    "(4 , \"Womencollection\" , 4 , 10 , 20 )]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6f5f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+---------------+-----+------+------+\n",
      "|product_id| name|units|   product_name|Width|Length|Height|\n",
      "+----------+-----+-----+---------------+-----+------+------+\n",
      "|         1|Room1|    1|  Mencollection|    5|    50|    40|\n",
      "|         1|Room2|    2|  Mencollection|    5|    50|    40|\n",
      "|         2|Room1|   10|Girlscollection|    5|     5|     5|\n",
      "|         2|Room2|    2|Girlscollection|    5|     5|     5|\n",
      "|         3|Room1|    5|Childcollection|    2|    10|    10|\n",
      "|         4|Room3|    1|Womencollection|    4|    10|    20|\n",
      "+----------+-----+-----+---------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinData = df1.join(df2, on=\"product_id\", how=\"inner\")\n",
    "joinData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c90a3002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+---------------+-----+------+------+------+\n",
      "|product_id| name|units|   product_name|Width|Length|Height|volume|\n",
      "+----------+-----+-----+---------------+-----+------+------+------+\n",
      "|         1|Room1|    1|  Mencollection|    5|    50|    40| 10000|\n",
      "|         1|Room2|    2|  Mencollection|    5|    50|    40| 20000|\n",
      "|         2|Room1|   10|Girlscollection|    5|     5|     5|  1250|\n",
      "|         2|Room2|    2|Girlscollection|    5|     5|     5|   250|\n",
      "|         3|Room1|    5|Childcollection|    2|    10|    10|  1000|\n",
      "|         4|Room3|    1|Womencollection|    4|    10|    20|   800|\n",
      "+----------+-----+-----+---------------+-----+------+------+------+\n",
      "\n",
      "+-----+------------+\n",
      "| name|Total_Volume|\n",
      "+-----+------------+\n",
      "|Room3|         800|\n",
      "|Room2|       20250|\n",
      "|Room1|       12250|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "volumeData = joinData.withColumn(\"volume\", col(\"Width\") * col(\"Length\") * col(\"Height\") * col(\"units\"))\n",
    "volumeData.show()\n",
    "\n",
    "grp = volumeData.groupBy(\"name\").agg(sum(\"volume\").alias(\"Total_Volume\"))\n",
    "grp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3ab1d",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7116565359666610176/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116565359666610176%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "Question 22 : Explain self join in SQL\n",
    "\n",
    "A self-join in SQL is when a table is joined with itself. It's often used to retrieve related information from the same table. Here's an example:\n",
    "Let's say you have a table called \"Employees\" with the following structure:\n",
    "\n",
    "```\n",
    "CREATE TABLE Employees (\n",
    "  EmployeeID INT PRIMARY KEY,\n",
    "  FirstName VARCHAR(50),\n",
    "  LastName VARCHAR(50),\n",
    "  ManagerID INT\n",
    ");\n",
    "```\n",
    "In this table, each employee has an EmployeeID, FirstName, LastName, and a ManagerID that represents the ID of their manager, which is also an employee in the same table.\n",
    "To perform a self-join to retrieve the names of employees and their managers, you can write a SQL query like this:\n",
    "```\n",
    "SELECT e1.FirstName AS EmployeeFirstName, e1.LastName AS EmployeeLastName,\n",
    "    e2.FirstName AS ManagerFirstName, e2.LastName AS ManagerLastName\n",
    "FROM Employees e1\n",
    "LEFT JOIN Employees e2 ON e1.ManagerID = e2.EmployeeID;\n",
    "```\n",
    "In this query, we alias the \"Employees\" table as both e1 and e2 and use a LEFT JOIN to connect employees to their managers based on the ManagerID and EmployeeID columns.\n",
    "```\n",
    "The result? \n",
    "A clean and insightful report that provides Employee and Manager names side by side. \n",
    "📊 Self-joins are just one of the many SQL techniques that can help you make sense of complex data relationships. Stay curious and keep learning! 💡\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55067d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7de5bbd8",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7116608449236353024/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116608449236353024%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "```\n",
    "𝗪𝗶𝗴𝗴𝗹𝗲 𝗦𝗼𝗿𝘁 rearranges elements in an array in the below pattern:\n",
    "Odd element --> should be larger than both its adjacent elements.\n",
    "Even element --> should be smaller than both its adjacent elements.\n",
    "```\n",
    "```\n",
    "𝗥𝗲𝗮𝗹 #𝗪𝗼𝗿𝗹𝗱 𝘂𝘀𝗲𝗰𝗮𝘀𝗲 𝗼𝗳 𝗪𝗶𝗴𝗴𝗹𝗲 𝗦𝗼𝗿𝘁:\n",
    "Given Product whose prices change twice a week.\n",
    "Get the best to worst and worst to best performing weeks.\n",
    "```\n",
    "Scenario\n",
    "```\n",
    "Given below data:\n",
    "📌sort the prices in \"wiggle\" pattern from highest to lowest\n",
    "📌get the week of the year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fa89d4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+-----+\n",
      "|stockQty|rating| priceDate|price|\n",
      "+--------+------+----------+-----+\n",
      "|      54|   4.0|2020-10-24|63.69|\n",
      "|      73|   4.7|2020-01-29|62.21|\n",
      "|      77|   2.2|2020-05-16| 73.9|\n",
      "|      68|   2.6|2020-02-08|77.25|\n",
      "|      67|   3.2|2020-01-13|83.35|\n",
      "|      66|   1.5|2020-12-04|92.34|\n",
      "|      41|   2.8|2020-10-12|51.01|\n",
      "|      86|   2.7|2020-05-30|69.58|\n",
      "|      97|   5.0|2020-08-14|57.71|\n",
      "|      33|   2.7|2020-09-22|83.54|\n",
      "|      44|   4.7|2020-10-13| 85.7|\n",
      "|      96|   3.7|2020-11-26|95.03|\n",
      "|      98|   4.4|2020-06-07|85.11|\n",
      "|      72|   1.9|2020-10-08|61.84|\n",
      "|      37|   3.1|2020-11-20|70.28|\n",
      "|      27|   4.1|2020-07-24|60.59|\n",
      "|      67|   3.0|2020-06-08|57.12|\n",
      "|      34|   1.2|2020-07-11|76.86|\n",
      "|      17|   3.8|2020-03-20|75.33|\n",
      "|      45|   3.5|2020-08-17|98.38|\n",
      "+--------+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7116608449236353024/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116608449236353024%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "data = [Row(54, 4.0, \"2020-10-24\", 63.69),\n",
    "Row(73, 4.7, \"2020-01-29\", 62.21),\n",
    "Row(77, 2.2, \"2020-05-16\", 73.90),\n",
    "Row(68, 2.6, \"2020-02-08\", 77.25),\n",
    "Row(67, 3.2, \"2020-01-13\", 83.35),\n",
    "Row(66, 1.5, \"2020-12-04\", 92.34),\n",
    "Row(41, 2.8, \"2020-10-12\", 51.01),\n",
    "Row(86, 2.7, \"2020-05-30\", 69.58),\n",
    "Row(97, 5.0, \"2020-08-14\", 57.71),\n",
    "Row(33, 2.7, \"2020-09-22\", 83.54),\n",
    "Row(44, 4.7, \"2020-10-13\", 85.70),\n",
    "Row(96, 3.7, \"2020-11-26\", 95.03),\n",
    "Row(98, 4.4, \"2020-06-07\", 85.11),\n",
    "Row(72, 1.9, \"2020-10-08\", 61.84),\n",
    "Row(37, 3.1, \"2020-11-20\", 70.28),\n",
    "Row(27, 4.1, \"2020-07-24\", 60.59),\n",
    "Row(67, 3.0, \"2020-06-08\", 57.12),\n",
    "Row(34, 1.2, \"2020-07-11\", 76.86),\n",
    "Row(17, 3.8, \"2020-03-20\", 75.33),\n",
    "Row(45, 3.5, \"2020-08-17\", 98.38),\n",
    "Row(66, 4.7, \"2020-12-18\", 54.45),\n",
    "Row(37, 1.7, \"2020-06-18\", 61.04),\n",
    "Row(88, 2.5, \"2020-08-24\", 65.07),\n",
    "Row(31, 3.5, \"2020-02-11\", 51.85),\n",
    "Row(64, 2.7, \"2020-01-02\", 63.11),\n",
    "Row(26, 4.9, \"2020-12-17\", 96.99),\n",
    "Row(82, 3.7, \"2020-03-08\", 77.37),\n",
    "Row(72, 5.0, \"2020-08-27\", 79.58),\n",
    "Row(56, 2.2, \"2020-02-07\", 76.18),\n",
    "Row(57, 2.9, \"2020-07-15\", 76.38),\n",
    "Row(19, 3.5, \"2020-03-30\", 84.51),\n",
    "Row(98, 2.4, \"2020-07-05\", 57.23),\n",
    "Row(79, 2.7, \"2020-09-28\", 85.21),\n",
    "Row(58, 3.5, \"2020-11-13\", 93.67),\n",
    "Row(52, 2.6, \"2020-04-17\", 77.32)]\n",
    "\n",
    "\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "schema = StructType([StructField(\"stockQty\", IntegerType(), nullable = True),\n",
    "StructField(\"rating\", DoubleType(), nullable = True),\n",
    "StructField(\"priceDate\", StringType(), nullable = True),\n",
    "StructField(\"price\", DoubleType(), nullable = True)])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c97fc763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "+--------+------+----------+-----+----+----+----------------+\n",
      "|stockQty|rating| priceDate|price|week|rank|alternating_rank|\n",
      "+--------+------+----------+-----+----+----+----------------+\n",
      "|      41|   2.8|2020-10-12|51.01|  42|   1|               1|\n",
      "|      31|   3.5|2020-02-11|51.85|   7|   2|               3|\n",
      "|      66|   4.7|2020-12-18|54.45|  51|   3|               5|\n",
      "|      67|   3.0|2020-06-08|57.12|  24|   4|               7|\n",
      "|      98|   2.4|2020-07-05|57.23|  27|   5|               9|\n",
      "|      97|   5.0|2020-08-14|57.71|  33|   6|              11|\n",
      "|      27|   4.1|2020-07-24|60.59|  30|   7|              13|\n",
      "|      37|   1.7|2020-06-18|61.04|  25|   8|              15|\n",
      "|      72|   1.9|2020-10-08|61.84|  41|   9|              17|\n",
      "|      73|   4.7|2020-01-29|62.21|   5|  10|              19|\n",
      "|      64|   2.7|2020-01-02|63.11|   1|  11|              21|\n",
      "|      54|   4.0|2020-10-24|63.69|  43|  12|              23|\n",
      "|      88|   2.5|2020-08-24|65.07|  35|  13|              25|\n",
      "|      86|   2.7|2020-05-30|69.58|  22|  14|              27|\n",
      "|      37|   3.1|2020-11-20|70.28|  47|  15|              29|\n",
      "|      77|   2.2|2020-05-16| 73.9|  20|  16|              31|\n",
      "|      17|   3.8|2020-03-20|75.33|  12|  17|              33|\n",
      "|      56|   2.2|2020-02-07|76.18|   6|  18|              34|\n",
      "|      57|   2.9|2020-07-15|76.38|  29|  19|              32|\n",
      "|      34|   1.2|2020-07-11|76.86|  28|  20|              30|\n",
      "+--------+------+----------+-----+----+----+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+------+----------+-----+----+\n",
      "|stockQty|rating| priceDate|price|week|\n",
      "+--------+------+----------+-----+----+\n",
      "|      45|   3.5|2020-08-17|98.38|  34|\n",
      "|      41|   2.8|2020-10-12|51.01|  42|\n",
      "|      26|   4.9|2020-12-17|96.99|  51|\n",
      "|      31|   3.5|2020-02-11|51.85|   7|\n",
      "|      96|   3.7|2020-11-26|95.03|  48|\n",
      "|      66|   4.7|2020-12-18|54.45|  51|\n",
      "|      58|   3.5|2020-11-13|93.67|  46|\n",
      "|      67|   3.0|2020-06-08|57.12|  24|\n",
      "|      66|   1.5|2020-12-04|92.34|  49|\n",
      "|      98|   2.4|2020-07-05|57.23|  27|\n",
      "|      44|   4.7|2020-10-13| 85.7|  42|\n",
      "|      97|   5.0|2020-08-14|57.71|  33|\n",
      "|      79|   2.7|2020-09-28|85.21|  40|\n",
      "|      27|   4.1|2020-07-24|60.59|  30|\n",
      "|      98|   4.4|2020-06-07|85.11|  23|\n",
      "|      37|   1.7|2020-06-18|61.04|  25|\n",
      "|      19|   3.5|2020-03-30|84.51|  14|\n",
      "|      72|   1.9|2020-10-08|61.84|  41|\n",
      "|      33|   2.7|2020-09-22|83.54|  39|\n",
      "|      73|   4.7|2020-01-29|62.21|   5|\n",
      "+--------+------+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rankedDF = df.withColumn(\"week\", weekofyear(\"priceDate\"))\\\n",
    "            .withColumn(\"rank\", row_number().over(Window.orderBy(\"price\")))\n",
    "# rankedDF.show()\n",
    "\n",
    "totalRows = rankedDF.count()\n",
    "print(totalRows)\n",
    "\n",
    "alternatingRankDF = rankedDF.withColumn(\"alternating_rank\", \n",
    "                            when(col(\"rank\") <= totalRows/2, col(\"rank\") * lit(2) - lit(1))\\\n",
    "                            .otherwise((lit(totalRows) - col(\"rank\")) * 2))\n",
    "alternatingRankDF.show()\n",
    "\n",
    "sortedDF = alternatingRankDF.sort(\"alternating_rank\")\n",
    "\n",
    "sortedDF.select(\"stockQty\", \"rating\", \"priceDate\",\"price\",\"week\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7116565359335268352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116565359335268352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "# https://github.com/imadhaka/PyLeetProblems/blob/master/spark/PairedRows.py\n",
    "    \n",
    "# https://github.com/imadhaka/PyLeetProblems/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1391100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| ID|    Name|\n",
      "+---+--------+\n",
      "|  1| Person1|\n",
      "|  2| Person2|\n",
      "|  3| Person3|\n",
      "|  4| Person4|\n",
      "|  5| Person5|\n",
      "|  6| Person6|\n",
      "|  7| Person7|\n",
      "|  8| Person8|\n",
      "|  9| Person9|\n",
      "| 10|Person10|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7116565359335268352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116565359335268352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"ID\", \"Name\"]\n",
    "\n",
    "data = [(1, \"Person1\"),\n",
    "(2, \"Person2\"),\n",
    "(3, \"Person3\"),\n",
    "(4, \"Person4\"),\n",
    "(5, \"Person5\"),\n",
    "(6, \"Person6\"),\n",
    "(7, \"Person7\"),\n",
    "(8, \"Person8\"),\n",
    "(9, \"Person9\"),\n",
    "(10, \"Person10\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "53c1dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.alias('df1')\n",
    "df2 = df.alias('df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "14672ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|       r1|         r2|\n",
      "+---------+-----------+\n",
      "|1 Person1|  2 Person2|\n",
      "|3 Person3|  4 Person4|\n",
      "|5 Person5|  6 Person6|\n",
      "|7 Person7|  8 Person8|\n",
      "|9 Person9|10 Person10|\n",
      "+---------+-----------+\n",
      "\n",
      "+-----------------------+\n",
      "|RESULT                 |\n",
      "+-----------------------+\n",
      "|1 Person1 , 2 Person2  |\n",
      "|3 Person3 , 4 Person4  |\n",
      "|5 Person5 , 6 Person6  |\n",
      "|7 Person7 , 8 Person8  |\n",
      "|9 Person9 , 10 Person10|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDf = df1.join(df2, col('df1.ID') + 1 == col('df2.ID'), 'INNER') \\\n",
    "                        .filter(col('df1.ID') % 2 == 1)\\\n",
    "                        .select(concat_ws(' ', col('df1.ID'), col('df1.Name')).alias('r1'),\n",
    "                                concat_ws(' ', col('df2.ID'), col('df2.Name')).alias('r2'))\n",
    "resultDf.show()\n",
    "\n",
    "resultDf = resultDf.select(concat_ws(' , ', col('r1'), col('r2')).alias('RESULT'))\n",
    "resultDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e1cc1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+\n",
      "| ID|   Name|ID1|   Name1|\n",
      "+---+-------+---+--------+\n",
      "|  1|Person1|  2| Person2|\n",
      "|  2|Person2|  3| Person3|\n",
      "|  3|Person3|  4| Person4|\n",
      "|  4|Person4|  5| Person5|\n",
      "|  5|Person5|  6| Person6|\n",
      "|  6|Person6|  7| Person7|\n",
      "|  7|Person7|  8| Person8|\n",
      "|  8|Person8|  9| Person9|\n",
      "|  9|Person9| 10|Person10|\n",
      "+---+-------+---+--------+\n",
      "\n",
      "+---+-------+---+--------+\n",
      "| ID|   Name|ID1|   Name1|\n",
      "+---+-------+---+--------+\n",
      "|  1|Person1|  2| Person2|\n",
      "|  3|Person3|  4| Person4|\n",
      "|  5|Person5|  6| Person6|\n",
      "|  7|Person7|  8| Person8|\n",
      "|  9|Person9| 10|Person10|\n",
      "+---+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.alias('df1')\n",
    "df2 = df.alias('df2').withColumnRenamed(\"ID\",\"ID1\").withColumnRenamed(\"Name\",\"Name1\")\n",
    "\n",
    "joindata = df1.join(df2, df1.ID == df2.ID1-1, \"inner\")\n",
    "joindata.show()\n",
    "\n",
    "filterData = joindata.filter(col(\"ID1\") % 2 == 0)\n",
    "filterData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5fea9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------+---------+-----------+\n",
      "| ID|   Name|ID1|   Name1|       r1|         r2|\n",
      "+---+-------+---+--------+---------+-----------+\n",
      "|  1|Person1|  2| Person2|1 Person1|  2 Person2|\n",
      "|  3|Person3|  4| Person4|3 Person3|  4 Person4|\n",
      "|  5|Person5|  6| Person6|5 Person5|  6 Person6|\n",
      "|  7|Person7|  8| Person8|7 Person7|  8 Person8|\n",
      "|  9|Person9| 10|Person10|9 Person9|10 Person10|\n",
      "+---+-------+---+--------+---------+-----------+\n",
      "\n",
      "+-----------------------+\n",
      "|RESULT                 |\n",
      "+-----------------------+\n",
      "|1 Person1 , 2 Person2  |\n",
      "|3 Person3 , 4 Person4  |\n",
      "|5 Person5 , 6 Person6  |\n",
      "|7 Person7 , 8 Person8  |\n",
      "|9 Person9 , 10 Person10|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concact1 = filterData.withColumn(\"r1\", concat_ws(\" \", col(\"ID\"), col(\"Name\")))\\\n",
    "        .withColumn(\"r2\", concat_ws(\" \", col(\"ID1\"), col(\"Name1\")))\n",
    "\n",
    "concact1.show()\n",
    "\n",
    "concact2 = concact1.withColumn(\"RESULT\", concat_ws(\" , \", col(\"r1\"), col(\"r2\")))\n",
    "concact2.select(\"RESULT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e8e9ded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117350533186695168/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117350533186695168%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 📌Question: find the largest sum contiguous subarray?\n",
    "\n",
    "def maximumSubarraySum(arr):\n",
    "    n = len(arr)\n",
    "    maxSum = -1e8\n",
    "\n",
    "    for i in range(0, n):\n",
    "        currSum = 0\n",
    "        for j in range(i, n):\n",
    "            currSum = currSum + arr[j]\n",
    "            if(currSum > maxSum):\n",
    "                maxSum = currSum\n",
    "\n",
    "    return maxSum\n",
    "\n",
    "# Your code goes here\n",
    "a = [1, 3, 8, -2, 6, -8, 5];\n",
    "print(maximumSubarraySum(a));\n",
    "\n",
    "# 🔸output: 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "30e015bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+\n",
      "|student_name|exam_score|quartile|\n",
      "+------------+----------+--------+\n",
      "|           D|        65|       1|\n",
      "|           E|        75|       1|\n",
      "|           C|        78|       2|\n",
      "|           A|        85|       2|\n",
      "|           F|        88|       3|\n",
      "|           B|        92|       3|\n",
      "|           G|        95|       4|\n",
      "+------------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117377810200952832/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117377810200952832%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 🌴 Scenario: Distributing Data with PySpark🐍 NTILE Function\n",
    "\n",
    "# Sample student data\n",
    "data = [(\"A\", 85),(\"B\", 92),(\"C\", 78),(\"D\", 65),(\"E\", 75),(\"F\", 88),(\"G\", 95)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"student_name\", \"exam_score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(col(\"exam_score\"))\n",
    "\n",
    "# Apply the PySpark NTILE function\n",
    "df_with_ntile = df.withColumn(\"quartile\", ntile(4).over(window_spec))\n",
    "df_with_ntile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b79a4740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------+----+-----------+-----+\n",
      "|emp_name|emp_id|    Month|Year|Base_Salary|Bonus|\n",
      "+--------+------+---------+----+-----------+-----+\n",
      "|  Rajesh|  1001|  January|2022|     100000|    0|\n",
      "|  Rajesh|  1001| February|2022|     100000|    0|\n",
      "|  Rajesh|  1001|    March|2022|     100000|    0|\n",
      "|  Rajesh|  1001|    April|2022|     100000|50000|\n",
      "|  Rajesh|  1001|      May|2022|     100000|    0|\n",
      "|  Rajesh|  1001|     June|2022|     100000|    0|\n",
      "|  Rajesh|  1001|     July|2022|     100000|    0|\n",
      "|  Rajesh|  1001|   August|2022|     100000|50000|\n",
      "|  Rajesh|  1001|September|2022|     100000|    0|\n",
      "|  Rajesh|  1001|  October|2022|     100000|    0|\n",
      "|  Rajesh|  1001| November|2022|     100000|    0|\n",
      "|  Rajesh|  1001| December|2022|     100000|50000|\n",
      "+--------+------+---------+----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117103379687821313/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117103379687821313%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 🏹 CHALLENGE : We have input_df -> emp_name, emp_id, year, month, base salary, bonus\n",
    "# Output should be total salary bracket, count of employees in salary bracket\n",
    "\n",
    "df = spark.createDataFrame([(\"Rajesh\", 1001 ,\"January\", 2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"February\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"March\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"April\",2022, 100000, 50000 ),\n",
    "(\"Rajesh\", 1001 ,\"May\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"June\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"July\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"August\",2022, 100000, 50000 ),\n",
    "(\"Rajesh\", 1001 ,\"September\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"October\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"November\",2022, 100000, 0 ),\n",
    "(\"Rajesh\", 1001 ,\"December\",2022, 100000, 50000 )\n",
    "], [\"emp_name\", \"emp_id\", \"Month\",\"Year\", \"Base_Salary\", \"Bonus\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e93dc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------------+--------------------+\n",
      "|emp_name|Year|Total_salary|      Salary_Bracket|\n",
      "+--------+----+------------+--------------------+\n",
      "|  Rajesh|2022|     1350000|Greater Than 1000000|\n",
      "+--------+----+------------+--------------------+\n",
      "\n",
      "+--------------------+---------+\n",
      "|Salary_Bracket      |emp_count|\n",
      "+--------------------+---------+\n",
      "|Greater Than 1000000|1        |\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"Monthly_Salary\", df[\"Base_Salary\"]+df[\"Bonus\"])\\\n",
    "    .groupby(\"emp_name\",\"Year\").agg(sum(col(\"Monthly_Salary\")).alias(\"Total_salary\"))\\\n",
    "    .withColumn(\"Salary_Bracket\", when(col(\"Total_salary\")<500000,lit('Less Than 500000'))\\\n",
    "    .when(col(\"Total_salary\")>1000000,lit('Greater Than 1000000'))\n",
    "    .otherwise(lit('Between 500000 to 1000000')))\n",
    "df1.show()\n",
    "\n",
    "df1.createOrReplaceTempView(\"temp\")\n",
    "\n",
    "result = spark.sql(\"select Salary_Bracket, count(Salary_Bracket) as emp_count from temp group by Salary_Bracket \")\n",
    "\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d3367573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|customer_id|product_key|\n",
      "+-----------+-----------+\n",
      "|          1|          6|\n",
      "|          1|          5|\n",
      "|          2|          6|\n",
      "|          3|          6|\n",
      "|          3|          5|\n",
      "+-----------+-----------+\n",
      "\n",
      "+-----------+\n",
      "|product_key|\n",
      "+-----------+\n",
      "|          5|\n",
      "|          6|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117667622749421568/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117667622749421568%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "cust = [(1,5),(2,6),(3,5),(3,6),(1,6)]\n",
    "\n",
    "custDf = spark.createDataFrame(cust, [\"customer_id\",\"product_key\"]).orderBy(\"customer_id\")\n",
    "custDf.show()\n",
    "\n",
    "prod_df = spark.createDataFrame([(5,),(6,)], [\"product_key\"])\n",
    "prod_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "cf2069e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|          1|    2|\n",
      "|          3|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "\n",
    "prod_distinct = prod_df.distinct().count()\n",
    "print(prod_distinct)\n",
    "\n",
    "cust_group = custDf.groupBy(\"customer_id\").agg(countDistinct(\"product_key\").alias(\"count\"))\n",
    "cust_group.show()\n",
    "\n",
    "final_df = cust_group.filter(col(\"count\") == prod_distinct)\n",
    "final_df.select(\"customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82450fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "+-----------+\n",
      "|customer_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          3|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "\n",
    "prod_distinct = prod_df.distinct().count()\n",
    "print(prod_distinct)\n",
    "\n",
    "win=Window.partitionBy(\"customer_id\")\n",
    "\n",
    "result_df=custDf.withColumn(\"count\",count(col(\"customer_id\")).over(win))\\\n",
    "                .filter(col(\"count\")==prod_distinct)\\\n",
    "                .select(\"customer_id\")\\\n",
    "                .distinct()\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c3d36395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|product  |category  |total_sales|\n",
      "+---------+----------+-----------+\n",
      "|Product A|Category X|300        |\n",
      "|Product B|Category Y|400        |\n",
      "|Product A|Category Z|300        |\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7118105404118593538/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118105404118593538%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample sales data\n",
    "data = [(\"Product A\", \"Category X\", 100),\n",
    "    (\"Product B\", \"Category Y\", 150),\n",
    "    (\"Product A\", \"Category X\", 200),\n",
    "    (\"Product B\", \"Category Y\", 250),\n",
    "    (\"Product A\", \"Category Z\", 300)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"product\", \"category\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Use PySpark's aggregation functions\n",
    "df_summary = df.groupBy(\"product\", \"category\").agg(sum(\"sales\").alias(\"total_sales\"))\n",
    "df_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d895b",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7118305823767883776/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118305823767883776%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "### What is \"Factless fact table\" ?\n",
    "```\n",
    "A factless fact table is a type of fact table that does not contain any measures. \n",
    "It is essentially an intersection of dimensions (it contains nothing but dimensional keys). \n",
    "Factless fact tables are used to capture events and relationships between dimensions.\n",
    "```\n",
    "### There are two main types of factless fact tables:\n",
    "```\n",
    "🔹Event fact tables: These tables are used to capture events that occur, such as a customer placing an order or a student attending a class.\n",
    "🔹Coverage fact tables: These tables are used to track whether or not something has happened, such as a customer visiting a website or a product being sold in a store.\n",
    "```\n",
    "\n",
    "#### Factless fact tables can be used to answer a variety of business questions, such as:\n",
    "```\n",
    "❓How many students attended each class in a semester?\n",
    "\n",
    "❓Which products were not sold in any stores last month?\n",
    "\n",
    "❓Which customers have not visited our website in the past six months?\n",
    "```\n",
    "\n",
    "Factless fact tables are often used in conjunction with regular fact tables to provide a more complete view of the data.\n",
    "\n",
    "For example, a company might have a fact table that tracks sales transactions, as well as a factless fact table that tracks customer interactions with the company. This would allow the company to see which customers are most likely to make a purchase, and to target them with marketing campaigns.\n",
    "```\n",
    "Here are some examples of factless fact tables:\n",
    "\n",
    "🔸A table that tracks which students attended each class on a given day.\n",
    "🔸A table that tracks which customers visited each store on a given day.\n",
    "🔸A table that tracks which products were on sale each day.\n",
    "🔸A table that tracks which customers have not placed an order in the past six months.\n",
    "🔸A table that tracks which products have not been sold in the past six months.\n",
    "```\n",
    "Factless fact tables can be a valuable tool for businesses of all sizes. By tracking events and relationships between dimensions, businesses can gain a better understanding of their customers, products, and operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "103674cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "|  2|   Science|    fiction|   8.5|\n",
      "|  3|     irish|     boring|   6.2|\n",
      "|  4|  Ice song|    Fantacy|   8.6|\n",
      "|  5|House card|Interesting|   9.1|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7117841840699052032/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117841840699052032%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Write a Pyspark query to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "# Return the result table in descending order by rating.\n",
    "\n",
    "schema_ = [\"id\" , \"movie\" , \"description\" , \"rating\"]\n",
    "\n",
    "data = [(1 , \"War\", \"great 3D\", 8.9),\n",
    "(2 , \"Science\" , \"fiction\"  , 8.5),\n",
    "(3 , \"irish\", \"boring\"   , 6.2 ) ,\n",
    "(4 , \"Ice song\", \"Fantacy\"  , 8.6 ) ,\n",
    "(5 , \"House card\" , \"Interesting\" , 9.1)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fdb3be9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| id|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf = df.filter((col(\"id\") % 2 != 0) & (col(\"description\") != \"boring\"))\n",
    "finaldf.orderBy(desc(\"rating\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f21c794b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|call|         start_time|\n",
      "+----+-------------------+\n",
      "| PN1|2022-01-01 10:20:00|\n",
      "| PN1|2022-01-01 16:25:00|\n",
      "| PN2|2022-01-01 12:30:00|\n",
      "| PN3|2022-01-02 10:00:00|\n",
      "| PN3|2022-01-02 12:30:00|\n",
      "| PN3|2022-01-03 09:20:00|\n",
      "+----+-------------------+\n",
      "\n",
      "+----+-------------------+\n",
      "|call|           end_time|\n",
      "+----+-------------------+\n",
      "| PN1|2022-01-01 10:45:00|\n",
      "| PN1|2022-01-01 17:05:00|\n",
      "| PN2|2022-01-01 12:55:00|\n",
      "| PN3|2022-01-02 10:20:00|\n",
      "| PN3|2022-01-02 12:50:00|\n",
      "| PN3|2022-01-03 09:40:00|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7118075350764445697/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118075350764445697%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data_start = [\n",
    "(\"PN1\", \"2022-01-01 10:20:00\"),\n",
    "(\"PN1\", \"2022-01-01 16:25:00\"), \n",
    "(\"PN2\", \"2022-01-01 12:30:00\"),\n",
    "(\"PN3\", \"2022-01-02 10:00:00\"),\n",
    "(\"PN3\", \"2022-01-02 12:30:00\"),\n",
    "(\"PN3\", \"2022-01-03 09:20:00\")]\n",
    "\n",
    "data_end = [\n",
    "(\"PN1\", \"2022-01-01 10:45:00\"), \n",
    "(\"PN1\", \"2022-01-01 17:05:00\"),\n",
    "(\"PN2\", \"2022-01-01 12:55:00\"),\n",
    "(\"PN3\", \"2022-01-02 10:20:00\"),\n",
    "(\"PN3\", \"2022-01-02 12:50:00\"),\n",
    "(\"PN3\", \"2022-01-03 09:40:00\")]\n",
    "\n",
    "\n",
    "df_start = spark.createDataFrame(data_start, [\"call\", \"start_time\"])\n",
    "df_start.show()\n",
    "\n",
    "df_end = spark.createDataFrame(data_end, [\"call\", \"end_time\"])\n",
    "df_end.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a2400fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------+\n",
      "|call|         start_time|start_unix|\n",
      "+----+-------------------+----------+\n",
      "| PN1|2022-01-01 10:20:00|1641012600|\n",
      "| PN1|2022-01-01 16:25:00|1641034500|\n",
      "| PN2|2022-01-01 12:30:00|1641020400|\n",
      "| PN3|2022-01-02 10:00:00|1641097800|\n",
      "| PN3|2022-01-02 12:30:00|1641106800|\n",
      "| PN3|2022-01-03 09:20:00|1641181800|\n",
      "+----+-------------------+----------+\n",
      "\n",
      "+----+-------------------+----------+\n",
      "|call|           end_time|  end_unix|\n",
      "+----+-------------------+----------+\n",
      "| PN1|2022-01-01 10:45:00|1641014100|\n",
      "| PN1|2022-01-01 17:05:00|1641036900|\n",
      "| PN2|2022-01-01 12:55:00|1641021900|\n",
      "| PN3|2022-01-02 10:20:00|1641099000|\n",
      "| PN3|2022-01-02 12:50:00|1641108000|\n",
      "| PN3|2022-01-03 09:40:00|1641183000|\n",
      "+----+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert timestamps to Unix timestamps\n",
    "df_start = df_start.withColumn(\"start_unix\", unix_timestamp(\"start_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_start.show()\n",
    "\n",
    "df_end = df_end.withColumn(\"end_unix\", unix_timestamp(\"end_time\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_end.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f54825c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------+-------+\n",
      "|call|         start_time|start_unix|row_num|\n",
      "+----+-------------------+----------+-------+\n",
      "| PN1|2022-01-01 10:20:00|1641012600|      1|\n",
      "| PN1|2022-01-01 16:25:00|1641034500|      2|\n",
      "| PN2|2022-01-01 12:30:00|1641020400|      1|\n",
      "| PN3|2022-01-02 10:00:00|1641097800|      1|\n",
      "| PN3|2022-01-02 12:30:00|1641106800|      2|\n",
      "| PN3|2022-01-03 09:20:00|1641181800|      3|\n",
      "+----+-------------------+----------+-------+\n",
      "\n",
      "+----+-------------------+----------+-------+\n",
      "|call|           end_time|  end_unix|row_num|\n",
      "+----+-------------------+----------+-------+\n",
      "| PN1|2022-01-01 10:45:00|1641014100|      1|\n",
      "| PN1|2022-01-01 17:05:00|1641036900|      2|\n",
      "| PN2|2022-01-01 12:55:00|1641021900|      1|\n",
      "| PN3|2022-01-02 10:20:00|1641099000|      1|\n",
      "| PN3|2022-01-02 12:50:00|1641108000|      2|\n",
      "| PN3|2022-01-03 09:40:00|1641183000|      3|\n",
      "+----+-------------------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec_start = Window.partitionBy(\"call\").orderBy(\"start_time\")\n",
    "window_spec_end = Window.partitionBy(\"call\").orderBy(\"end_time\")\n",
    "\n",
    "df_start = df_start.withColumn(\"row_num\", row_number().over(window_spec_start))\n",
    "df_start.show()\n",
    "\n",
    "df_end = df_end.withColumn(\"row_num\", row_number().over(window_spec_end))\n",
    "df_end.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c32525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+----------+-------------------+----------+----------------+----------------+\n",
      "|call|         start_time|start_unix|           end_time|  end_unix|duration_seconds|duration_minutes|\n",
      "+----+-------------------+----------+-------------------+----------+----------------+----------------+\n",
      "| PN1|2022-01-01 10:20:00|1641012600|2022-01-01 10:45:00|1641014100|            1500|            25.0|\n",
      "| PN1|2022-01-01 16:25:00|1641034500|2022-01-01 17:05:00|1641036900|            2400|            40.0|\n",
      "| PN2|2022-01-01 12:30:00|1641020400|2022-01-01 12:55:00|1641021900|            1500|            25.0|\n",
      "| PN3|2022-01-02 10:00:00|1641097800|2022-01-02 10:20:00|1641099000|            1200|            20.0|\n",
      "| PN3|2022-01-02 12:30:00|1641106800|2022-01-02 12:50:00|1641108000|            1200|            20.0|\n",
      "| PN3|2022-01-03 09:20:00|1641181800|2022-01-03 09:40:00|1641183000|            1200|            20.0|\n",
      "+----+-------------------+----------+-------------------+----------+----------------+----------------+\n",
      "\n",
      "+----+-------------------+-------------------+----------------+\n",
      "|call|         start_time|           end_time|duration_minutes|\n",
      "+----+-------------------+-------------------+----------------+\n",
      "| PN1|2022-01-01 10:20:00|2022-01-01 10:45:00|            25.0|\n",
      "| PN1|2022-01-01 16:25:00|2022-01-01 17:05:00|            40.0|\n",
      "| PN2|2022-01-01 12:30:00|2022-01-01 12:55:00|            25.0|\n",
      "| PN3|2022-01-02 10:00:00|2022-01-02 10:20:00|            20.0|\n",
      "| PN3|2022-01-02 12:30:00|2022-01-02 12:50:00|            20.0|\n",
      "| PN3|2022-01-03 09:20:00|2022-01-03 09:40:00|            20.0|\n",
      "+----+-------------------+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine data based on the row number and call identifier\n",
    "\n",
    "df_combined = df_start.join(df_end, on=[\"call\", \"row_num\"]).drop(\"row_num\")\n",
    "\n",
    "#Calculate call duration in seconds\n",
    "df_combined = df_combined.withColumn(\"duration_seconds\", abs(df_combined[\"end_unix\"] - df_combined[\"start_unix\"]))\n",
    "#Calculate call duration in minutes.\n",
    "df_combined = df_combined.withColumn(\"duration_minutes\", df_combined[\"duration_seconds\"] / 60)\n",
    "df_combined.show()\n",
    "\n",
    "df_combined = df_combined.drop(\"start_unix\", \"end_unix\", \"duration_seconds\")\n",
    "df_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cf5629f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------+\n",
      "|student_name|test_score|percent_rank|\n",
      "+------------+----------+------------+\n",
      "|C           |92        |0.0         |\n",
      "|A           |85        |0.25        |\n",
      "|E           |78        |0.5         |\n",
      "|B           |70        |0.75        |\n",
      "|D           |60        |1.0         |\n",
      "+------------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7118772247946797056/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118772247946797056%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 🌴 Scenario: Unveiling Percentile Insights with PySpark🐍 PERCENT_RANK Function\n",
    "\n",
    "# Sample student data\n",
    "data = [(\"A\", 85),(\"B\", 70),(\"C\", 92),(\"D\", 60),(\"E\", 78)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"student_name\", \"test_score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(desc(\"test_score\"))\n",
    "\n",
    "# Apply the PySpark PERCENT_RANK function\n",
    "df_with_percent_rank = df.withColumn(\"percent_rank\", percent_rank().over(window_spec))\n",
    "df_with_percent_rank.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9c13aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|customer|iphone-model|\n",
      "+--------+------------+\n",
      "|       1|        i-11|\n",
      "|       1|        i-15|\n",
      "|       2|        i-15|\n",
      "|       3|        i-12|\n",
      "|       3|        i-15|\n",
      "|       1|        i-12|\n",
      "|       1|        i-13|\n",
      "|       1|        i-14|\n",
      "+--------+------------+\n",
      "\n",
      "+------------+\n",
      "|iphone-model|\n",
      "+------------+\n",
      "|        i-11|\n",
      "|        i-12|\n",
      "|        i-13|\n",
      "|        i-14|\n",
      "|        i-15|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7118472512916643840/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118472512916643840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data1 = [(1,\"i-11\"), (1,\"i-15\"), (2,\"i-15\"), (3,\"i-12\"), (3,\"i-15\"), (1,\"i-12\"), (1,\"i-13\"), (1,\"i-14\")]\n",
    "\n",
    "data2 = [(\"i-11\",), (\"i-12\",), (\"i-13\",), (\"i-14\",), (\"i-15\",)]\n",
    "\n",
    "purchaseDf = spark.createDataFrame(data1, schema=[\"customer\" , \"iphone-model\"])\n",
    "purchaseDf.show()\n",
    "\n",
    "productDf = spark.createDataFrame(data2, schema=[\"iphone-model\"])\n",
    "productDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0768e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------+\n",
      "|customer|iphone-model|row_number|\n",
      "+--------+------------+----------+\n",
      "|       1|        i-11|         1|\n",
      "|       1|        i-12|         2|\n",
      "|       1|        i-13|         3|\n",
      "|       1|        i-14|         4|\n",
      "|       1|        i-15|         5|\n",
      "|       2|        i-15|         1|\n",
      "|       3|        i-12|         1|\n",
      "|       3|        i-15|         2|\n",
      "+--------+------------+----------+\n",
      "\n",
      "+--------+------------+----------+\n",
      "|customer|iphone-model|row_number|\n",
      "+--------+------------+----------+\n",
      "|       2|        i-15|         1|\n",
      "+--------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ❓Find customers who have bought only iphone 15\n",
    "\n",
    "window_spec = Window.partitionBy(\"customer\").orderBy(\"iphone-model\")\n",
    "\n",
    "row_data = purchaseDf.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "row_data.show()\n",
    "\n",
    "filterdata = row_data.filter((col(\"iphone-model\") == \"i-15\") & (col(\"row_number\") ==  1))\n",
    "filterdata.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98be0574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------+\n",
      "|customer|iphone-model|row_number|\n",
      "+--------+------------+----------+\n",
      "|       3|        i-12|         1|\n",
      "|       3|        i-15|         2|\n",
      "+--------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ❓Find customers who upgraded from 12 to 15 (he should have bought only these models)\n",
    "\n",
    "filterdata = row_data.filter(((col(\"iphone-model\") == \"i-12\") & (col(\"row_number\") ==  1) |\n",
    "                                (col(\"iphone-model\") == \"i-15\") & (col(\"row_number\") ==  2)))\n",
    "filterdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5f70e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+--------+-----+\n",
      "|customer|count|\n",
      "+--------+-----+\n",
      "|       1|    5|\n",
      "|       3|    2|\n",
      "|       2|    1|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+-----+\n",
      "|customer|count|\n",
      "+--------+-----+\n",
      "|       1|    5|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ❓Find customers who have bought all the model in Product Data.\n",
    "\n",
    "prodDistnct = productDf.distinct().count()\n",
    "print(prodDistnct)\n",
    "\n",
    "allModel = purchaseDf.groupBy(\"customer\").agg(countDistinct(\"iphone-model\").alias(\"count\"))\n",
    "allModel.show()\n",
    "\n",
    "allModel.filter(col('count') == prodDistnct).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "151a27d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----+----+-----+-----+\n",
      "|Seat_no|is_empty|lag1|lag2|lead1|lead2|\n",
      "+-------+--------+----+----+-----+-----+\n",
      "|      1|       N|null|null|    Y|    N|\n",
      "|      2|       Y|   N|null|    N|    Y|\n",
      "|      3|       N|   Y|   N|    Y|    Y|\n",
      "|      4|       Y|   N|   Y|    Y|    Y|\n",
      "|      5|       Y|   Y|   N|    Y|    N|\n",
      "|      6|       Y|   Y|   Y|    N|    Y|\n",
      "|      7|       N|   Y|   Y|    Y|    Y|\n",
      "|      8|       Y|   N|   Y|    Y|    Y|\n",
      "|      9|       Y|   Y|   N|    Y|    Y|\n",
      "|     10|       Y|   Y|   Y|    Y|    N|\n",
      "|     11|       Y|   Y|   Y|    N|    Y|\n",
      "|     12|       N|   Y|   Y|    Y|    Y|\n",
      "|     13|       Y|   N|   Y|    Y| null|\n",
      "|     14|       Y|   Y|   N| null| null|\n",
      "+-------+--------+----+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7118426384892379136/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118426384892379136%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Question: Write a Query to get 3 or more Consecutive seats?\n",
    "\n",
    "data = [(1,'N'),(2,'Y'),(3,'N'),(4,'Y'),(5,'Y'),(6,'Y'),\n",
    "(7,'N'),(8,'Y'),(9,'Y'),(10,'Y'),(11,'Y'),(12,'N'),(13,'Y'),(14,'Y')]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Seat_no\", \"is_empty\"])\n",
    "# df.show()\n",
    "\n",
    "win_spec = Window.orderBy(\"Seat_no\")\n",
    "df1 = df.withColumn('lag1', lag(\"is_empty\", 1).over(win_spec))\\\n",
    "        .withColumn('lag2', lag(\"is_empty\", 2).over(win_spec))\\\n",
    "        .withColumn('lead1', lead(\"is_empty\", 1).over(win_spec))\\\n",
    "        .withColumn('lead2', lead(\"is_empty\", 2).over(win_spec))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "914d6fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|Seat_no|is_empty|\n",
      "+-------+--------+\n",
      "|      4|       Y|\n",
      "|      5|       Y|\n",
      "|      6|       Y|\n",
      "|      8|       Y|\n",
      "|      9|       Y|\n",
      "|     10|       Y|\n",
      "|     11|       Y|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterData = df1.filter(\n",
    "            (((col(\"is_empty\") == \"Y\") & (col(\"lead1\") == 'Y') & (col(\"lead2\") == 'Y')) |\n",
    "            ((col(\"is_empty\") == \"Y\") & (col(\"lag1\") == 'Y') & (col(\"lead1\") == 'Y')) |\n",
    "            ((col(\"is_empty\") == \"Y\") & (col(\"lag1\") == 'Y') & (col(\"lag2\") == 'Y')))\n",
    "        )\n",
    "\n",
    "filterData.select(\"Seat_no\", \"is_empty\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74d51099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+------+----------+----------------------+\n",
      "|emp_name|   emp_doj|emp_increment_date|salary|salary_seq|emp_increment_date_seq|\n",
      "+--------+----------+------------------+------+----------+----------------------+\n",
      "|       A|06-09-2003|        06-09-2022|   100|         1|                     1|\n",
      "|       A|06-09-2003|        06-09-2023|   110|         2|                     2|\n",
      "|       B|06-09-2010|        06-09-2020|   280|         3|                     1|\n",
      "|       B|06-09-2010|        06-09-2021|   250|         2|                     2|\n",
      "|       B|06-09-2010|        06-09-2022|   280|         4|                     3|\n",
      "|       B|06-09-2010|        06-09-2023|   200|         1|                     4|\n",
      "|       C|06-09-2015|        06-09-2022|   250|         1|                     1|\n",
      "|       C|06-09-2015|        06-09-2023|   260|         2|                     2|\n",
      "|       D|06-09-2012|        06-09-2023|   400|         1|                     1|\n",
      "|       E|06-09-2018|        06-09-2022|    90|         1|                     1|\n",
      "|       E|06-09-2018|        06-09-2023|   110|         2|                     2|\n",
      "|       F|06-09-2010|        06-09-2022|   120|         1|                     1|\n",
      "|       G|06-09-2020|        06-09-2023|    80|         1|                     1|\n",
      "|       K|06-09-2010|        06-09-2023|    40|         1|                     1|\n",
      "|       Z|06-09-2021|        06-09-2023|   220|         1|                     1|\n",
      "+--------+----------+------------------+------+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7118905100168777729/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118905100168777729%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"emp_name\", \"emp_doj\", \"emp_increment_date\", \"salary\"]\n",
    "\n",
    "data = [\n",
    "('A' , '06-09-2003' , '06-09-2022' , 100),\n",
    "('B' , '06-09-2010' , '06-09-2023' , 200),\n",
    "('C' , '06-09-2015' , '06-09-2022' , 250),\n",
    "('D' , '06-09-2012' , '06-09-2023' , 400),\n",
    "('E' , '06-09-2018' , '06-09-2022' , 90),\n",
    "('B' , '06-09-2010' , '06-09-2022' , 280),\n",
    "('F' , '06-09-2010' , '06-09-2022' , 120),\n",
    "('G' , '06-09-2020' , '06-09-2023' , 80),\n",
    "('K' , '06-09-2010' , '06-09-2023' , 40),\n",
    "('E' , '06-09-2018' , '06-09-2023' , 110),\n",
    "('Z' , '06-09-2021' , '06-09-2023' , 220),\n",
    "('A' , '06-09-2003' , '06-09-2023' , 110),\n",
    "('C' , '06-09-2015' , '06-09-2023' , 260),\n",
    "('B' , '06-09-2010' , '06-09-2021' , 250),\n",
    "('B' , '06-09-2010' , '06-09-2020' , 280)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "# df.show()\n",
    "\n",
    "window_spec1 = Window.partitionBy(\"emp_name\", \"emp_doj\").orderBy(\"salary\", \"emp_increment_date\")\n",
    "window_spec2 = Window.partitionBy(\"emp_name\", \"emp_doj\").orderBy(\"emp_increment_date\", \"salary\")\n",
    "\n",
    "df1 = df.withColumn(\"salary_seq\", dense_rank().over(window_spec1))\\\n",
    "        .withColumn(\"emp_increment_date_seq\", dense_rank().over(window_spec2))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "834a1b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|emp_name|\n",
      "+--------+\n",
      "|       A|\n",
      "|       C|\n",
      "|       E|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.groupBy(\"emp_name\").agg(count(\"*\").alias(\"count\"))\\\n",
    "                            .filter(col(\"count\") == 2)\\\n",
    "                            .select(\"emp_name\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa2c04d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+\n",
      "|emp_id|  name|salary|dept_id|\n",
      "+------+------+------+-------+\n",
      "|   101| sohan|  3000|     11|\n",
      "|   102| rohan|  4000|     12|\n",
      "|   103| mohan|  5000|     13|\n",
      "|   104|   cat|  3000|     11|\n",
      "|   105|suresh|  4000|     12|\n",
      "|   109|mahesh|  7000|     12|\n",
      "|   108| kamal|  8000|     11|\n",
      "+------+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7119913279933603840/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119913279933603840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# ❓Find those employees who have same salary in the same department.\n",
    "\n",
    "data = [(101, \"sohan\", \"3000\", 11), (102, \"rohan\", \"4000\", 12),\n",
    " (103, \"mohan\", \"5000\", 13), (104, \"cat\", \"3000\", 11),\n",
    " (105, \"suresh\", \"4000\", 12), (109, \"mahesh\", \"7000\", 12),(108, \"kamal\", \"8000\", 11)]\n",
    "\n",
    "\n",
    "schema_ = [\"emp_id\", \"name\", \"salary\", \"dept_id\"]\n",
    "df = spark.createDataFrame(data, schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6a57cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|dept_id|salary|count|\n",
      "+-------+------+-----+\n",
      "|     11|  3000|    2|\n",
      "|     12|  4000|    2|\n",
      "+-------+------+-----+\n",
      "\n",
      "+-------+------+------+------+\n",
      "|dept_id|salary|emp_id|  name|\n",
      "+-------+------+------+------+\n",
      "|     11|  3000|   101| sohan|\n",
      "|     11|  3000|   104|   cat|\n",
      "|     12|  4000|   102| rohan|\n",
      "|     12|  4000|   105|suresh|\n",
      "+-------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grp = df.groupBy(\"dept_id\", \"salary\").agg(count(\"salary\").alias(\"count\"))\\\n",
    "                                .filter(col(\"count\") > 1)\n",
    "grp.show()\n",
    "\n",
    "# joined = df.join(grp, ((grp.dept_id == df.dept_id) & (grp.salary == df.salary)), \"inner\")\n",
    "joined = df.join(grp, on=[\"dept_id\", \"salary\"], how=\"inner\").drop(\"count\")\n",
    "joined = joined.orderBy(\"dept_id\")\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b277524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+-------------+\n",
      "|ticket_id|create_date|resolved_date|business_days|\n",
      "+---------+-----------+-------------+-------------+\n",
      "|        1| 2022-08-01|   2022-08-03|            1|\n",
      "|        2| 2022-08-01|   2022-08-12|           10|\n",
      "|        3| 2022-08-01|   2022-08-16|           14|\n",
      "+---------+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7119548123739226112/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119548123739226112%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# ❓Find out the total working days between ticket creation and resolution dates.\n",
    "\n",
    "# Sample Jira ticket and holiday data\n",
    "ticket_data = [\n",
    "  (1, '2022-08-01', '2022-08-03'),\n",
    "  (2, '2022-08-01', '2022-08-12'),\n",
    "  (3, '2022-08-01', '2022-08-16')\n",
    "]\n",
    "\n",
    "holiday_data = [('2022-08-11',),('2022-08-15',)]\n",
    "\n",
    "# Create DataFrames\n",
    "ticket_df = spark.createDataFrame(ticket_data, [\"ticket_id\", \"create_date\", \"resolved_date\"])\n",
    "holiday_df = spark.createDataFrame(holiday_data, [\"holiday_date\"])\n",
    "\n",
    "# Convert date strings to date types\n",
    "ticket_df = ticket_df.withColumn(\"create_date\", col(\"create_date\").cast(\"date\"))\n",
    "ticket_df = ticket_df.withColumn(\"resolved_date\", col(\"resolved_date\").cast(\"date\"))\n",
    "holiday_df = holiday_df.withColumn(\"holiday_date\", col(\"holiday_date\").cast(\"date\"))\n",
    "\n",
    "working_days = ticket_df.groupby(\"ticket_id\", \"create_date\", \"resolved_date\").agg(\n",
    "                        datediff(col(\"resolved_date\"), col(\"create_date\") + 1).alias(\"business_days\"))\n",
    "\n",
    "working_days.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8c03a8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "| id|     name|gender|\n",
      "+---+---------+------+\n",
      "|107|     Days|     F|\n",
      "|145| Hawbaker|     M|\n",
      "|155|   Hansel|     F|\n",
      "|202|Blackston|     M|\n",
      "|227|    Criss|     F|\n",
      "|278|   Keffer|     M|\n",
      "|305|    Canty|     M|\n",
      "|329|  Mozingo|     M|\n",
      "|425|     Nolf|     M|\n",
      "|534|    Waugh|     M|\n",
      "|586|     Tong|     M|\n",
      "|618|Dimartino|     M|\n",
      "|747|    Beane|     M|\n",
      "|878|  Chatmon|     F|\n",
      "|904|  Hansard|     F|\n",
      "+---+---------+------+\n",
      "\n",
      "+----+----+\n",
      "|c_id|p_id|\n",
      "+----+----+\n",
      "| 145| 202|\n",
      "| 145| 107|\n",
      "| 278| 305|\n",
      "| 278| 155|\n",
      "| 329| 425|\n",
      "| 329| 227|\n",
      "| 534| 586|\n",
      "| 534| 878|\n",
      "| 618| 747|\n",
      "| 618| 904|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7119597076719509504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119597076719509504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# ✅Write a query that prints the names of a child and his parents in individual columns respectively \n",
    "# in order of the name of the child as shown in the picture.\n",
    "\n",
    "schema1 = [\"id\", \"name\", \"gender\"]\n",
    "data1 = [(107,'Days','F'), (145,'Hawbaker','M'),\n",
    "(155,'Hansel','F'), (202,'Blackston','M'), (227,'Criss','F'),\n",
    "(278,'Keffer','M'), (305,'Canty','M'),\n",
    "(329,'Mozingo','M'), (425,'Nolf','M'), \n",
    "(534,'Waugh','M'), (586,'Tong','M'),\n",
    "(618,'Dimartino','M'), (747,'Beane','M'),\n",
    "(878,'Chatmon','F'), (904,'Hansard','F')]\n",
    "\n",
    "schema2 = [\"c_id\", \"p_id\"]\n",
    "data2 = [(145, 202),(145, 107),\n",
    "(278,305),(278,155),\n",
    "(329, 425),(329,227),\n",
    "(534,586),(534,878),\n",
    "(618,747),(618,904)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42740d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+\n",
      "|c_id|p_id|     name|\n",
      "+----+----+---------+\n",
      "| 145| 202| Hawbaker|\n",
      "| 145| 107| Hawbaker|\n",
      "| 278| 305|   Keffer|\n",
      "| 278| 155|   Keffer|\n",
      "| 329| 425|  Mozingo|\n",
      "| 329| 227|  Mozingo|\n",
      "| 534| 586|    Waugh|\n",
      "| 534| 878|    Waugh|\n",
      "| 618| 747|Dimartino|\n",
      "| 618| 904|Dimartino|\n",
      "+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftjoin = df2.join(df1, df1.id == df2.c_id, \"left\").select(\"c_id\", \"p_id\", \"name\")\n",
    "leftjoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e381448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "| id|     name|gender|\n",
      "+---+---------+------+\n",
      "|107|     Days|     F|\n",
      "|155|   Hansel|     F|\n",
      "|202|Blackston|     M|\n",
      "|227|    Criss|     F|\n",
      "|305|    Canty|     M|\n",
      "|425|     Nolf|     M|\n",
      "|586|     Tong|     M|\n",
      "|747|    Beane|     M|\n",
      "|904|  Hansard|     F|\n",
      "|878|  Chatmon|     F|\n",
      "+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# removing child ids\n",
    "leftantijoin = df1.join(df2, df2.c_id == df1.id, \"left_anti\")\n",
    "leftantijoin.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "27768524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "| id|  mothers|gender|\n",
      "+---+---------+------+\n",
      "|202|Blackston|     M|\n",
      "|305|    Canty|     M|\n",
      "|425|     Nolf|     M|\n",
      "|586|     Tong|     M|\n",
      "|747|    Beane|     M|\n",
      "+---+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mothers = leftantijoin.filter(leftantijoin.gender == \"M\").withColumnRenamed(\"name\", \"mothers\")\n",
    "mothers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2c204d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|fathers|gender|\n",
      "+---+-------+------+\n",
      "|107|   Days|     F|\n",
      "|155| Hansel|     F|\n",
      "|227|  Criss|     F|\n",
      "|904|Hansard|     F|\n",
      "|878|Chatmon|     F|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fathers = leftantijoin.filter(leftantijoin.gender == \"F\").withColumnRenamed(\"name\", \"fathers\")\n",
    "fathers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94d5cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+----+---------+------+----+-------+------+\n",
      "|c_id|p_id|     name|  id|  mothers|gender|  id|fathers|gender|\n",
      "+----+----+---------+----+---------+------+----+-------+------+\n",
      "| 145| 202| Hawbaker| 202|Blackston|     M|null|   null|  null|\n",
      "| 145| 107| Hawbaker|null|     null|  null| 107|   Days|     F|\n",
      "| 278| 305|   Keffer| 305|    Canty|     M|null|   null|  null|\n",
      "| 278| 155|   Keffer|null|     null|  null| 155| Hansel|     F|\n",
      "| 329| 425|  Mozingo| 425|     Nolf|     M|null|   null|  null|\n",
      "| 329| 227|  Mozingo|null|     null|  null| 227|  Criss|     F|\n",
      "| 534| 586|    Waugh| 586|     Tong|     M|null|   null|  null|\n",
      "| 534| 878|    Waugh|null|     null|  null| 878|Chatmon|     F|\n",
      "| 618| 747|Dimartino| 747|    Beane|     M|null|   null|  null|\n",
      "| 618| 904|Dimartino|null|     null|  null| 904|Hansard|     F|\n",
      "+----+----+---------+----+---------+------+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leftmothers = leftjoin.join(mothers, leftjoin.p_id == mothers.id, \"left\")\n",
    "leftfathers = leftmothers.join(fathers, leftmothers.p_id == fathers.id, \"left\").orderBy(\"c_id\")\n",
    "\n",
    "leftfathers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b1a11e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---------+-------+\n",
      "|c_id|p_id|     name|  mothers|fathers|\n",
      "+----+----+---------+---------+-------+\n",
      "| 145| 202| Hawbaker|Blackston|   null|\n",
      "| 145| 107| Hawbaker|     null|   Days|\n",
      "| 278| 305|   Keffer|    Canty|   null|\n",
      "| 278| 155|   Keffer|     null| Hansel|\n",
      "| 329| 425|  Mozingo|     Nolf|   null|\n",
      "| 329| 227|  Mozingo|     null|  Criss|\n",
      "| 534| 586|    Waugh|     Tong|   null|\n",
      "| 534| 878|    Waugh|     null|Chatmon|\n",
      "| 618| 747|Dimartino|    Beane|   null|\n",
      "| 618| 904|Dimartino|     null|Hansard|\n",
      "+----+----+---------+---------+-------+\n",
      "\n",
      "+---------+---------+--------+\n",
      "|     name|  mothers|fathers1|\n",
      "+---------+---------+--------+\n",
      "| Hawbaker|Blackston|    Days|\n",
      "| Hawbaker|Blackston|    null|\n",
      "| Hawbaker|     null|    Days|\n",
      "| Hawbaker|     null|    null|\n",
      "|   Keffer|    Canty|    null|\n",
      "|   Keffer|    Canty|  Hansel|\n",
      "|   Keffer|     null|    null|\n",
      "|   Keffer|     null|  Hansel|\n",
      "|  Mozingo|     Nolf|    null|\n",
      "|  Mozingo|     Nolf|   Criss|\n",
      "|  Mozingo|     null|    null|\n",
      "|  Mozingo|     null|   Criss|\n",
      "|    Waugh|     Tong| Chatmon|\n",
      "|    Waugh|     Tong|    null|\n",
      "|    Waugh|     null| Chatmon|\n",
      "|    Waugh|     null|    null|\n",
      "|Dimartino|    Beane| Hansard|\n",
      "|Dimartino|    Beane|    null|\n",
      "|Dimartino|     null| Hansard|\n",
      "|Dimartino|     null|    null|\n",
      "+---------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2 = leftfathers.select(\"c_id\", \"p_id\", \"name\",\"mothers\", \"fathers\")\n",
    "data2.show()\n",
    "\n",
    "data22 = data2.withColumnRenamed(\"fathers\", \"fathers1\").withColumnRenamed(\"name\", \"name1\")\\\n",
    ".withColumnRenamed(\"p_id\", \"pid1\").withColumnRenamed(\"mothers\", \"mothers1\")\n",
    "\n",
    "data3 = data2.join(data22, on=\"c_id\", how=\"inner\").select(\"name\", \"mothers\", \"fathers1\")\n",
    "# data3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c02a79f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+\n",
      "|     name|  mothers|fathers|\n",
      "+---------+---------+-------+\n",
      "|Dimartino|    Beane|Hansard|\n",
      "| Hawbaker|Blackston|   Days|\n",
      "|   Keffer|    Canty| Hansel|\n",
      "|  Mozingo|     Nolf|  Criss|\n",
      "|    Waugh|     Tong|Chatmon|\n",
      "+---------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = data3.filter((col(\"mothers\") != \"null\") & (col(\"fathers1\") != \"null\"))\n",
    "final.selectExpr(\"name\", \"mothers\", \"fathers1 as fathers\").orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d81104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+\n",
      "|c_id|Mother_Name|Father_Name|\n",
      "+----+-----------+-----------+\n",
      "| 145|       Days|  Blackston|\n",
      "| 278|     Hansel|      Canty|\n",
      "| 329|      Criss|       Nolf|\n",
      "| 534|    Chatmon|       Tong|\n",
      "| 618|    Hansard|      Beane|\n",
      "+----+-----------+-----------+\n",
      "\n",
      "+---+---------+-----------+-----------+\n",
      "| id|     name|Mother_Name|Father_Name|\n",
      "+---+---------+-----------+-----------+\n",
      "|145| Hawbaker|       Days|  Blackston|\n",
      "|278|   Keffer|     Hansel|      Canty|\n",
      "|329|  Mozingo|      Criss|       Nolf|\n",
      "|534|    Waugh|    Chatmon|       Tong|\n",
      "|618|Dimartino|    Hansard|      Beane|\n",
      "+---+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# easy way\n",
    "\n",
    "parentsDF = df1.join(df2, col(\"p_id\") == col(\"id\")).groupBy(\"c_id\").agg(\n",
    " max(when(col(\"gender\") == 'F', col(\"name\"))).alias(\"Mother_Name\"),\n",
    " max(when(col(\"gender\") == 'M', col(\"name\"))).alias(\"Father_Name\"))\n",
    "parentsDF.show()\n",
    "\n",
    "resultDF = parentsDF.join(df1, parentsDF.c_id == col(\"id\")).select(\"id\", \"name\", \"Mother_Name\",\"Father_Name\")\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9ff99d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith father_mother as\\n(\\nselect a.c_id ,  \\n  max(case when b.gender = 'M' then b.name end) as father , \\n  max(case when b.gender = 'F' then b.name end) as mother \\nfrom relations as a inner join people as b \\non a.p_id = b.id\\ngroup by a.c_id\\n)\\nselect \\na.name as child , \\nb.father , \\nb.mother \\nfrom people as a inner join father_mother as b on a.id = b.c_id\\norder by a.name ;\\n\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SQL Solution:\n",
    "\"\"\"\n",
    "with father_mother as(\n",
    "        select a.c_id ,  \n",
    "          max(case when b.gender = 'M' then b.name end) as father , \n",
    "          max(case when b.gender = 'F' then b.name end) as mother \n",
    "        from relations as a inner join people as b \n",
    "        on a.p_id = b.id\n",
    "        group by a.c_id\n",
    ")\n",
    "select a.name as child , b.father , b.mother \n",
    "    from people as a inner join father_mother as b on a.id = b.c_id\n",
    "    order by a.name ;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87cce11",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7119912119998840832/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119912119998840832%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "## Question :- Explain SCD Type 1 and 2 with a table structure!!!\n",
    "\n",
    "### 🤔 What are SCDs ?\n",
    "\n",
    "Slowly Changing Dimensions (SCDs) are a way to manage changes in dimension tables in a data warehouse. Dimension tables store descriptive information about the fact tables in a data warehouse, such as customer, product, and time. SCDs are important because they allow you to track changes to dimension attributes over time, which is essential for accurate data analysis.\n",
    "```\n",
    "💁‍♂️ There are six main types of SCDs, However SCD Type 1 and SCD Type 2 are most commonly used.\n",
    "\n",
    "👉 Type 0 SCD: The dimension table is never updated. \n",
    "    This is the simplest type of SCD, but it is also the least informative.\n",
    "\n",
    "👉 Type 1 SCD: The dimension table is updated directly with the new value. \n",
    "    This type of SCD is easy to implement, but it does not track any historical data.\n",
    "\n",
    "👉 Type 2 SCD: A new row is created in the dimension table for each change. \n",
    "    This type of SCD provides a complete history of all changes, but it can lead to table bloat.\n",
    "```\n",
    "🙌 Checkout the image which explains with Table Structure.\n",
    "\n",
    "The best type of SCD to use depends on the specific needs of your data warehouse. If you need to track all historical changes, then you will need to use a Type 2\n",
    "```\n",
    "Here are some examples of how SCDs can be used:\n",
    "💎Tracking changes in customer addresses over time\n",
    "💎Tracking changes in product prices over time\n",
    "💎Tracking changes in employee job titles over time\n",
    "```\n",
    "SCDs are an essential part of any data warehouse. By understanding the different types of SCDs and how to use them, you can ensure that your data warehouse is accurate and informative.\n",
    "\n",
    "<img src=\"https://media.licdn.com/dms/image/D5622AQGWE3MARRNCnw/feedshare-shrink_800/0/1697519329857?e=1707350400&v=beta&t=7zeRHKxWHssy9ZsJkJUT2iktmDc0Ydh-mw7j_LFFqp0\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7775a775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|     Toycar1|2000-01-16|\n",
      "|      2|    toYcar2 |2000-01-17|\n",
      "|      3|     toycaR3|2000-02-18|\n",
      "|      4|      doll1 |2000-02-19|\n",
      "|      5|       doll2|2000-02-28|\n",
      "|      6|      data  |2000-03-31|\n",
      "|      7|      doll1 |2000-02-19|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7119585361919418368/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119585361919418368%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"sale_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the data as a list of rows\n",
    "data = [\n",
    "    (1, \" Toycar1\", \"2000-01-16\"),\n",
    "    (2, \"toYcar2 \", \"2000-01-17\"),\n",
    "    (3, \" toycaR3\", \"2000-02-18\"),\n",
    "    (4, \" doll1 \", \"2000-02-19\"),\n",
    "    (5, \" doll2\", \"2000-02-28\"),\n",
    "    (6, \"data  \", \"2000-03-31\"),\n",
    "    (7, \"doll1 \", \"2000-02-19\")\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert the \"sale_date\" column to the DateType\n",
    "df = df.withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "69c13377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|     toycar1|2000-01-16|\n",
      "|      2|     toycar2|2000-01-17|\n",
      "|      3|     toycar3|2000-02-18|\n",
      "|      4|       doll1|2000-02-19|\n",
      "|      5|       doll2|2000-02-28|\n",
      "|      6|        data|2000-03-31|\n",
      "|      7|       doll1|2000-02-19|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 𝐜𝐨𝐧𝐯𝐞𝐫𝐭 𝐩𝐫𝐨𝐝𝐮𝐜𝐭_𝐧𝐚𝐦𝐞 𝐢𝐧 𝐥𝐨𝐰𝐞𝐫𝐜𝐚𝐬𝐞 𝐰𝐢𝐭𝐡𝐨𝐮𝐭 𝐥𝐞𝐚𝐝𝐢𝐧𝐠 𝐨𝐫 𝐭𝐫𝐚𝐢𝐥𝐢𝐧𝐠 𝐰𝐡𝐢𝐭𝐞 𝐬𝐩𝐚𝐜𝐞𝐬.\n",
    "\n",
    "df1 = df.withColumn(\"product_name\", lower(trim(col(\"product_name\"))))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b029867b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+---------+\n",
      "|sale_id|product_name|sale_date|\n",
      "+-------+------------+---------+\n",
      "|      1|     toycar1|  2000-01|\n",
      "|      2|     toycar2|  2000-01|\n",
      "|      3|     toycar3|  2000-02|\n",
      "|      4|       doll1|  2000-02|\n",
      "|      5|       doll2|  2000-02|\n",
      "|      6|        data|  2000-03|\n",
      "|      7|       doll1|  2000-02|\n",
      "+-------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. 𝐬𝐞𝐭 𝐬𝐚𝐥𝐞_𝐝𝐚𝐭𝐞 𝐢𝐧 𝐭𝐡𝐞 𝐟𝐨𝐫𝐦𝐚𝐭 ('𝐘𝐘𝐘𝐘-𝐌𝐌').\n",
    "\n",
    "df2 = df1.withColumn(\"sale_date\", date_format(col(\"sale_date\"), \"yyyy-MM\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "953b0a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----+\n",
      "|product_name|sale_date|count|\n",
      "+------------+---------+-----+\n",
      "|        data|  2000-03|    1|\n",
      "|       doll1|  2000-02|    2|\n",
      "|       doll2|  2000-02|    1|\n",
      "|     toycar1|  2000-01|    1|\n",
      "|     toycar2|  2000-01|    1|\n",
      "|     toycar3|  2000-02|    1|\n",
      "+------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. 𝐜𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐨𝐭𝐚𝐥 𝐭𝐡𝐞 𝐧𝐮𝐦𝐛𝐞𝐫 𝐨𝐟 𝐭𝐢𝐦𝐞𝐬 𝐭𝐡𝐞 𝐩𝐫𝐨𝐝𝐮𝐜𝐭 𝐰𝐚𝐬 𝐬𝐨𝐥𝐝 𝐢𝐧 𝐭𝐡𝐢𝐬 𝐦𝐨𝐧𝐭𝐡.\n",
    "# 𝐑𝐞𝐭𝐮𝐫𝐧 𝐭𝐡𝐞 𝐫𝐞𝐬𝐮𝐥𝐭 𝐭𝐚𝐛𝐥𝐞 𝐨𝐫𝐝𝐞𝐫𝐞𝐝 𝐛𝐲 𝐩𝐫𝐨𝐝𝐮𝐜𝐭_𝐧𝐚𝐦𝐞 𝐢𝐧 𝐚𝐬𝐜𝐞𝐧𝐝𝐢𝐧𝐠 𝐨𝐫𝐝𝐞𝐫. \n",
    "# 𝐈𝐧 𝐜𝐚𝐬𝐞 𝐨𝐟 𝐚 𝐭𝐢𝐞, 𝐨𝐫𝐝𝐞𝐫 𝐢𝐭 𝐛𝐲 𝐬𝐚𝐥𝐞_𝐝𝐚𝐭𝐞 𝐢𝐧 𝐚𝐬𝐜𝐞𝐧𝐝𝐢𝐧𝐠 𝐨𝐫𝐝𝐞𝐫.\n",
    "\n",
    "df3 = df2.groupBy(\"product_name\", \"sale_date\").agg(count(\"product_name\").alias(\"count\"))\\\n",
    "                            .orderBy(\"product_name\", \"sale_date\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6cd8242f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '🎯' (U+1F3AF) (2015875050.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[126], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    🎯show me the distinct number of users.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '🎯' (U+1F3AF)\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7120995100997939200/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120995100997939200%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "# 📌Question: Given one list of events like below example.\n",
    "# write in the RDD form.                \n",
    "                \n",
    "# 🎯show me the distinct number of users.\n",
    "# 🎯show me the how many number of events.\n",
    "# 🎯show me the which users having most events.\n",
    "\n",
    "# 🍀For the above tasks, \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d5d9c3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|user1 click 06:04:00|\n",
      "|     user2 log ERROR|\n",
      "|user1 click 04:09:08|\n",
      "|user1 click 00:00:01|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user1 click 06:04:00,user2 log ERROR,user1 click 04:09:08,user1 click 00:00:01\n",
      "user1 click 06:04:00\n",
      "user2 log ERROR\n",
      "user1 click 04:09:08\n",
      "user1 click 00:00:01\n"
     ]
    }
   ],
   "source": [
    "# using RDD \n",
    "\n",
    "rdd = sc.textFile(Location + \"dummy.txt\")\n",
    "rdd.foreach(print)\n",
    "\n",
    "splitData = rdd.flatMap( lambda x : x.split(\",\"))\n",
    "splitData.foreach(print)\n",
    "\n",
    "Rowdata = splitData.map(lambda x : Row(x))\n",
    "# Rowdata.foreach(print)\n",
    "\n",
    "schema_ = StructType([\n",
    "        StructField(\"value\", StringType(), True)\n",
    "])\n",
    "\n",
    "RowDf = spark.createDataFrame(data=Rowdata, schema=schema_)\n",
    "RowDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1ee3cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               event| user|\n",
      "+--------------------+-----+\n",
      "|user1 click 06:04:00|user1|\n",
      "|     user2 log ERROR|user2|\n",
      "|user1 click 04:09:08|user1|\n",
      "|user1 click 00:00:01|user1|\n",
      "+--------------------+-----+\n",
      "\n",
      "2\n",
      "Distinct number of users: 2\n",
      "Number of events: 4\n",
      "users with most events: Row(user='user1', event_count=3)\n"
     ]
    }
   ],
   "source": [
    "# using Dataframe (DSL)\n",
    "\n",
    "list1 = [('user1 click 06:04:00',), ('user2 log ERROR',), \n",
    "         ('user1 click 04:09:08',), ('user1 click 00:00:01',)]\n",
    "\n",
    "df = spark.createDataFrame(list1, [\"event\"])\n",
    "\n",
    "df = df.withColumn(\"user\", split(df.event, \" \")[0])\n",
    "df.show()\n",
    "\n",
    "# Q1) show me the distinct number of users.\n",
    "\n",
    "distinct_users = df.select(\"user\").distinct().count()\n",
    "print(distinct_users)\n",
    "\n",
    "# Q2) show me the how many number of events.\n",
    "event_count = df.count()\n",
    "\n",
    "# Q3) show me the which users having most events.\n",
    "most_event_users = df.groupby(\"user\").agg(count(\"*\").alias(\"event_count\"))\\\n",
    "                    .orderBy(\"event_count\", ascending=False).first()\n",
    "\n",
    "print(\"Distinct number of users:\", distinct_users)\n",
    "print(\"Number of events:\", event_count)\n",
    "print(\"users with most events:\", most_event_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0365d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+---------+\n",
      "|teamID|memberID|Criteria1|Criteria2|\n",
      "+------+--------+---------+---------+\n",
      "|    T1| T1_mbr1|        Y|        Y|\n",
      "|    T1| T1_mbr2|        Y|        Y|\n",
      "|    T1| T1_mbr3|        Y|        Y|\n",
      "|    T1| T1_mbr4|        Y|        Y|\n",
      "|    T1| T1_mbr5|        Y|        N|\n",
      "|    T2| T2_mbr1|        Y|        Y|\n",
      "|    T2| T2_mbr2|        Y|        N|\n",
      "|    T2| T2_mbr3|        N|        Y|\n",
      "|    T2| T2_mbr4|        N|        N|\n",
      "|    T2| T2_mbr5|        N|        N|\n",
      "|    T3| T3_mbr1|        Y|        Y|\n",
      "|    T3| T3_mbr2|        Y|        Y|\n",
      "|    T3| T3_mbr3|        N|        Y|\n",
      "|    T3| T3_mbr4|        N|        Y|\n",
      "|    T3| T3_mbr5|        Y|        N|\n",
      "+------+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7120618783588687872/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120618783588687872%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data = [(\"T1\", \"T1_mbr1\", \"Y\", \"Y\"),\n",
    " (\"T1\", \"T1_mbr2\", \"Y\", \"Y\"),\n",
    " (\"T1\", \"T1_mbr3\", \"Y\", \"Y\"),\n",
    " (\"T1\", \"T1_mbr4\", \"Y\", \"Y\"),\n",
    " (\"T1\", \"T1_mbr5\", \"Y\", \"N\"),\n",
    " (\"T2\", \"T2_mbr1\", \"Y\", \"Y\"),\n",
    " (\"T2\", \"T2_mbr2\", \"Y\", \"N\"),\n",
    " (\"T2\", \"T2_mbr3\", \"N\", \"Y\"),\n",
    " (\"T2\", \"T2_mbr4\", \"N\", \"N\"),\n",
    " (\"T2\", \"T2_mbr5\", \"N\", \"N\"),\n",
    " (\"T3\", \"T3_mbr1\", \"Y\", \"Y\"),\n",
    " (\"T3\", \"T3_mbr2\", \"Y\", \"Y\"),\n",
    " (\"T3\", \"T3_mbr3\", \"N\", \"Y\"),\n",
    " (\"T3\", \"T3_mbr4\", \"N\", \"Y\"),\n",
    " (\"T3\", \"T3_mbr5\", \"Y\", \"N\")]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"teamID\", \"memberID\", \"Criteria1\", \"Criteria2\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0807d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+---------+\n",
      "|teamID|memberID|Criteria1|Criteria2|\n",
      "+------+--------+---------+---------+\n",
      "|    T1| T1_mbr1|        Y|        Y|\n",
      "|    T1| T1_mbr2|        Y|        Y|\n",
      "|    T1| T1_mbr3|        Y|        Y|\n",
      "|    T1| T1_mbr4|        Y|        Y|\n",
      "|    T2| T2_mbr1|        Y|        Y|\n",
      "|    T3| T3_mbr1|        Y|        Y|\n",
      "|    T3| T3_mbr2|        Y|        Y|\n",
      "+------+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. for a member criteria 1 and criteria 2 should be Y\n",
    "\n",
    "filterdata = df.filter((col(\"Criteria1\") == \"Y\") & (col(\"Criteria2\") == \"Y\"))\n",
    "filterdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eabf904f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|teamID|count|\n",
      "+------+-----+\n",
      "|    T1|    4|\n",
      "|    T3|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. there must be at least 2 members of that team that qualifies then only a team qualifies\n",
    "\n",
    "qualifyData = filterdata.groupBy(\"teamID\").agg(count(\"memberID\").alias(\"count\")).filter(col(\"count\") >= 2)\n",
    "qualifyData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20bfb5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----+\n",
      "|teamID|memberID|count|\n",
      "+------+--------+-----+\n",
      "|    T1| T1_mbr1|    4|\n",
      "|    T1| T1_mbr2|    4|\n",
      "|    T1| T1_mbr3|    4|\n",
      "|    T1| T1_mbr4|    4|\n",
      "|    T2| T2_mbr1| null|\n",
      "|    T3| T3_mbr1|    2|\n",
      "|    T3| T3_mbr2|    2|\n",
      "+------+--------+-----+\n",
      "\n",
      "+------+--------+---------+---------+-----+\n",
      "|teamID|memberID|Criteria1|Criteria2|count|\n",
      "+------+--------+---------+---------+-----+\n",
      "|    T1| T1_mbr1|        Y|        Y|    4|\n",
      "|    T1| T1_mbr2|        Y|        Y|    4|\n",
      "|    T1| T1_mbr3|        Y|        Y|    4|\n",
      "|    T1| T1_mbr4|        Y|        Y|    4|\n",
      "|    T1| T1_mbr5|        Y|        N| null|\n",
      "|    T2| T2_mbr1|        Y|        Y| null|\n",
      "|    T2| T2_mbr2|        Y|        N| null|\n",
      "|    T2| T2_mbr3|        N|        Y| null|\n",
      "|    T2| T2_mbr4|        N|        N| null|\n",
      "|    T2| T2_mbr5|        N|        N| null|\n",
      "|    T3| T3_mbr1|        Y|        Y|    2|\n",
      "|    T3| T3_mbr2|        Y|        Y|    2|\n",
      "|    T3| T3_mbr3|        N|        Y| null|\n",
      "|    T3| T3_mbr4|        N|        Y| null|\n",
      "|    T3| T3_mbr5|        Y|        N| null|\n",
      "+------+--------+---------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf1 = filterdata.join(qualifyData, on=\"teamID\", how=\"left\").select(\"teamID\", \"memberID\", \"count\")\n",
    "finaldf1.show()\n",
    "\n",
    "finaldf = df.join(finaldf1, on=[\"teamID\",\"memberID\"], how=\"full\") # use left join also\n",
    "finaldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0850ba21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------+---------+---------+\n",
      "|teamID|memberID|Criteria1|Criteria2|Qualified|\n",
      "+------+--------+---------+---------+---------+\n",
      "|    T1| T1_mbr1|        Y|        Y|        Y|\n",
      "|    T1| T1_mbr2|        Y|        Y|        Y|\n",
      "|    T1| T1_mbr3|        Y|        Y|        Y|\n",
      "|    T1| T1_mbr4|        Y|        Y|        Y|\n",
      "|    T1| T1_mbr5|        Y|        N|        N|\n",
      "|    T2| T2_mbr1|        Y|        Y|        N|\n",
      "|    T2| T2_mbr2|        Y|        N|        N|\n",
      "|    T2| T2_mbr3|        N|        Y|        N|\n",
      "|    T2| T2_mbr4|        N|        N|        N|\n",
      "|    T2| T2_mbr5|        N|        N|        N|\n",
      "|    T3| T3_mbr1|        Y|        Y|        Y|\n",
      "|    T3| T3_mbr2|        Y|        Y|        Y|\n",
      "|    T3| T3_mbr3|        N|        Y|        N|\n",
      "|    T3| T3_mbr4|        N|        Y|        N|\n",
      "|    T3| T3_mbr5|        Y|        N|        N|\n",
      "+------+--------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf2 = finaldf.withColumn(\"Qualified\", expr(\"case when count >= 2 then 'Y' else 'N' end\")).drop(\"count\")\n",
    "finaldf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55eaa906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Add a \"Qualified\" column based on the criteria\n",
    "# #If there are less than 2 qualified members in the team, mark all members as \"N\"\n",
    "# df = df.withColumn(\"Qualified\", when(col(\"teamID\")\\\n",
    "#                             .isin([row.teamID for row in df.filter(col(\"Qualified\") == \"Y\")\\\n",
    "#                             .groupby(\"teamID\").count().filter(col(\"count\") < 2).collect()]), \"N\")\\\n",
    "#                             .otherwise(col(\"Qualified\")))\n",
    "\n",
    "# # Sort the DataFrame by teamID and memberID\n",
    "# # Show all rows with the updated \"Qualified\" column, excluding \"qualified_members column\n",
    "\n",
    "# df = df.orderBy(\"teamID\", \"memberID\")\n",
    "# df.select(\"teamID\", \"memberID\", \"Criterial\", \"Criteria2\", \"Qualified\").show()\n",
    "\n",
    "#  ------------------------------------------------------------------------------------\n",
    "\n",
    "# result = df.withColumn(\"output\", when(\n",
    "# ((col(\"Criteria1\" == \"Y\")) & (col(\"Criteria2\" == \"Y\"))) & \\\n",
    "# sum(when(((col(\"Criteria1\" == \"Y\")) & (col(\"Criteria2\" == \"Y\"))),1)).otherwise(0))\\\n",
    "# .over(Window.partitionBy(\"teamID\")) >= 2, \"Y\").otherwise(\"N\")\n",
    "\n",
    "# result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c547642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c8394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL approach:\n",
    "\n",
    "# WITH cte c AS (\n",
    "#         SELECT team_id,\n",
    "#         COUNT(team_id) AS total_qual\n",
    "#         FROM ameriprise_llc\n",
    "#         WHERE criteria1 = 'Y' AND criteria2 = 'Y'\n",
    "#         GROUP BY 1\n",
    "# )\n",
    "# SELECT c.team_id, a.member_id, a.criteria1, a.crtiteria2\n",
    "#     CASE (WHEN a.criteria1 = 'Y' AND a,criterai2 = 'Y' AND \n",
    "#                       total_qual > 1 THEN 'Y' ELSE 'N' END) AS qualifiers\n",
    "#     FROM cte c\n",
    "#     RIGHT JOIN ameriprise_llc a\n",
    "#     ON a.team_id = c.team_id;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f5f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n",
      "|order_id|customer_id|order_date|total_amount|\n",
      "+--------+-----------+----------+------------+\n",
      "|       1|  Customer1|2023-01-10|         100|\n",
      "|       2|  Customer2|2023-01-11|         150|\n",
      "|       3|  Customer1|2023-01-12|          80|\n",
      "|       4|  Customer3|2023-01-12|         200|\n",
      "|       5|  Customer2|2023-01-13|          50|\n",
      "+--------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7120110212866306048/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120110212866306048%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "# 🔹Total Revenue Analysis\n",
    "# 🔹Highest Value Order\n",
    "# 🔹Top-Spending Customer\n",
    "# 🔹Order Count per Customer\n",
    "\n",
    "schema = [\"order_id\",\"customer_id\",\"order_date\",\"total_amount\"]\n",
    "\n",
    "data = [(1,\"Customer1\",\"2023-01-10\",100),\n",
    "(2,\"Customer2\",\"2023-01-11\",150),\n",
    "(3,\"Customer1\",\"2023-01-12\",80),\n",
    "(4,\"Customer3\",\"2023-01-12\",200),\n",
    "(5,\"Customer2\",\"2023-01-13\",50)]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf2875cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue: 580\n",
      "Highest Value Order :  200\n",
      "customer id :  4\n",
      "+-----------+-----+-----------+\n",
      "|customer_id|total|order_count|\n",
      "+-----------+-----+-----------+\n",
      "|  Customer1|  180|          2|\n",
      "|  Customer2|  200|          2|\n",
      "|  Customer3|  200|          1|\n",
      "+-----------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Total Revenue Analysis\n",
    "total_revenue = df.selectExpr(\"sum(total_amount)\").collect()[0][0]\n",
    "print(\"Total Revenue:\", total_revenue)\n",
    "\n",
    "# Highest Value Order\n",
    "max_amount = df.selectExpr(\"max(total_amount)\").collect()[0][0]\n",
    "print(\"Highest Value Order : \", max_amount)\n",
    "\n",
    "# 🔹Top-Spending Customer\n",
    "cust = df.filter(col(\"total_amount\") == max_amount).collect()[0][0]\n",
    "print(\"customer id : \", cust)\n",
    "\n",
    "# 🔹Order Count per Customer\n",
    "\n",
    "df3 = df.groupBy(\"customer_id\").agg(sum(\"total_amount\").alias(\"total\"), count(\"order_date\").alias(\"order_count\"))\n",
    "df3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa615c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+\n",
      "| id|     name|gender|\n",
      "+---+---------+------+\n",
      "|107|     Days|     F|\n",
      "|145| Hawbaker|     M|\n",
      "|155|   Hansel|     F|\n",
      "|202|Blackston|     M|\n",
      "|227|    Criss|     F|\n",
      "|278|   Keffer|     M|\n",
      "|305|    Canty|     M|\n",
      "|329|  Mozingo|     M|\n",
      "|425|     Nolf|     M|\n",
      "|534|    Waugh|     M|\n",
      "|586|     Tong|     M|\n",
      "|618|Dimartino|     M|\n",
      "|747|    Beane|     M|\n",
      "|878|  Chatmon|     F|\n",
      "|904|  Hansard|     F|\n",
      "+---+---------+------+\n",
      "\n",
      "+----+----+\n",
      "|c_id|p_id|\n",
      "+----+----+\n",
      "| 145| 202|\n",
      "| 145| 107|\n",
      "| 278| 305|\n",
      "| 278| 155|\n",
      "| 329| 425|\n",
      "| 329| 227|\n",
      "| 534| 586|\n",
      "| 534| 878|\n",
      "| 618| 747|\n",
      "| 618| 904|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7120256705694736385/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120256705694736385%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema1 = [\"id\", \"name\", \"gender\"]\n",
    "data1 = [(107,'Days','F'), (145,'Hawbaker','M'),\n",
    "(155,'Hansel','F'), (202,'Blackston','M'), (227,'Criss','F'),\n",
    "(278,'Keffer','M'), (305,'Canty','M'),\n",
    "(329,'Mozingo','M'), (425,'Nolf','M'), \n",
    "(534,'Waugh','M'), (586,'Tong','M'),\n",
    "(618,'Dimartino','M'), (747,'Beane','M'),\n",
    "(878,'Chatmon','F'), (904,'Hansard','F')]\n",
    "\n",
    "schema2 = [\"c_id\", \"p_id\"]\n",
    "data2 = [(145, 202),(145, 107),\n",
    "(278,305),(278,155),\n",
    "(329, 425),(329,227),\n",
    "(534,586),(534,878),\n",
    "(618,747),(618,904)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db1242d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+----+----+\n",
      "| id|     name|gender|c_id|p_id|\n",
      "+---+---------+------+----+----+\n",
      "|107|     Days|     F| 145| 107|\n",
      "|155|   Hansel|     F| 278| 155|\n",
      "|202|Blackston|     M| 145| 202|\n",
      "|227|    Criss|     F| 329| 227|\n",
      "|305|    Canty|     M| 278| 305|\n",
      "|425|     Nolf|     M| 329| 425|\n",
      "|586|     Tong|     M| 534| 586|\n",
      "|747|    Beane|     M| 618| 747|\n",
      "|878|  Chatmon|     F| 534| 878|\n",
      "|904|  Hansard|     F| 618| 904|\n",
      "+---+---------+------+----+----+\n",
      "\n",
      "+----+-----------+-----------+\n",
      "|c_id|Mother_Name|Father_Name|\n",
      "+----+-----------+-----------+\n",
      "| 145|       Days|  Blackston|\n",
      "| 278|     Hansel|      Canty|\n",
      "| 329|      Criss|       Nolf|\n",
      "| 534|    Chatmon|       Tong|\n",
      "| 618|    Hansard|      Beane|\n",
      "+----+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parentsDF = df1.join(df2, col(\"p_id\") == col(\"id\"))\n",
    "parentsDF.show()\n",
    "\n",
    "parentsDF = parentsDF.groupBy(\"c_id\").agg(\n",
    " max(when(col(\"gender\") == 'F', col(\"name\"))).alias(\"Mother_Name\"),\n",
    " max(when(col(\"gender\") == 'M', col(\"name\"))).alias(\"Father_Name\"))\n",
    "parentsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20af9887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------+-----------+\n",
      "| id|     name|Mother_Name|Father_Name|\n",
      "+---+---------+-----------+-----------+\n",
      "|145| Hawbaker|       Days|  Blackston|\n",
      "|278|   Keffer|     Hansel|      Canty|\n",
      "|329|  Mozingo|      Criss|       Nolf|\n",
      "|534|    Waugh|    Chatmon|       Tong|\n",
      "|618|Dimartino|    Hansard|      Beane|\n",
      "+---+---------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF = parentsDF.join(df1, parentsDF.c_id == col(\"id\")).select(\"id\", \"name\", \"Mother_Name\",\"Father_Name\")\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65fa8ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+\n",
      "|  sms_date|person1|person2|sms_no|\n",
      "+----------+-------+-------+------+\n",
      "|2020-04-01|Avinash| Vibhor|    10|\n",
      "|2020-04-01| Vibhor|Avinash|    20|\n",
      "|2020-04-01|Avinash|  Pawan|    30|\n",
      "|2020-04-01|  Pawan|Avinash|    20|\n",
      "|2020-04-01| Vibhor|  Pawan|     5|\n",
      "|2020-04-01|  Pawan| Vibhor|     8|\n",
      "|2020-04-01| Vibhor| Deepak|    50|\n",
      "+----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7121112905302863873/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121112905302863873%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "  ('2020-04-01', 'Avinash', 'Vibhor', 10),\n",
    "  ('2020-04-01', 'Vibhor', 'Avinash', 20),\n",
    "  ('2020-04-01', 'Avinash', 'Pawan', 30),\n",
    "  ('2020-04-01', 'Pawan', 'Avinash', 20),\n",
    "  ('2020-04-01', 'Vibhor', 'Pawan', 5),\n",
    "  ('2020-04-01', 'Pawan', 'Vibhor', 8),\n",
    "  ('2020-04-01', 'Vibhor', 'Deepak', 50)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"sms_date\", \"person1\", \"person2\", \"sms_no\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1baf05df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+---------+\n",
      "|  sms_date|person1|person2|Total SMS|\n",
      "+----------+-------+-------+---------+\n",
      "|2020-04-01|Avinash| Vibhor|       10|\n",
      "|2020-04-01| Vibhor|Avinash|       20|\n",
      "|2020-04-01|Avinash|  Pawan|       30|\n",
      "|2020-04-01|  Pawan|Avinash|       20|\n",
      "|2020-04-01| Vibhor|  Pawan|        5|\n",
      "|2020-04-01|  Pawan| Vibhor|        8|\n",
      "|2020-04-01| Vibhor| Deepak|       50|\n",
      "+----------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of SMS exchanged between two people on the day\n",
    "total_sms = df.groupBy(\"sms_date\", \"person1\", \"person2\").agg(sum(\"sms_no\").alias(\"Total SMS\"))\n",
    "total_sms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03c81015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------+------+\n",
      "|  sms_date|person1|person2|sms_no|\n",
      "+----------+-------+-------+------+\n",
      "|2020-04-01|Avinash| Vibhor|    10|\n",
      "|2020-04-01|Avinash|Avinash|    20|\n",
      "|2020-04-01|Avinash|  Pawan|    30|\n",
      "|2020-04-01|Avinash|Avinash|    20|\n",
      "|2020-04-01|  Pawan|  Pawan|     5|\n",
      "|2020-04-01|  Pawan| Vibhor|     8|\n",
      "|2020-04-01| Deepak| Deepak|    50|\n",
      "+----------+-------+-------+------+\n",
      "\n",
      "+----------+-------+-------+------+\n",
      "|  sms_date|person1|person2|sms_no|\n",
      "+----------+-------+-------+------+\n",
      "|2020-04-01|Avinash| Vibhor|    10|\n",
      "|2020-04-01|Avinash|Avinash|    20|\n",
      "|2020-04-01|Avinash|  Pawan|    30|\n",
      "|2020-04-01|Avinash|Avinash|    20|\n",
      "|2020-04-01|  Pawan|  Pawan|     5|\n",
      "|2020-04-01|  Pawan| Vibhor|     8|\n",
      "|2020-04-01| Deepak| Deepak|    50|\n",
      "+----------+-------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ensure consistent order of person1 and person2\n",
    "df = df.withColumn(\"person1\", when(col(\"person1\") < col(\"person2\"), col(\"person1\")).otherwise(col(\"person2\")))\n",
    "df.show()\n",
    "\n",
    "df = df.withColumn(\"person2\", when(col(\"person1\") < col(\"person2\"), col(\"person2\")).otherwise(col(\"person1\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef1a56d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "|  1|  Alice|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7121905553978593280/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121905553978593280%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Describe various PySpark union operators and their respective applications..\n",
    "\n",
    "# union():\n",
    "# Use:1) This operation combines two DataFrames or RDDs vertically, stacking one on top of the other. \n",
    "#     It keeps all rows, including duplicates.\n",
    "#union\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "df2 = spark.createDataFrame([(3, 'Charlie'), (1, 'Alice')], ['id', 'name'])\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11e78733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|   name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|    Bob|\n",
      "|  3|Charlie|\n",
      "|  1|  Alice|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionAll():\n",
    "# Use: Same as union(). It combines two DataFrames or RDDs vertically and keeps all rows, including duplicates.\n",
    "# #unionAll\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "df2 = spark.createDataFrame([(3, 'Charlie'), (1, 'Alice')], ['id', 'name'])\n",
    "df1.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a3f7a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---------+\n",
      "| id| name|user_name|\n",
      "+---+-----+---------+\n",
      "|  1|Alice|     null|\n",
      "|  2|  Bob|     null|\n",
      "|  3| null|  Charlie|\n",
      "|  1| null|    Alice|\n",
      "+---+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionByName: unionByName() function is also used to combine two or more data frames but it might be used \n",
    "#     to combine dataframes having different schema. This is because it combines data frames by the name \n",
    "#     of the column and not the order of the columns.\n",
    "\n",
    "#unionByName\n",
    "\n",
    "df1 = spark.createDataFrame([(1, 'Alice'), (2, 'Bob')], ['id', 'name'])\n",
    "df2 = spark.createDataFrame([(3, 'Charlie'), (1, 'Alice')], ['id', 'user_name'])\n",
    "result = df1.unionByName(df2, allowMissingColumns=True)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc0ab7",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7122282162531241984/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122282162531241984%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "# Topic: Exploring Essential DateTime Functions in PySpark!\n",
    "\n",
    "In PySpark, we have a powerful toolkit of datetime functions to help us manipulate and analyze temporal data. Let's dive into some fundamental datetime functions and their usage with examples:\n",
    "\n",
    "\n",
    "### 1️⃣ to_date():\n",
    "This function converts a string representation of a date to a DateType. It's great for filtering or grouping by date.\n",
    "```\n",
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn(\"date_column\", to_date(\"date_string_column\", \"yyyy-MM-dd\"))\n",
    "```\n",
    "\n",
    "### 2️⃣ date_add() and date_sub():\n",
    "These functions allow you to add or subtract days from a date.\n",
    "```\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "df = df.withColumn(\"future_date\", date_add(\"date_column\", 7))\n",
    "df = df.withColumn(\"past_date\", date_sub(\"date_column\", 3))\n",
    "```\n",
    "### 3️⃣ datediff():\n",
    "Calculates the difference in days between two dates.\n",
    "```\n",
    "from pyspark.sql.functions import datediff\n",
    "df = df.withColumn(\"date_difference\", datediff(\"date2\", \"date1\"))\n",
    "```\n",
    "### 4️⃣ date_format():\n",
    "This function converts a date to a string with a specified format.\n",
    "```\n",
    "from pyspark.sql.functions import date_format\n",
    "df = df.withColumn(\"formatted_date\", date_format(\"date_column\", \"dd-MM-yyyy\"))\n",
    "```\n",
    "### 5️⃣ trunc():\n",
    "Truncates a date to a specified level (day, month, year).\n",
    "```\n",
    "from pyspark.sql.functions import trunc\n",
    "df = df.withColumn(\"year\", trunc(\"date_column\", \"yyyy\"))\n",
    "df = df.withColumn(\"month\", trunc(\"date_column\", \"MM\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bbba69",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7122199508280606720/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122199508280606720%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "### Q:how to read a csv in standard method in pyspark\n",
    "\n",
    "📢 📢In PySpark, you can read a CSV file using the 'spark.read.csv()' method, which is part of the `SparkSession` object. Here's the standard method to read a CSV file:\n",
    "\n",
    "```\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"CSV Reader\").getOrCreate()\n",
    "\n",
    "# Read CSV file\n",
    "df = spark.read.csv(\"path_to_your_csv_file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "```\n",
    "1. Import Libraries :\n",
    "  - Import the necessary libraries. In this case, we import 'SparkSession' from 'pyspark.sql'.\n",
    "\n",
    "2. Initialize SparkSession :\n",
    "  - Create a 'SparkSession' object named 'spark'. This is the entry point for reading data in Spark.\n",
    "\n",
    "3. Read CSV :\n",
    "  - Use 'spark.read.csv()' to read the CSV file. \n",
    "  - Provide the path to your CSV file as the first argument.\n",
    "  - Set 'header=True' if the first row of the CSV contains the column names. If not, set it to 'False'.\n",
    "  - Set 'inferSchema=True' to let Spark automatically infer the data types of columns.\n",
    "\n",
    "4. Show DataFrame :\n",
    "  - Use 'df.show()' to display the DataFrame contents.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bbeaf8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|emp_id|experience|salary|\n",
      "+------+----------+------+\n",
      "|     1|    Junior| 10000|\n",
      "|     2|    Junior| 15000|\n",
      "|     3|    Junior| 40000|\n",
      "|     4|    Senior| 16000|\n",
      "|     5|    Senior| 20000|\n",
      "|     6|    Senior| 50000|\n",
      "+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7122128627739086848/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122128627739086848%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data = [(1, \"Junior\", 10000),\n",
    "(2, \"Junior\", 15000),\n",
    "(3, \"Junior\", 40000),\n",
    "(4, \"Senior\", 16000),\n",
    "(5, \"Senior\", 20000),\n",
    "(6, \"Senior\", 50000)]\n",
    "\n",
    "columns = [\"emp_id\", \"experience\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e8606d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----------------+\n",
      "|emp_id|experience|salary|cumulative_salary|\n",
      "+------+----------+------+-----------------+\n",
      "|     1|    Junior| 10000|            10000|\n",
      "|     2|    Junior| 15000|            25000|\n",
      "|     4|    Senior| 16000|            41000|\n",
      "|     5|    Senior| 20000|            61000|\n",
      "|     3|    Junior| 40000|           101000|\n",
      "|     6|    Senior| 50000|           151000|\n",
      "+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cumulative salary for each candidate\n",
    "df = df.withColumn(\"cumulative_salary\", sum(col(\"salary\")).over(Window.orderBy(col(\"salary\"))))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0420a4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----------------+\n",
      "|emp_id|experience|salary|cumulative_salary|\n",
      "+------+----------+------+-----------------+\n",
      "|     4|    Senior| 16000|            41000|\n",
      "|     5|    Senior| 20000|            61000|\n",
      "+------+----------+------+-----------------+\n",
      "\n",
      "34000\n"
     ]
    }
   ],
   "source": [
    "# ❓Hire the candidates who fall under budget of 70000 according to below criteria:\n",
    "# 📌 First hire Senior within budget\n",
    "\n",
    "budget = 70000\n",
    "\n",
    "senior_hires = df.filter((col(\"experience\") == \"Senior\") & (col(\"cumulative_salary\") <= budget))\n",
    "senior_hires.show()\n",
    "\n",
    "remaining_budget = budget - senior_hires.agg(sum(\"salary\")).collect()[0][0]\n",
    "print(remaining_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0606ab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----------------+\n",
      "|emp_id|experience|salary|cumulative_salary|\n",
      "+------+----------+------+-----------------+\n",
      "|     1|    Junior| 10000|            10000|\n",
      "|     2|    Junior| 15000|            25000|\n",
      "|     4|    Senior| 16000|            41000|\n",
      "|     5|    Senior| 20000|            61000|\n",
      "+------+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 📌 Then hire Junior withing remaining budget.\n",
    "\n",
    "junior_hires = df.filter((col(\"experience\") == \"Junior\") & (col(\"cumulative_salary\") <= remaining_budget))\n",
    "\n",
    "final_hires = senior_hires.union(junior_hires).orderBy(\"experience\", \"cumulative_salary\")\n",
    "final_hires.show()\n",
    "# Show the final list of hired candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4cf17a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-01\n",
      "+---------+----------+---------+\n",
      "|StudentID| ClassDate|IsPresent|\n",
      "+---------+----------+---------+\n",
      "|        1|2023-10-01|        1|\n",
      "|        2|2023-10-01|        1|\n",
      "|        3|2023-10-01|        0|\n",
      "|        1|2023-10-02|        1|\n",
      "|        2|2023-10-02|        0|\n",
      "|        3|2023-10-02|        0|\n",
      "|        1|2023-10-03|        1|\n",
      "|        2|2023-10-03|        0|\n",
      "|        3|2023-10-03|        0|\n",
      "|        1|2023-10-04|        1|\n",
      "|        2|2023-10-04|        0|\n",
      "|        3|2023-10-04|        1|\n",
      "|        1|2023-10-05|        1|\n",
      "|        2|2023-10-05|        1|\n",
      "|        3|2023-10-05|        1|\n",
      "|        1|2023-10-06|        1|\n",
      "|        2|2023-10-06|        0|\n",
      "|        3|2023-10-06|        1|\n",
      "|        1|2023-10-07|        1|\n",
      "|        2|2023-10-07|        0|\n",
      "+---------+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7122001177667395585/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122001177667395585%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "# https://github.com/imadhaka/PyLeetProblems/blob/master/spark/StudentAttendance.py     \n",
    "\n",
    "# 📊 Question: You are given a table named \"Attendance\" with columns:\n",
    "# StudentID, ClassDate, IsPresent (a boolean where 1 indicates presence and 0 indicates absence).\n",
    "\n",
    "# Write a SQL query to identify students who have missed at least 3 consecutive classes (DO NOT use LAG / LEAD).\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "data = [(1, date(2023, 10, 1), 1),(2, date(2023, 10, 1), 1),\n",
    "        (3, date(2023, 10, 1), 0),(1, date(2023, 10, 2), 1),\n",
    "        (2, date(2023, 10, 2), 0),(3, date(2023, 10, 2), 0),\n",
    "        (1, date(2023, 10, 3), 1),(2, date(2023, 10, 3), 0),\n",
    "        (3, date(2023, 10, 3), 0),(1, date(2023, 10, 4), 1),\n",
    "        (2, date(2023, 10, 4), 0),(3, date(2023, 10, 4), 1),\n",
    "        (1, date(2023, 10, 5), 1),(2, date(2023, 10, 5), 1),\n",
    "        (3, date(2023, 10, 5), 1),(1, date(2023, 10, 6), 1),\n",
    "        (2, date(2023, 10, 6), 0),(3, date(2023, 10, 6), 1),\n",
    "        (1, date(2023, 10, 7), 1),(2, date(2023, 10, 7), 0),\n",
    "        (3, date(2023, 10, 7), 1),(1, date(2023, 10, 8), 1),\n",
    "        (2, date(2023, 10, 8), 0),(3, date(2023, 10, 8), 1),\n",
    "        (1, date(2023, 10, 9), 1),(2, date(2023, 10, 9), 0),\n",
    "        (3, date(2023, 10, 9), 1)]\n",
    "    \n",
    "columns = ['StudentID', 'ClassDate', 'IsPresent']\n",
    "studentDf = spark.createDataFrame(data, columns)\n",
    "studentDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eec278ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+---+-----+\n",
      "|StudentID| ClassDate|IsPresent|rnk|rnkId|\n",
      "+---------+----------+---------+---+-----+\n",
      "|        1|2023-10-01|        1|  1|    1|\n",
      "|        1|2023-10-02|        1|  2|    2|\n",
      "|        1|2023-10-03|        1|  3|    3|\n",
      "|        1|2023-10-04|        1|  4|    4|\n",
      "|        1|2023-10-05|        1|  5|    5|\n",
      "|        1|2023-10-06|        1|  6|    6|\n",
      "|        1|2023-10-07|        1|  7|    7|\n",
      "|        1|2023-10-08|        1|  8|    8|\n",
      "|        1|2023-10-09|        1|  9|    9|\n",
      "|        2|2023-10-02|        0|  2|    1|\n",
      "|        2|2023-10-03|        0|  3|    2|\n",
      "|        2|2023-10-04|        0|  4|    3|\n",
      "|        2|2023-10-06|        0|  6|    4|\n",
      "|        2|2023-10-07|        0|  7|    5|\n",
      "|        2|2023-10-08|        0|  8|    6|\n",
      "|        2|2023-10-09|        0|  9|    7|\n",
      "|        2|2023-10-01|        1|  1|    1|\n",
      "|        2|2023-10-05|        1|  5|    2|\n",
      "|        3|2023-10-01|        0|  1|    1|\n",
      "|        3|2023-10-02|        0|  2|    2|\n",
      "+---------+----------+---------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedDf = studentDf.withColumn('rnk', row_number().over(Window.partitionBy(col('StudentID')).orderBy(col('ClassDate'))))\\\n",
    "                            .withColumn('rnkId', row_number().over(Window.partitionBy(col('StudentID'),col('IsPresent')).orderBy(col('ClassDate'))))\n",
    "\n",
    "groupedDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2cddf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-------+\n",
      "|StudentID| ClassDate|IsPresent|numDays|\n",
      "+---------+----------+---------+-------+\n",
      "|        1|2023-10-01|        1|      0|\n",
      "|        1|2023-10-02|        1|      0|\n",
      "|        1|2023-10-03|        1|      0|\n",
      "|        1|2023-10-04|        1|      0|\n",
      "|        1|2023-10-05|        1|      0|\n",
      "|        1|2023-10-06|        1|      0|\n",
      "|        1|2023-10-07|        1|      0|\n",
      "|        1|2023-10-08|        1|      0|\n",
      "|        1|2023-10-09|        1|      0|\n",
      "|        2|2023-10-02|        0|      1|\n",
      "|        2|2023-10-03|        0|      1|\n",
      "|        2|2023-10-04|        0|      1|\n",
      "|        2|2023-10-06|        0|      2|\n",
      "|        2|2023-10-07|        0|      2|\n",
      "|        2|2023-10-08|        0|      2|\n",
      "|        2|2023-10-09|        0|      2|\n",
      "|        2|2023-10-01|        1|      0|\n",
      "|        2|2023-10-05|        1|      3|\n",
      "|        3|2023-10-01|        0|      0|\n",
      "|        3|2023-10-02|        0|      0|\n",
      "+---------+----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+-----------+------------------+\n",
      "|StudentID|MissingFrom|NumberOfMissedDays|\n",
      "+---------+-----------+------------------+\n",
      "|        2| 2023-10-02|                 3|\n",
      "|        2| 2023-10-06|                 4|\n",
      "|        3| 2023-10-01|                 3|\n",
      "+---------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedDf = groupedDf.withColumn('numDays', col('rnk') - col('rnkId'))\\\n",
    "                            .select(col('StudentID'),\n",
    "                                    col('ClassDate'),\n",
    "                                    col('IsPresent'),\n",
    "                                    col('numDays'))\n",
    "groupedDf.show()\n",
    "\n",
    "resultDf = groupedDf.filter(col('IsPresent').__eq__('0'))\\\n",
    "                            .groupBy(col('StudentID'), col('numDays'))\\\n",
    "                            .agg(min(col('ClassDate')).alias('MissingFrom'),\n",
    "                                 count(col('numDays')).alias('NumberOfMissedDays'))\\\n",
    "                            .drop(col('numDays'))\n",
    "\n",
    "resultDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ SQL Approach--\n",
    "\n",
    "# SELECT\n",
    "# A.StudentID,\n",
    "# A.ClassDate,\n",
    "# A.IsPresent,\n",
    "# (row_number() over(PARTITION BY A.StudentID ORDER BY A.ClassDate) - row_number() \n",
    "#         over(PARTITION BY A.StudentID, A.IsPresent ORDER BY A.ClassDate)) as numDays\n",
    "# FROM Attendance A\n",
    "\n",
    "# PS: This is one of the approach for this problem (specific to Spark SQL).\n",
    "    \n",
    "    \n",
    "# query = \"with cte as ( \" \\\n",
    "#                 \"SELECT \" \\\n",
    "#                 \"A.StudentID, \" \\\n",
    "#                 \"A.ClassDate, \" \\\n",
    "#                 \"A.IsPresent, \" \\\n",
    "#                 \"(row_number() over(PARTITION BY A.StudentID ORDER BY A.ClassDate) - row_number() \\\n",
    "#                         over(PARTITION BY A.StudentID, A.IsPresent ORDER BY A.ClassDate)) as numDays \" \\\n",
    "#                 \"FROM Attendance A \" \\\n",
    "#                 \") \" \\\n",
    "#                 \"select \" \\\n",
    "#                 \"M1.StudentID, \" \\\n",
    "#                 \"min(M1.ClassDate) as MissingFrom, \" \\\n",
    "#                 \"count(M1.numDays) as NumberOfMissedDays \" \\\n",
    "#                 \"from cte M1 \" \\\n",
    "#                 \"where M1.IsPresent = 0 \" \\\n",
    "#                 \"group by M1.StudentID, numDays\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4a5384",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7122827934645563393/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122827934645563393%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "Hive is a query engine which works on the processing engine MR or Spark or tez. Hive uses data from hdfs or stores also in hdfs. Hive maintains metadata in rdbms also known as remote metastore. (such as oracle, DB2) and Bydefault metadata will store in derby also known as embedded metastore.\n",
    "AWS-Instead of using hdfs we can use S3.\n",
    "```\n",
    "📈drop vs truncate vs purge in HIVE\n",
    "🖋-drop\n",
    "Managed table -Both data and metadata will be deleted.\n",
    "External table-Only metadata is deleted. Data is untouched.\n",
    "\n",
    "🖋-truncate\n",
    "All the data is deleted. Metadata is untouched.\n",
    "\n",
    "🖋-purge\n",
    "If purge command- set to true\n",
    "If we delete data, data is permanently gone\n",
    "If purge command- set to false\n",
    "If we delete data, data can be recovered (with the help of Hadoop admins)\n",
    "\n",
    "🖋tblproperties(\"auto.purge\"=\"true\")\n",
    "    by default its false\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38867462",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7122634055266803712/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122634055266803712%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29 \n",
    "\n",
    "## DATA Engineer interview experience.\n",
    "#### Round-1\n",
    "```\n",
    "Q1) How do you handle nulls in data?\n",
    "Ans : (df.na.drop() or df.dropna() or use when and otherwise to assign any value:df=df.withColumn(\"a\", when(col(\"b\").isnull(),\"M\")))\n",
    "\n",
    "Q2) How to read files recursively in a folder in pyspark?\n",
    "Ans : (use recursivelookup option or use *.file_type at folder level)\n",
    "\n",
    "Q3) When to use partitioning and when to use bucketing?\n",
    "Ans : (when partitioning of data creates small file problem then go for bucketing.eg: partition data on age(1-100) will generate 100 small partition files , instead do bucketing of data in 3 buckets(1-33, 34-67,68-100)).\n",
    "\n",
    "Q4) Query to generate the 2nd highest salary and to remove dups.\n",
    "        select * from employee where salary=(select Max(salary) from employee); or\n",
    "        select *from employee group by salary order by salary desc limit 1,1;\n",
    "\n",
    "Q5) delta table and the transaction log and its time travel properties were asked.\n",
    "    (restore delta table, print older version of delta table etc.)\n",
    "\n",
    "Q6) optimisation techniques used in project\n",
    "```\n",
    "#### Round-2\n",
    "```\n",
    "Q1) Asked about the basic working of various window functions like lead, lag, dense_rank() etc.\n",
    "\n",
    "Q2) Generate cumulative sum over salary column of a table per department.\n",
    "            win_spec = Window.partitionBy(\"account_no\").orderBy(\"transaction_date\")\n",
    "            df2 = df1.withColumn(\"cum_sum\", sum(\"transaction_amount\").over(win_spec))\n",
    "\n",
    "Q3) Questions on Dimension tables vs fact tables and about star schema.\n",
    "\n",
    "Q4) General idea of Implementation of scd-2.\n",
    "(sql code must have 3 columns like the flag, start_date and end_date to denote each record whether active or not and from when till when as per scd-2)\n",
    "\n",
    "Q5) Count number of occurrences of 'p' in column Fruits as given below.\n",
    "\n",
    "Example:\n",
    "Fruit\n",
    "----\n",
    "apple-------------> 2\n",
    "pineapple -------> 3\n",
    "\n",
    "(with cte as(select len(fruit) as l1, len(replace('fruit','p','')) as l2 from t)\n",
    "select (l1-l2) as p_count from cte)\n",
    "\n",
    "Q6) 0,1,1,2,3,5,8 \n",
    "    write function to return fibbonacci series.(used recursion like : return fibb(n-1)+fibb(n-2))\n",
    "    \n",
    "    # Python program to display the Fibonacci sequence\n",
    "\n",
    "    def recur_fibo(n):\n",
    "       if n <= 1:\n",
    "           return n\n",
    "       else:\n",
    "           return(recur_fibo(n-1) + recur_fibo(n-2))\n",
    "\n",
    "\n",
    "Q7) Find the customer who missed atleast two due dates using pyspark.\n",
    "        +-----------+----------+-----------+\n",
    "        |customer_id|   Duedate|PaymentDate|\n",
    "        +-----------+----------+-----------+\n",
    "        |         C1|2019-01-01| 2018-12-30|\n",
    "        |         C1|2019-01-02| 2019-01-25|\n",
    "        |         C1|2019-01-02| 2019-01-24|\n",
    "        |         C2|2019-05-01| 2019-06-01|\n",
    "        |         C2|2019-05-02| 2019-02-02|\n",
    "        |         C2|2019-05-03| 2019-07-03|\n",
    "        +-----------+----------+-----------+\n",
    "\n",
    "```\n",
    "\n",
    "### Round-3\n",
    "```\n",
    "Q1) SQL problem, DDL statements \n",
    "Q2) situation based question to equally distribute water between 3 people with some conditions.\n",
    "Q3) reading a csv file without headers and writing it into parquet?\n",
    "Q4) avro vs parquet vs orc\n",
    "Q5) Driver vs worker nodes how will they behave when increasing memory/cores aggressively.\n",
    "Q6) questions on Compression algo working internally for parquet and internal working and serilization and       de-serialization of data.\n",
    "Q7) How did u handles data skewness?\n",
    "    1.calculate total raw data size:%sh du -sch /dbfs:\"source path\"\n",
    "    2.calculate total partitions needed by dividing data size by 128 mb(140 * 1024/128)\n",
    "    3.to get total existing partitions in source:df.rdd.getnumpartitions()\n",
    "    4.to check skewness:df.withcolumn(\"c\",spark_partition_id()).groupby(\"c\").count()\n",
    "    compare step 2 and 3,if total partitions in step-2>step-3 use repartition(as we need to increase the number of existing partitions)else use coalesce()).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e967cec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|    fruit|\n",
      "+---------+\n",
      "|    apple|\n",
      "|pineapple|\n",
      "+---------+\n",
      "\n",
      "+---------+-----+\n",
      "|    fruit|count|\n",
      "+---------+-----+\n",
      "|    apple|    2|\n",
      "|pineapple|    3|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q)Count number of occurrences of 'p' in column Fruits as given below.\n",
    "\n",
    "# Fruit\n",
    "# -------\n",
    "# apple -----------> 2\n",
    "# pineapple -------> 3\n",
    "\n",
    "# (with cte as(select len(fruit) as l1, len(replace('fruit','p','')) as l2 from t)\n",
    "# select (l1-l2) as p_count from cte)\n",
    "\n",
    "df = spark.createDataFrame([(\"apple\",), (\"pineapple\",)], [\"fruit\"])\n",
    "df.show()\n",
    "# df.printSchema()\n",
    "\n",
    "df.withColumn(\"count\", expr(\"len(fruit) - len(replace(fruit,'p',''))\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "93f22f05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+\n",
      "|customer_id|   Duedate|PaymentDate|\n",
      "+-----------+----------+-----------+\n",
      "|         C1|2019-01-01| 2018-12-30|\n",
      "|         C1|2019-01-02| 2019-01-25|\n",
      "|         C1|2019-01-02| 2019-01-24|\n",
      "|         C2|2019-05-01| 2019-06-01|\n",
      "|         C2|2019-05-02| 2019-02-02|\n",
      "|         C2|2019-05-03| 2019-07-03|\n",
      "+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q)Find the customer who missed atleast two due dates using pyspark.\n",
    "\n",
    "data = [\n",
    "   (\"C1\", \"01/01/2019\", \"12/30/2018\"),\n",
    "   (\"C1\", \"01/02/2019\", \"01/25/2019\"),\n",
    "   (\"C1\", \"01/02/2019\", \"01/24/2019\"),\n",
    "   (\"C2\", \"05/01/2019\", \"06/01/2019\"),\n",
    "   (\"C2\", \"05/02/2019\", \"02/02/2019\"),\n",
    "   (\"C2\", \"05/03/2019\", \"07/03/2019\")]\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "        \n",
    "data1 = []\n",
    "for i in data:\n",
    "    str1 = i[1].split(\"/\")\n",
    "    date1 = date(int(str1[2]), int(str1[0]), int(str1[1]))\n",
    "    \n",
    "    str2 = i[2].split(\"/\")\n",
    "    date2 = date(int(str2[2]), int(str2[0]), int(str2[1]))\n",
    "    \n",
    "    data1.append((i[0], date1, date2))\n",
    "    \n",
    "\n",
    "columns = [\"customer_id\", \"Duedate\", \"PaymentDate\"]\n",
    "df = spark.createDataFrame(data1, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "07c1ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+-------------+\n",
      "|customer_id|   Duedate|PaymentDate|Missedduedate|\n",
      "+-----------+----------+-----------+-------------+\n",
      "|         C1|2019-01-01| 2018-12-30|            0|\n",
      "|         C1|2019-01-02| 2019-01-25|            1|\n",
      "|         C1|2019-01-02| 2019-01-24|            1|\n",
      "|         C2|2019-05-01| 2019-06-01|            1|\n",
      "|         C2|2019-05-02| 2019-02-02|            0|\n",
      "|         C2|2019-05-03| 2019-07-03|            1|\n",
      "+-----------+----------+-----------+-------------+\n",
      "\n",
      "+-----------+-------------+\n",
      "|customer_id|Missedduedate|\n",
      "+-----------+-------------+\n",
      "|         C1|            2|\n",
      "|         C2|            2|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Missedduedate\", when(col(\"paymentDate\") > col(\"Duedate\"), 1).otherwise(0))\n",
    "df.show()\n",
    "\n",
    "result = df.groupby(\"customer_id\").agg(sum(\"Missedduedate\").alias(\"Missedduedate\"))\n",
    "result = result.filter(col(\"Missedduedate\") >= 2)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "222d75c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+----------+\n",
      "|employee_id|employee_name|salary|manager_id|\n",
      "+-----------+-------------+------+----------+\n",
      "|          1|         John|160000|         3|\n",
      "|          2|        Susan|155000|         3|\n",
      "|          3|      Michael| 80000|         5|\n",
      "|          4|        Linda| 72000|         5|\n",
      "|          5|        David| 55000|         6|\n",
      "|          6|        Sarah|110008|      null|\n",
      "+-----------+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7122938682243772416/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122938682243772416%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 𝐇𝐨𝐰 𝐜𝐚𝐧 𝐈 𝐢𝐝𝐞𝐧𝐭𝐢𝐟𝐲 𝐞𝐦𝐩𝐥𝐨𝐲𝐞𝐞𝐬 𝐢𝐧 𝐚 𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐃𝐚𝐭𝐚𝐅𝐫𝐚𝐦𝐞 𝐰𝐡𝐨𝐬𝐞 𝐬𝐚𝐥𝐚𝐫𝐲 𝐞𝐱𝐜𝐞𝐞𝐝𝐬 𝐭𝐡𝐚𝐭 𝐨𝐟 𝐭𝐡𝐞𝐢𝐫 𝐫𝐞𝐬𝐩𝐞𝐜𝐭𝐢𝐯𝐞 𝐦𝐚𝐧𝐚𝐠𝐞𝐫𝐬, \n",
    "# 𝐰𝐡𝐢𝐥𝐞 𝐚𝐥𝐬𝐨 𝐥𝐨𝐚𝐝𝐢𝐧𝐠 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐢𝐧𝐭𝐨 𝐭𝐡𝐞 𝐃𝐚𝐭𝐚𝐅𝐫𝐚𝐦𝐞?\n",
    "# important self join thing\n",
    "\n",
    "data =  [Row(employee_id=1, employee_name=\"John\", salary=160000, manager_id=3),\n",
    "         Row(employee_id=2, employee_name=\"Susan\", salary=155000, manager_id=3), \n",
    "         Row(employee_id=3, employee_name=\"Michael\", salary=80000, manager_id=5),\n",
    "         Row(employee_id=4, employee_name=\"Linda\", salary=72000, manager_id=5), \n",
    "         Row(employee_id=5, employee_name=\"David\", salary=55000, manager_id=6), \n",
    "         Row(employee_id=6, employee_name=\"Sarah\", salary=110008, manager_id = None)]\n",
    "\n",
    "employee_df = spark.createDataFrame(data)\n",
    "employee_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a3bca175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+----------+-----------+-------------+------+----------+\n",
      "|employee_id|employee_name|salary|manager_id|employee_id|employee_name|salary|manager_id|\n",
      "+-----------+-------------+------+----------+-----------+-------------+------+----------+\n",
      "|          1|         John|160000|         3|          3|      Michael| 80000|         5|\n",
      "|          2|        Susan|155000|         3|          3|      Michael| 80000|         5|\n",
      "|          3|      Michael| 80000|         5|          5|        David| 55000|         6|\n",
      "|          4|        Linda| 72000|         5|          5|        David| 55000|         6|\n",
      "|          5|        David| 55000|         6|          6|        Sarah|110008|      null|\n",
      "+-----------+-------------+------+----------+-----------+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = employee_df.alias(\"e1\").join(employee_df.alias(\"e2\"), \n",
    "                                         col(\"e1.manager_id\") == col(\"e2.employee_id\"),\n",
    "                                         \"inner\")\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "debc8a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+----------+-----------+-------------+------+----------+\n",
      "|employee_id|employee_name|salary|manager_id|employee_id|employee_name|salary|manager_id|\n",
      "+-----------+-------------+------+----------+-----------+-------------+------+----------+\n",
      "|          1|         John|160000|         3|          3|      Michael| 80000|         5|\n",
      "|          2|        Susan|155000|         3|          3|      Michael| 80000|         5|\n",
      "|          3|      Michael| 80000|         5|          5|        David| 55000|         6|\n",
      "|          4|        Linda| 72000|         5|          5|        David| 55000|         6|\n",
      "+-----------+-------------+------+----------+-----------+-------------+------+----------+\n",
      "\n",
      "+-----------+-------------+------+-------------+------+\n",
      "|employee_id|employee_name|salary|employee_name|salary|\n",
      "+-----------+-------------+------+-------------+------+\n",
      "|          1|         John|160000|         John|160000|\n",
      "|          2|        Susan|155000|        Susan|155000|\n",
      "|          3|      Michael| 80000|      Michael| 80000|\n",
      "|          4|        Linda| 72000|        Linda| 72000|\n",
      "+-----------+-------------+------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter employees with higher salaries than their managers \n",
    "result_df = joined_df.filter(col(\"e1.salary\") > col(\"e2.salary\"))\n",
    "result_df.show()\n",
    "result_df.select(\"e1.employee_id\", \"e1.employee_name\", \"e1.salary\", \"e1.employee_name\", \"e1.salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a763edd0",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7121361196137484288/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121361196137484288%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "SQOOP\n",
    "------------------\n",
    "## 1. 📈Sqoop Import\n",
    "```\n",
    "Sqoop Import is used to transfer data from relational databases to HDFS.\n",
    "\n",
    "        Sqoop import \\\n",
    "        --connect \"jdbc://(IpAddress):(Port number(eg 3306)) /(databaseName) \\\n",
    "        --username user \\\n",
    "        --password ****** \\\n",
    "        --table (eg - students_MarkList) \\\n",
    "        --target-dir /students_MarkList_results\n",
    "    \n",
    "                or\n",
    "        \n",
    "(Sqoop commands without using \"\\\", in a single line)\n",
    "        Sqoop import --connect \"jdbc://(IpAddress):(Port number(eg 3306)) /(databaseName) --username user --password ****** --table (eg - students_MarkList) --target-dir /students_MarkList_results\n",
    "\n",
    "\n",
    "--target dir : This is the final path(location) where data get stored. This need to be defined by the user.\n",
    "\n",
    "        Sqoop import \\\n",
    "        --connect \"jdbc://(IpAddress):(Port number(eg 3306)) /(databaseName) \\\n",
    "        --username user \\\n",
    "        --password ****** \\\n",
    "        --table (eg - students_MarkList) \\\n",
    "        --target-dir /user/results/students_MarkList_results\n",
    "\n",
    "--warehouse dir : This is also the path(location) where data get stored. But in this path Sqoop will create a subdirectory along with the user-defined path when the user fails to mention the final destination where data needs to be stored.\n",
    "(when multiple tables are imported at the same time then \"warehouse dir\" is preferable.\n",
    "\n",
    "        Sqoop import \\\n",
    "        --connect \"jdbc://(IpAddress):(Port number(eg 3306)) /(databaseName) \\\n",
    "        --username user \\\n",
    "        --password ****** \\\n",
    "        --table (eg - students_MarkList) \\\n",
    "        --warehouse-dir /user/results\n",
    "\n",
    " \n",
    "📈How Sqoop Import Works -\n",
    "        🖋It is MapReduce Job.\n",
    "        🖋But it's a special MapReduce Job. Only mappers work but no reducer\n",
    "        works.\n",
    "        🖋In default there are 4 mappers get employed in the Sqoop import job.\n",
    "        🖋The Number of mappers can be modified.\n",
    "        🖋Mappers divides the job based on the Primary Key present in the table.\n",
    "        🖋If there is no primary key then change Number of Mappers to 1 (Though\n",
    "        it works but there won't be any parallelism in the job).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bcbfdc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+-------+-------+\n",
      "|year|Wimbledon|Fr_Open|US_Open|Au_Open|\n",
      "+----+---------+-------+-------+-------+\n",
      "|2017|        2|      1|      1|      2|\n",
      "|2018|        3|      1|      3|      2|\n",
      "|2019|        3|      1|      1|      3|\n",
      "+----+---------+-------+-------+-------+\n",
      "\n",
      "+---------+-----------+\n",
      "|Player_ID|player_Name|\n",
      "+---------+-----------+\n",
      "|        1|      Nadal|\n",
      "|        2|    Federer|\n",
      "|        3|      Novak|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7123234638797373440/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123234638797373440%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "matches_list = [(2017, 2, 1, 1, 2),(2018, 3, 1, 3, 2),(2019, 3, 1, 1, 3)]\n",
    "matches_header = [\"year\", \"Wimbledon\", \"Fr_Open\", \"US_Open\", \"Au_Open\"]\n",
    "\n",
    "players_list = [(1, \"Nadal\"),(2, \"Federer\"),(3, \"Novak\")]\n",
    "players_header = [\"Player_ID\", \"player_Name\"]\n",
    "\n",
    "matches = spark.createDataFrame(matches_list, matches_header)\n",
    "matches.show()\n",
    "players = spark.createDataFrame(players_list, players_header)\n",
    "players.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fdc0b4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|PlayerID|count|\n",
      "+--------+-----+\n",
      "|       2|    1|\n",
      "|       3|    2|\n",
      "|       1|    3|\n",
      "|       1|    2|\n",
      "|       3|    1|\n",
      "|       2|    2|\n",
      "|       3|    1|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+----------------+\n",
      "|PlayerID|No_of_GrandSlams|\n",
      "+--------+----------------+\n",
      "|       3|               4|\n",
      "|       2|               3|\n",
      "|       1|               5|\n",
      "+--------+----------------+\n",
      "\n",
      "+---------+-----------+----------------+\n",
      "|Player_ID|player_Name|No_of_GrandSlams|\n",
      "+---------+-----------+----------------+\n",
      "|        1|      Nadal|               5|\n",
      "|        2|    Federer|               3|\n",
      "|        3|      Novak|               4|\n",
      "+---------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Used GroupedCount\n",
    "inter_res= matches.groupBy(col(\"Wimbledon\").alias(\"PlayerID\")).count().union(\n",
    "matches.groupBy(col(\"Fr_Open\").alias(\"PlayerID\")).count()).union(\n",
    "matches.groupBy(col(\"US_Open\").alias(\"PlayerID\")).count()).union(\n",
    "matches.groupBy(col(\"Au_Open\").alias(\"PlayerID\")).count())\n",
    "inter_res.show()\n",
    "\n",
    "#Getting the No_of_GrandSlams per player\n",
    "res = inter_res.groupBy(\"PlayerID\").agg(sum(\"count\").alias(\"No_of_GrandSlams\"))\n",
    "res.show()\n",
    "\n",
    "#Player name with player ID\n",
    "res.join(players, res.PlayerID==players.Player_ID, \"inner\")\\\n",
    ".select(col(\"Player_ID\"), col(\"player_Name\"), col(\"No_of_GrandSlams\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44e32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "632d6f3c",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7123345666528935936/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123345666528935936%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "### COALESCE function:\n",
    "\n",
    "COALESCE is a SQL function which returns the first non-NULL value in a list of expressions.\n",
    "It helps in situations where you need to work with the data manipulation to obtain the not null values and get the default in case of nulls.\n",
    "\n",
    "COALESCE function takes two or more arguments and returns the first non-NULL value from a list of values If all the arguments are NULL, then the COALESCE function returns NULL.\n",
    "\n",
    "Syntax: NVL takes two arguments, while COALESCE takes two or more arguments.\n",
    "\n",
    "COALESCE (expression1, expression2, expression3, ...)\n",
    "\n",
    "### NVL function:\n",
    "\n",
    "The NVL function is used to replace a NULL value with a specified default value. NVL is a function which is specific to Oracle database and provides the data consistency to avoid the undefined data while dealing with nulls.\n",
    "\n",
    "The NVL function takes two arguments. The first argument is the expression to be checked for NULL values, and the second argument is the value to return if the first argument is NULL. If the first argument is not NULL, then the NVL function returns the value of the first argument.\n",
    "\n",
    "Syntax: NVL(expression1, expression2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f0457b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+----------+\n",
      "|product_id|product_name|quantity_sold|order_date|\n",
      "+----------+------------+-------------+----------+\n",
      "|         1|   Product A|           50|2023-07-01|\n",
      "|         2|   Product B|           75|2023-07-05|\n",
      "|         3|   Product C|           30|2023-07-12|\n",
      "|         4|   Product A|           45|2023-07-15|\n",
      "|         5|   Product D|           60|2023-07-20|\n",
      "|         6|   Product B|           70|2023-07-25|\n",
      "|         7|   Product C|           35|2023-07-29|\n",
      "|         8|   Product D|           55|2023-07-30|\n",
      "|         9|   Product E|           40|2023-07-01|\n",
      "|        10|   Product F|           65|2023-07-10|\n",
      "+----------+------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7123353474204852224/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123353474204852224%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# The dataset, named \"sales_data,\" contains the following columns:\n",
    "# product_id:    The unique identifier for each product.\n",
    "# product_name:  The name of the product.\n",
    "# quantity_sold: The quantity of each product sold.\n",
    "# order_date:    The date when the product was sold.\n",
    "    \n",
    "# Write a PySpark code to find the top-selling products for a given month. \n",
    "# Your code should rank the products based on the total quantity sold and return the top N products.\n",
    "\n",
    "schema_ = [\"product_id\",\"product_name\",\"quantity_sold\",\"order_date\"]\n",
    "\n",
    "data = [(1,\"Product A\",50,\"2023-07-01\"),\n",
    "(2,\"Product B\",75,\"2023-07-05\"),\n",
    "(3,\"Product C\",30,\"2023-07-12\"),\n",
    "(4,\"Product A\",45,\"2023-07-15\"),\n",
    "(5,\"Product D\",60,\"2023-07-20\"),\n",
    "(6,\"Product B\",70,\"2023-07-25\"),\n",
    "(7,\"Product C\",35,\"2023-07-29\"),\n",
    "(8,\"Product D\",55,\"2023-07-30\"),\n",
    "(9,\"Product E\",40,\"2023-07-01\"),\n",
    "(10,\"Product F\",65,\"2023-07-10\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema_)\n",
    "df = df.withColumn(\"order_date\", to_date((col(\"order_date\"))))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "89bc0b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|product_id|product_name|total|\n",
      "+----------+------------+-----+\n",
      "|         2|   Product B|   75|\n",
      "|         6|   Product B|   70|\n",
      "|        10|   Product F|   65|\n",
      "|         5|   Product D|   60|\n",
      "|         8|   Product D|   55|\n",
      "|         1|   Product A|   50|\n",
      "|         4|   Product A|   45|\n",
      "|         9|   Product E|   40|\n",
      "|         7|   Product C|   35|\n",
      "|         3|   Product C|   30|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(\"product_id\", \"product_name\").agg(sum(col(\"quantity_sold\")).alias(\"total\")).orderBy(desc(\"total\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "d175bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|avg_sales|\n",
      "+----------+---------+\n",
      "|2023-10-24|    150.0|\n",
      "|2023-10-25|    200.0|\n",
      "|2023-10-26|    300.0|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7123527963072970752/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123527963072970752%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# \"𝐇𝐨𝐰 𝐜𝐚𝐧 𝐈 𝐜𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐭𝐡𝐞 𝐝𝐚𝐢𝐥𝐲 𝐚𝐯𝐞𝐫𝐚𝐠𝐞 𝐬𝐚𝐥𝐞𝐬 𝐟𝐫𝐨𝐦 𝐚 𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐝𝐚𝐭𝐚𝐬𝐞𝐭 𝐜𝐨𝐧𝐭𝐚𝐢𝐧𝐢𝐧𝐠 𝐬𝐚𝐥𝐞𝐬 𝐝𝐚𝐭𝐚 𝐰𝐢𝐭𝐡 𝐯𝐚𝐫𝐢𝐨𝐮𝐬 𝐝𝐚𝐭𝐞𝐬?\"\n",
    "\n",
    "data = [(\"2023-10-24\", 100.0),\n",
    "        (\"2023-10-24\", 200.0),\n",
    "        (\"2023-10-25\", 150.0),\n",
    "        (\"2023-10-25\", 250.0),\n",
    "        (\"2023-10-26\", 300.0),]\n",
    "\n",
    "columns = [\"date\", \"sales\"]\n",
    "df =spark.createDataFrame(data,columns)\n",
    "\n",
    "df1=df.withColumn('date',to_date('date'))\n",
    "df1.groupBy('date').agg(avg('sales').alias('avg_sales')).sort('date').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "343e5f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n",
      "|order_id|customer_id|order_date|order_amount|\n",
      "+--------+-----------+----------+------------+\n",
      "|       1|        100|2022-01-01|        2000|\n",
      "|       2|        200|2022-01-01|        2500|\n",
      "|       3|        300|2022-01-01|        2100|\n",
      "|       4|        100|2022-01-02|        2000|\n",
      "|       5|        400|2022-01-02|        2200|\n",
      "|       6|        500|2022-01-02|        2700|\n",
      "|       7|        100|2022-01-03|        3000|\n",
      "|       8|        400|2022-01-03|        1000|\n",
      "|       9|        600|2022-01-03|        3000|\n",
      "+--------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7123522447164776448/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123522447164776448%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭:\n",
    "# Consider a dataset representing customer orders, as shown in the first table of the image. \n",
    "# 𝐓𝐡𝐞 𝐭𝐚𝐬𝐤 𝐢𝐬 𝐭𝐨 𝐢𝐝𝐞𝐧𝐭𝐢𝐟𝐲 𝐧𝐞𝐰 𝐚𝐧𝐝 𝐫𝐞𝐩𝐞𝐚𝐭 𝐜𝐮𝐬𝐭𝐨𝐦𝐞𝐫𝐬, 𝐩𝐫𝐨𝐯𝐢𝐝𝐢𝐧𝐠 𝐢𝐧𝐬𝐢𝐠𝐡𝐭𝐬 𝐢𝐧𝐭𝐨 𝐭𝐡𝐞𝐢𝐫 𝐨𝐫𝐝𝐞𝐫 𝐚𝐦𝐨𝐮𝐧𝐭𝐬. \n",
    "\n",
    "schema_ = [\"order_id\",\"customer_id\",\"order_date\",\"order_amount\"]\n",
    "\n",
    "data = [(1,100,'2022-01-01',2000),\n",
    "(2,200,'2022-01-01',2500),(3,300,'2022-01-01',2100),\n",
    "(4,100,'2022-01-02',2000),(5,400,'2022-01-02',2200),\n",
    "(6,500,'2022-01-02',2700),(7,100,'2022-01-03',3000),\n",
    "(8,400,'2022-01-03',1000),(9,600,'2022-01-03',3000)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema_)\n",
    "df = df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4675d591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|customer_id|first_order_date|\n",
      "+-----------+----------------+\n",
      "|        100|      2022-01-01|\n",
      "|        200|      2022-01-01|\n",
      "|        300|      2022-01-01|\n",
      "|        400|      2022-01-02|\n",
      "|        500|      2022-01-02|\n",
      "|        600|      2022-01-03|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mingroup1 = df.groupBy(\"customer_id\").agg(min(col(\"order_date\")).alias(\"first_order_date\"))\n",
    "mingroup1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b5014b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+----------------+\n",
      "|customer_id|order_id|order_date|order_amount|first_order_date|\n",
      "+-----------+--------+----------+------------+----------------+\n",
      "|        100|       1|2022-01-01|        2000|      2022-01-01|\n",
      "|        200|       2|2022-01-01|        2500|      2022-01-01|\n",
      "|        300|       3|2022-01-01|        2100|      2022-01-01|\n",
      "|        100|       4|2022-01-02|        2000|      2022-01-01|\n",
      "|        400|       5|2022-01-02|        2200|      2022-01-02|\n",
      "|        500|       6|2022-01-02|        2700|      2022-01-02|\n",
      "|        100|       7|2022-01-03|        3000|      2022-01-01|\n",
      "|        600|       9|2022-01-03|        3000|      2022-01-03|\n",
      "|        400|       8|2022-01-03|        1000|      2022-01-02|\n",
      "+-----------+--------+----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedData = df.join(mingroup1, on=\"customer_id\", how=\"inner\")\n",
    "joinedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2e9ee713",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+------------+----------------+----------------+-------------------+----------------------+-------------------------+\n",
      "|customer_id|order_id|order_date|order_amount|first_order_date|new_customer_cnt|repeat_customer_cnt|new_customer_order_amt|repeat_customer_order_amt|\n",
      "+-----------+--------+----------+------------+----------------+----------------+-------------------+----------------------+-------------------------+\n",
      "|        100|       1|2022-01-01|        2000|      2022-01-01|               1|                  0|                  2000|                        0|\n",
      "|        200|       2|2022-01-01|        2500|      2022-01-01|               1|                  0|                  2500|                        0|\n",
      "|        300|       3|2022-01-01|        2100|      2022-01-01|               1|                  0|                  2100|                        0|\n",
      "|        100|       4|2022-01-02|        2000|      2022-01-01|               0|                  1|                     0|                     2000|\n",
      "|        400|       5|2022-01-02|        2200|      2022-01-02|               1|                  0|                  2200|                        0|\n",
      "|        500|       6|2022-01-02|        2700|      2022-01-02|               1|                  0|                  2700|                        0|\n",
      "|        100|       7|2022-01-03|        3000|      2022-01-01|               0|                  1|                     0|                     3000|\n",
      "|        600|       9|2022-01-03|        3000|      2022-01-03|               1|                  0|                  3000|                        0|\n",
      "|        400|       8|2022-01-03|        1000|      2022-01-02|               0|                  1|                     0|                     1000|\n",
      "+-----------+--------+----------+------------+----------------+----------------+-------------------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "caseData = joinedData\\\n",
    "            .withColumn(\"new_customer_cnt\", expr(\"CASE WHEN order_date = first_order_date THEN 1 ELSE 0 END\"))\\\n",
    "            .withColumn(\"repeat_customer_cnt\", expr(\"CASE WHEN order_date <> first_order_date \\\n",
    "                                                        THEN 1 ELSE 0 END\"))\\\n",
    "            .withColumn(\"new_customer_order_amt\", expr(\"CASE WHEN order_date = first_order_date \\\n",
    "                                                       THEN order_amount ELSE 0 END\"))\\\n",
    "            .withColumn(\"repeat_customer_order_amt\", expr(\"CASE WHEN order_date <> first_order_date \\\n",
    "                                                       THEN order_amount ELSE 0 END\"))\n",
    "\n",
    "caseData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "78487ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------------+----------------------+-------------------------+\n",
      "|order_date|new_customer_cnt|repeat_customer_cnt|new_customer_order_amt|repeat_customer_order_amt|\n",
      "+----------+----------------+-------------------+----------------------+-------------------------+\n",
      "|2022-01-01|               3|                  0|                  6600|                        0|\n",
      "|2022-01-02|               2|                  1|                  4900|                     2000|\n",
      "|2022-01-03|               1|                  2|                  3000|                     4000|\n",
      "+----------+----------------+-------------------+----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df22 = caseData.groupBy(\"order_date\").agg(\n",
    "                sum(\"new_customer_cnt\").alias(\"new_customer_cnt\"),\n",
    "                sum(\"repeat_customer_cnt\").alias(\"repeat_customer_cnt\"),\n",
    "                sum(\"new_customer_order_amt\").alias(\"new_customer_order_amt\"),\n",
    "                sum(\"repeat_customer_order_amt\").alias(\"repeat_customer_order_amt\")\n",
    "                ).orderBy(\"order_date\")\n",
    "df22.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 𝐒𝐨𝐥𝐮𝐭𝐢𝐨𝐧𝟏 𝐮𝐬𝐢𝐧𝐠 𝐂𝐓𝐄 & 𝐂𝐀𝐒𝐄 𝐖𝐇𝐄𝐍 𝐰𝐢𝐭𝐡 𝐒𝐔𝐌:\n",
    "\n",
    "# WITH cte1\n",
    "# AS ( SELECT customer_id, min(order_date) AS first_order_date\n",
    "#     FROM customer_orders GROUP BY customer_id\n",
    "# )\n",
    "# SELECT order_date\n",
    "# ,SUM(CASE WHEN co.order_date = c1.first_order_date\n",
    "#     THEN 1 ELSE 0 END) AS new_customer_cnt\n",
    "# ,SUM(CASE WHEN co.order_date <> c1.first_order_date\n",
    "#     THEN 1 ELSE 0 END) AS repeat_customer_cnt\n",
    "# ,SUM(CASE \n",
    "# WHEN co.order_date = c1.first_order_date THEN co.order_amount\n",
    "#     ELSE 0 END) AS new_customer_order_amt\n",
    "# ,SUM(CASE \n",
    "# WHEN co.order_date <> c1.first_order_date\n",
    "#     THEN co.order_amount ELSE 0 END) AS repeat_customer_order_amt\n",
    "# FROM customer_orders co\n",
    "# JOIN cte1 c1 ON co.customer_id = c1.customer_id\n",
    "# GROUP BY order_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "131fd9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+-------------+----------+\n",
      "|product_id|product_name           |quantity_sold|order_date|\n",
      "+----------+-----------------------+-------------+----------+\n",
      "|1         |Category A - Product 1 |50           |2023-01-01|\n",
      "|3         |Category A - Product 3 |65           |2023-02-02|\n",
      "|6         |Category A - Product 6 |70           |2023-03-10|\n",
      "|8         |Category A - Product 8 |55           |2023-04-15|\n",
      "|2         |Category B - Product 2 |50           |2023-01-15|\n",
      "|5         |Category B - Product 5 |60           |2023-03-05|\n",
      "|9         |Category B - Product 9 |40           |2023-05-01|\n",
      "|10        |Category C - Product 10|65           |2023-05-20|\n",
      "|4         |Category C - Product 4 |45           |2023-02-20|\n",
      "|7         |Category C - Product 7 |80           |2023-04-02|\n",
      "+----------+-----------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7124085577792073728/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124085577792073728%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ =[\"product_id\",\"product_name\",\"quantity_sold\",\"order_date\"]\n",
    "\n",
    "data = [(1,\"Category A - Product 1\",50,\"01-01-2023\"),\n",
    "(3,\"Category A - Product 3\",65,\"02-02-2023\"),\n",
    "(6,\"Category A - Product 6\",70,\"10-03-2023\"),\n",
    "(8,\"Category A - Product 8\",55,\"15-04-2023\"),\n",
    "(2,\"Category B - Product 2\",50,\"15-01-2023\"),\n",
    "(5,\"Category B - Product 5\",60,\"05-03-2023\"),\n",
    "(9,\"Category B - Product 9\",40,\"1-05-2023\"),\n",
    "(10,\"Category C - Product 10\",65,\"20-05-2023\"),\n",
    "(4,\"Category C - Product 4\",45,\"20-02-2023\"),\n",
    "(7,\"Category C - Product 7\",80,\"02-04-2023\")]\n",
    "\n",
    "from datetime import date\n",
    "data1 = []\n",
    "for i in data:\n",
    "    str1 = i[3].split(\"-\")\n",
    "    date1 = date(int(str1[2]), int(str1[1]), int(str1[0]))\n",
    "    \n",
    "    data1.append((i[0],i[1], i[2], date1))\n",
    "\n",
    "df = spark.createDataFrame(data1, schema_)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e164b21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-------------------+\n",
      "|year_month|product_category|       sales_growth|\n",
      "+----------+----------------+-------------------+\n",
      "|   2023-01|      Category A|               null|\n",
      "|   2023-02|      Category A|               30.0|\n",
      "|   2023-03|      Category A| 7.6923076923076925|\n",
      "|   2023-04|      Category A|-21.428571428571427|\n",
      "|   2023-01|      Category B|               null|\n",
      "|   2023-03|      Category B|               20.0|\n",
      "|   2023-05|      Category B| -33.33333333333333|\n",
      "|   2023-02|      Category C|               null|\n",
      "|   2023-04|      Category C|  77.77777777777779|\n",
      "|   2023-05|      Category C|             -18.75|\n",
      "+----------+----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the product category from the product name (the format \"Category - ProductName\")\n",
    "sales_data = df.withColumn(\"product_category\", split(col(\"product_name\"), \" - \")[0])\n",
    "sales_data = sales_data.withColumn(\"year_month\", date_format(\"order_date\", \"yyyy-MM\"))\n",
    "\n",
    "# Calculate the total monthly sales for each product category\n",
    "monthly_sales = sales_data.groupBy(\"year_month\", \"product_category\")\\\n",
    "                                        .agg(sum(\"quantity_sold\").alias(\"total_sales\"))\n",
    "\n",
    "# Create a window specification for calculating the previous month's total sales\n",
    "window_spec = Window.partitionBy(\"product_category\").orderBy(\"year_month\")\n",
    "\n",
    "# Calculate the lagged (previous month) total sales for each product category\n",
    "monthly_sales = monthly_sales.withColumn(\"prev_month_sales\", lag(\"total_sales\", 1).over(window_spec))\n",
    "\n",
    "# Calculate the percentage change in sales compared to the previous month\n",
    "monthly_sales = monthly_sales.withColumn(\"sales_growth\", \n",
    "    (monthly_sales[\"total_sales\"] - monthly_sales[\"prev_month_sales\"]) / monthly_sales[\"prev_month_sales\"] * 100)\n",
    "\n",
    "# Show the market trend analysis results\n",
    "monthly_sales.select(\"year_month\", \"product_category\", \"sales_growth\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be468bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------------+\n",
      "|user_id|date_searched|filter_room_types       |\n",
      "+-------+-------------+------------------------+\n",
      "|1      |2022-01-01   |entire home,private room|\n",
      "|2      |2022-01-02   |entire home,shared room |\n",
      "|3      |2022-01-02   |private room,shared room|\n",
      "|4      |2022-01-03   |private room            |\n",
      "+-------+-------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7123558824749236224/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123558824749236224%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# find the room types that are searched most of the times and output should be the roomtype alongside \n",
    "# the number of search hits for it.\n",
    "\n",
    "schema_ = [\"user_id\", \"date_searched\", \"filter_room_types\"]\n",
    "\n",
    "data = [(1 , \"2022-01-01\" , \"entire home,private room\"),\n",
    "(2 , \"2022-01-02\" , \"entire home,shared room\"),\n",
    "(3 , \"2022-01-02\" , \"private room,shared room\"),\n",
    "(4 , \"2022-01-03\" , \"private room\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dd81a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+\n",
      "|user_id|date_searched|filter_room_types|\n",
      "+-------+-------------+-----------------+\n",
      "|      1|   2022-01-01|      entire home|\n",
      "|      1|   2022-01-01|     private room|\n",
      "|      2|   2022-01-02|      entire home|\n",
      "|      2|   2022-01-02|      shared room|\n",
      "|      3|   2022-01-02|     private room|\n",
      "|      3|   2022-01-02|      shared room|\n",
      "|      4|   2022-01-03|     private room|\n",
      "+-------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_data = df.withColumn(\"filter_room_types\", explode(split(df[\"filter_room_types\"], \",\")))\n",
    "split_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8fcb8734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|filter_room_types|count|\n",
      "+-----------------+-----+\n",
      "|     private room|    3|\n",
      "|      entire home|    2|\n",
      "|      shared room|    2|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = split_data.groupBy(\"filter_room_types\").agg(count(\"filter_room_types\").alias(\"count\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ee9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sql sol\n",
    "# SELECT Value as room_type,COUNT(1) AS No_of_Searches FROM airbnb_searches\n",
    "# cross apply string_split(filter_room_types,',') \n",
    "# GROUP BY Value\n",
    "# ORDER BY No_of_Searches DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9bdfc19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|CategoryID|CategoryName|\n",
      "+----------+------------+\n",
      "|         1| Electronics|\n",
      "|         2| Accessories|\n",
      "|         3| Photography|\n",
      "|         4|  Appliances|\n",
      "+----------+------------+\n",
      "\n",
      "+---------+-----------+----------+------+\n",
      "|ProductID|ProductName|CategoryID| Price|\n",
      "+---------+-----------+----------+------+\n",
      "|        1|     Laptop|         1|899.99|\n",
      "|        2| Smartphone|         1|699.99|\n",
      "|        3| Headphones|         2|149.99|\n",
      "|        4|     Tablet|         1|399.99|\n",
      "|        5|     Camera|         3|499.99|\n",
      "|        6|    Printer|         3|299.99|\n",
      "|        7|      Mouse|         2| 29.99|\n",
      "|        8|   Keyboard|         2| 49.99|\n",
      "|        9|         TV|         4|899.99|\n",
      "|       10|   Soundbar|         4|249.99|\n",
      "+---------+-----------+----------+------+\n",
      "\n",
      "+------+---------+------------+----------+\n",
      "|SaleID|ProductID|QuantitySold|  SaleDate|\n",
      "+------+---------+------------+----------+\n",
      "|     1|        1|         120|2023-09-01|\n",
      "|     2|        2|          90|2023-09-02|\n",
      "|     3|        3|         200|2023-09-03|\n",
      "|     4|        4|          35|2023-09-04|\n",
      "|     5|        5|          75|2023-09-05|\n",
      "|     6|        6|          50|2023-09-06|\n",
      "|     7|        7|         160|2023-09-07|\n",
      "|     8|        8|         120|2023-09-08|\n",
      "|     9|        9|          25|2023-09-09|\n",
      "|    10|       10|          40|2023-09-10|\n",
      "+------+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7124256858676035584/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124256858676035584%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Find the top-selling products in each category based on sales revenue?\n",
    "\n",
    "\n",
    "# Categories\n",
    "\n",
    "schema1 = [\"CategoryID\", \"CategoryName\"]\n",
    "data1 = [(1, 'Electronics'),\n",
    "(2, 'Accessories'),\n",
    "(3, 'Photography'),\n",
    "(4, 'Appliances')]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "\n",
    "# Products \n",
    "\n",
    "schema2 = [\"ProductID\", 'ProductName', \"CategoryID\", \"Price\"]\n",
    "\n",
    "data2 = [(1, 'Laptop', 1, 899.99),\n",
    "(2, 'Smartphone', 1, 699.99),\n",
    "(3, 'Headphones', 2, 149.99),\n",
    "(4, 'Tablet', 1, 399.99),\n",
    "(5, 'Camera', 3, 499.99),\n",
    "(6, 'Printer', 3, 299.99),\n",
    "(7, 'Mouse', 2, 29.99),\n",
    "(8, 'Keyboard', 2, 49.99),\n",
    "(9, 'TV', 4, 899.99),\n",
    "(10, 'Soundbar', 4, 249.99)]\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show()\n",
    "\n",
    "# Sales \n",
    "\n",
    "schema3 = [\"SaleID\", \"ProductID\" ,\"QuantitySold\" ,\"SaleDate\"]\n",
    "data3 = [(1, 1, 120, '2023-09-01'),\n",
    "(2, 2, 90, '2023-09-02'),\n",
    "(3, 3, 200, '2023-09-03'),\n",
    "(4, 4, 35, '2023-09-04'),\n",
    "(5, 5, 75, '2023-09-05'),\n",
    "(6, 6, 50, '2023-09-06'),\n",
    "(7, 7, 160, '2023-09-07'),\n",
    "(8, 8, 120, '2023-09-08'),\n",
    "(9, 9, 25, '2023-09-09'),\n",
    "(10, 10, 40, '2023-09-10')]\n",
    "\n",
    "df3 = spark.createDataFrame(data3, schema3)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b46fe0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+------+------------+------+------------+----------+-----------+\n",
      "|ProductID|CategoryID|ProductName| Price|CategoryName|SaleID|QuantitySold|  SaleDate|salesAmount|\n",
      "+---------+----------+-----------+------+------------+------+------------+----------+-----------+\n",
      "|        1|         1|     Laptop|899.99| Electronics|     1|         120|2023-09-01|   107998.8|\n",
      "|        2|         1| Smartphone|699.99| Electronics|     2|          90|2023-09-02|    62999.1|\n",
      "|        3|         2| Headphones|149.99| Accessories|     3|         200|2023-09-03|    29998.0|\n",
      "|        5|         3|     Camera|499.99| Photography|     5|          75|2023-09-05|   37499.25|\n",
      "|        4|         1|     Tablet|399.99| Electronics|     4|          35|2023-09-04|   13999.65|\n",
      "|        6|         3|    Printer|299.99| Photography|     6|          50|2023-09-06|    14999.5|\n",
      "|        7|         2|      Mouse| 29.99| Accessories|     7|         160|2023-09-07|     4798.4|\n",
      "|        8|         2|   Keyboard| 49.99| Accessories|     8|         120|2023-09-08|     5998.8|\n",
      "|        9|         4|         TV|899.99|  Appliances|     9|          25|2023-09-09|   22499.75|\n",
      "|       10|         4|   Soundbar|249.99|  Appliances|    10|          40|2023-09-10|     9999.6|\n",
      "+---------+----------+-----------+------+------------+------+------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedData1 = df2.join(df1, on=[\"CategoryID\"], how=\"inner\").join(df3, on=[\"ProductID\"], how=\"inner\")\n",
    "salesAmount = joinedData1.withColumn(\"salesAmount\", col(\"Price\") * col(\"QuantitySold\"))\n",
    "salesAmount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d86995af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+------------+------+-----------+\n",
      "|ProductName|CategoryName|QuantitySold| Price|salesAmount|\n",
      "+-----------+------------+------------+------+-----------+\n",
      "|     Laptop| Electronics|         120|899.99|   107998.8|\n",
      "|     Camera| Photography|          75|499.99|   37499.25|\n",
      "| Headphones| Accessories|         200|149.99|    29998.0|\n",
      "|         TV|  Appliances|          25|899.99|   22499.75|\n",
      "+-----------+------------+------------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"CategoryName\").orderBy(desc(\"salesAmount\"))\n",
    "rankDf = salesAmount.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "\n",
    "rankDf = rankDf.select(\"ProductName\", \"CategoryName\", \"QuantitySold\", \"Price\", \"salesAmount\")\\\n",
    "                        .filter(col(\"rank\") == 1).orderBy(desc(\"salesAmount\"))\n",
    "rankDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL SOLUTION\n",
    "\n",
    "# select productname, categoryname, price, QuantitySold, totalamount\n",
    "# from\n",
    "# (select *, s.QuantitySold *tbl1.price as totalamount,\n",
    "#     DENSE_RANK() OVER (partition by tbl1.categoryname ORDER BY s.QuantitySold *tbl1.price desc) as danse_rank \n",
    "#  from sales as s left join\n",
    "# (select p. productid, p.productname, p.price, c.categoryname \n",
    "#  from products as p inner join categories as c on p.CategoryID=c.CategoryID) as tbl1\n",
    "#     on s.productid =tbl1.productid) as tbl2\n",
    "# where danse_rank =1\n",
    "# order by totalamount desc\n",
    "\n",
    "# or\n",
    "\n",
    "# select p.ProductName,c.CategoryName,s.QuantitySold,p.Price,(p.Price * s.QuantitySold) as salesAMount\n",
    "# from Products as p\n",
    "# join Categories as c on p.CategoryID = c.CategoryID\n",
    "# join Sales as s\n",
    "# on p.productID = s.productID\n",
    "# group by c.CategoryName \n",
    "# having max(salesAMount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7389c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------+-----------------+\n",
      "|customer_id|order_id|order_date|total_order_value|\n",
      "+-----------+--------+----------+-----------------+\n",
      "|          1|     101|2023-01-05|            100.0|\n",
      "|          2|     102|2023-02-10|            150.0|\n",
      "|          1|     103|2023-03-15|             80.0|\n",
      "|          3|     104|2023-04-20|            200.0|\n",
      "|          2|     105|2023-05-25|            120.0|\n",
      "|          1|     106|2023-06-30|             90.0|\n",
      "|          4|     107|2023-07-05|            110.0|\n",
      "|          2|     108|2023-08-10|             70.0|\n",
      "|          1|     109|2023-09-15|             95.0|\n",
      "|          3|     110|2023-10-20|            180.0|\n",
      "|          2|     111|2023-11-25|            130.0|\n",
      "|          1|     112|2023-12-30|             85.0|\n",
      "|          4|     113|2024-01-05|            105.0|\n",
      "|          2|     114|2024-02-10|             75.0|\n",
      "|          1|     115|2024-03-15|             88.0|\n",
      "|          3|     116|2024-04-20|            210.0|\n",
      "|          2|     117|2024-05-25|            140.0|\n",
      "|          1|     118|2024-06-30|             92.0|\n",
      "|          4|     119|2024-07-05|            115.0|\n",
      "|          2|     120|2024-08-10|             80.0|\n",
      "+-----------+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7124450661517131776/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124450661517131776%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Write a PySpark code to analyze customer behavior by calculating the following metrics for each customer:\n",
    "\n",
    "# 1)Customer lifetime value (CLV): the total revenue generated by a customer over \n",
    "# their entire history with the company.\n",
    "# 2)Purchase frequency: the average number of days between orders.\n",
    "# 3)Average order value: the mean of the total order values for all their orders.\n",
    "# 4)Customer churn rate: customers who have not made a purchase in the last 90 days\n",
    "\n",
    "schema_ = [\"customer_id\",\"order_id\",\"order_date\",\"total_order_value\"]\n",
    "\n",
    "data = [(1,101,\"2023-01-05\",100.00),\n",
    "(2,102,\"2023-02-10\",150.00),\n",
    "(1,103,\"2023-03-15\",80.00),\n",
    "(3,104,\"2023-04-20\",200.00),\n",
    "(2,105,\"2023-05-25\",120.00),\n",
    "(1,106,\"2023-06-30\",90.00),\n",
    "(4,107,\"2023-07-05\",110.00),\n",
    "(2,108,\"2023-08-10\",70.00),\n",
    "(1,109,\"2023-09-15\",95.00),\n",
    "(3,110,\"2023-10-20\",180.00),\n",
    "(2,111,\"2023-11-25\",130.00),\n",
    "(1,112,\"2023-12-30\",85.00),\n",
    "(4,113,\"2024-01-05\",105.00),\n",
    "(2,114,\"2024-02-10\",75.00),\n",
    "(1,115,\"2024-03-15\",88.00),\n",
    "(3,116,\"2024-04-20\",210.00),\n",
    "(2,117,\"2024-05-25\",140.00),\n",
    "(1,118,\"2024-06-30\",92.00),\n",
    "(4,119,\"2024-07-05\",115.00),\n",
    "(2,120,\"2024-08-10\",80.00)]\n",
    " \n",
    "df = spark.createDataFrame(data, schema_)\n",
    "df = df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3398f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|customer_id|total_revenue|\n",
      "+-----------+-------------+\n",
      "|          1|        630.0|\n",
      "|          2|        765.0|\n",
      "|          3|        590.0|\n",
      "|          4|        330.0|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1)Customer lifetime value (CLV): the total revenue generated by a customer over \n",
    "# their entire history with the company.\n",
    "\n",
    "clv = df.groupBy(\"customer_id\").agg(sum(\"total_order_value\").alias(\"total_revenue\"))\n",
    "clv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c5b821f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|purchase_frequency|\n",
      "+-----------+------------------+\n",
      "|          1| 90.33333333333333|\n",
      "|          2| 91.16666666666667|\n",
      "|          3|             183.0|\n",
      "|          4|             183.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2)Purchase frequency: the average number of days between orders.\n",
    "\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "customer_orders = df.withColumn(\"prev_order_date\", lag(\"order_date\",1).over(window_spec))\n",
    "customer_orders = customer_orders.withColumn(\"days_between_orders\", datediff(\"order_date\",\"prev_order_date\"))\n",
    "# customer_orders.show()\n",
    "purchase_frequency = customer_orders.groupBy(\"customer_id\").agg(avg(\"days_between_orders\")\\\n",
    "                                        .alias(\"purchase_frequency\"))\n",
    "purchase_frequency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db280e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|   avg_order_value|\n",
      "+-----------+------------------+\n",
      "|          1|              90.0|\n",
      "|          2|109.28571428571429|\n",
      "|          3|196.66666666666666|\n",
      "|          4|             110.0|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3)Average order value: the mean of the total order values for all their orders.\n",
    "avg_order_value = df.groupBy(\"customer_id\").agg(avg(\"total_order_value\").alias(\"avg_order_value\"))\n",
    "avg_order_value.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8772dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|churn_rate|\n",
      "+-----------+----------+\n",
      "|          1|       6.0|\n",
      "|          2|       5.0|\n",
      "|          3|       3.0|\n",
      "|          4|       2.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4)Customer churn rate: customers who have not made a purchase in the last 90 days\n",
    "\n",
    "# Find the maximum order date\n",
    "max_order_date = df.agg(max(\"order_date\")).collect()[0][0]\n",
    "\n",
    "# Calculate Customer Churn Rate\n",
    "customer_orders = df.withColumn(\"last_order_date\", lit(max_order_date))\n",
    "customer_orders = customer_orders.withColumn(\"churned\", when(datediff(col(\"last_order_date\"),\n",
    "                                                col(\"order_date\")) > 90, 1).otherwise(0))\n",
    "# customer_orders.show()\n",
    "\n",
    "churn_rate = customer_orders.groupBy(\"customer_id\").agg(sum(\"churned\").cast(FloatType()).alias(\"churn_rate\"))\n",
    "churn_rate.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c548a444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------------+------------------+----------+\n",
      "|customer_id|total_revenue|purchase_frequency|   avg_order_value|churn_rate|\n",
      "+-----------+-------------+------------------+------------------+----------+\n",
      "|          1|        630.0| 90.33333333333333|              90.0|       6.0|\n",
      "|          2|        765.0| 91.16666666666667|109.28571428571429|       5.0|\n",
      "|          3|        590.0|             183.0|196.66666666666666|       3.0|\n",
      "|          4|        330.0|             183.0|             110.0|       2.0|\n",
      "+-----------+-------------+------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join all calculated metrics together\n",
    "# both left and full join will work \n",
    "\n",
    "customer_metrics = clv.join(purchase_frequency, \"customer_id\", \"inner\") \\\n",
    "             .join(avg_order_value, \"customer_id\", \"inner\") \\\n",
    "             .join(churn_rate, \"customer_id\", \"inner\")\n",
    "\n",
    "customer_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e14d406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|emp_id|   name|salary|\n",
      "+------+-------+------+\n",
      "|     2|   meir|  3000|\n",
      "|     3|micheal|  3000|\n",
      "|     7|addilyn|  7400|\n",
      "|     8|   juan|  6100|\n",
      "|     9| kannon|  7400|\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7124446283817201664/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124446283817201664%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema = [\"emp_id\" , \"name\" , \"salary\"]\n",
    "data = [(2 , 'meir' , 3000),\n",
    "(3 , 'micheal' , 3000),\n",
    "(7 , 'addilyn' , 7400),\n",
    "(8 , 'juan' , 6100),\n",
    "(9 , 'kannon' , 7400)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "abb920eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|salary|count|\n",
      "+------+-----+\n",
      "|  3000|    2|\n",
      "|  7400|    2|\n",
      "|  6100|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grp = df.groupBy(\"salary\").agg(count(\"emp_id\").alias(\"count\"))\n",
    "grp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c41993dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-----+\n",
      "|salary|emp_id|   name|count|\n",
      "+------+------+-------+-----+\n",
      "|  3000|     2|   meir|    2|\n",
      "|  3000|     3|micheal|    2|\n",
      "|  7400|     7|addilyn|    2|\n",
      "|  7400|     9| kannon|    2|\n",
      "+------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.join(grp, \"salary\", \"inner\").filter(col(\"count\") > 1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL Solution\n",
    "\n",
    "# ✅Solution-1:\n",
    "    \n",
    "# select a.* , dense_rank() over(order by b.salary) as team_id \n",
    "# from emp as a inner join (select salary from emp group by salary having count(*) > 1) as b on\n",
    "# a.salary = b.salary;\n",
    "\n",
    "# ✅Solution-2:\n",
    "\n",
    "# select emp_id , name , salary , dense_rank() over(order by salary) as team_id \n",
    "# from (select a.* , count(*) over(partition by salary) as [count] from emp as a) as a\n",
    "# where [count] > 1 ;\n",
    "\n",
    "# ✅Solution-3:\n",
    "\n",
    "# select distinct a.* , dense_rank() over(order by a.salary) as team_id \n",
    "# from emp as a , emp as b \n",
    "# where a.salary = b.salary and a.emp_id <> b.emp_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f7132",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7125009569696673794/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125009569696673794%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "## 🔍 𝑯𝒐𝒘 𝒉𝒂𝒔𝒉𝒊𝒏𝒈 𝒊𝒎𝒑𝒓𝒐𝒗𝒆𝒔 𝒃𝒆𝒕𝒕𝒆𝒓 𝒕𝒊𝒎𝒆 𝒄𝒐𝒎𝒑𝒍𝒆𝒙𝒊𝒕𝒚 𝒊𝒏 𝑺𝒑𝒂𝒓𝒌?: 𝑴𝒂𝒈𝒊𝒄 𝒐𝒇 𝑴𝒖𝒓𝒎𝒖𝒓3 🕒\n",
    "\n",
    "The pursuit of faster and more efficient data processing is relentless. Spark currently utilizes 𝐌𝐮𝐫𝐌𝐮𝐫𝐡𝐚𝐬𝐡 3 (𝐌𝐮𝐫𝐦𝐮𝐫𝐡𝐚𝐬𝐡3_𝐱86_32) to calculate hashcodes, known for its efficiency validated through tests like the 𝐜𝐡𝐢-𝐬𝐪𝐮𝐚𝐫𝐞𝐝 𝐭𝐞𝐬𝐭and 𝐚𝐯𝐚𝐥𝐚𝐧𝐜𝐡𝐞 𝐭𝐞𝐬𝐭. This efficiency minimizes the chances of hash collisions in search algorithms, making it a game-changer in the realm of data processing. 🚀\n",
    "\n",
    "Let's understand different join strategies of Spark which leverage the hashing mechanism and how it helps under the hood. 🔗\n",
    "\n",
    "In 𝐁𝐫𝐨𝐚𝐝𝐜𝐚𝐬𝐭 𝐇𝐚𝐬𝐡 𝐉𝐨𝐢𝐧, Spark boosts efficiency by caching the smaller table and constructing its hash equivalent, significantly reducing shuffling and enabling local aggregation. This transformation turns search time complexity from O(n log(n)) to an astonishing O(1). \n",
    "\n",
    "Conversely, 𝐒𝐡𝐮𝐟𝐟𝐥𝐞 𝐇𝐚𝐬𝐡 𝐉𝐨𝐢𝐧 takes a different approach, utilizing a hash table based on the join key without broadcasting it. This approach maintains a regular join, eliminating the need for key sorting and further optimizing time complexity. 📊\n",
    "\n",
    "🛠️ **𝑯𝒂𝒔𝒉 𝑱𝒐𝒊𝒏 𝑾𝒐𝒓𝒌𝒇𝒍𝒐𝒘** 🛠️:\n",
    "```\n",
    "\n",
    "1. 𝘏𝘢𝘴𝘩 𝘛𝘢𝘣𝘭𝘦 𝘊𝘳𝘦𝘢𝘵𝘪𝘰𝘯 📦:\n",
    "\n",
    "    ✔ A memory-efficient hash table emerges from the smaller table.\n",
    "    ✔ It houses key-value pairs, where each key corresponds to a column from the smaller table, and \n",
    "        the value represents the associated row data or a reference.\n",
    "    ✔ Hash codes, courtesy of Murmur3, dictate the exact storage location for each key.\n",
    "\n",
    "2. 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘵𝘩𝘦 𝘏𝘢𝘴𝘩 𝘛𝘢𝘣𝘭𝘦 🛠️:\n",
    "\n",
    "    ✔ As the smaller table data is parsed, the hash function generates hash codes for each key, placing them \n",
    "        within the hash table.\n",
    "    ✔ Occasional collisions are adeptly managed through methods like open addressing or chaining.\n",
    "\n",
    "3. 𝘑𝘰𝘪𝘯 𝘖𝘱𝘦𝘳𝘢𝘵𝘪𝘰𝘯 🤝:\n",
    "\n",
    "    ✔ The larger table (probe table) enters the stage.\n",
    "    ✔ For each row, the join key undergoes hashing, and the corresponding value location is pinpointed \n",
    "        within the hash table.\n",
    "    ✔ A successful match leads to a joined row.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7c30c",
   "metadata": {},
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4597467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|customer_id|purchase_amount|purchase_date|\n",
      "+-----------+---------------+-------------+\n",
      "|          1|            100|   2023-01-05|\n",
      "|          2|            200|   2023-02-10|\n",
      "|          1|            150|   2023-03-15|\n",
      "|          3|            300|   2023-04-20|\n",
      "|          2|             50|   2023-05-25|\n",
      "|          1|            300|   2023-06-30|\n",
      "|          4|             50|   2023-07-05|\n",
      "|          2|            500|   2023-08-10|\n",
      "|          1|             70|   2023-09-15|\n",
      "|          3|            120|   2023-10-20|\n",
      "|          5|           1100|   2023-10-16|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7125185637636333568/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125185637636333568%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# High-Value Customers: Customers with a total purchase amount greater than a specified threshold (e.g. 1000).\n",
    "# Occasional Shoppers: Customers who have made purchases, but their total purchase amount is below the threshold.\n",
    "# One-Time Buyers: Customers who have made only one purchase.\n",
    "\n",
    "schema_ = [\"customer_id\",\"purchase_amount\",\"purchase_date\"]\n",
    "\n",
    "data = [(1,100,\"2023-01-05\"),\n",
    "(2,200,\"2023-02-10\"),\n",
    "(1,150,\"2023-03-15\"),\n",
    "(3,300,\"2023-04-20\"),\n",
    "(2,50,\"2023-05-25\"),\n",
    "(1,300,\"2023-06-30\"),\n",
    "(4,50,\"2023-07-05\"),\n",
    "(2,500,\"2023-08-10\"),\n",
    "(1,70,\"2023-09-15\"),\n",
    "(3,120,\"2023-10-20\"),\n",
    "(5,1100,\"2023-10-16\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema_)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62acfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----+\n",
      "|customer_id|            segment|rank|\n",
      "+-----------+-------------------+----+\n",
      "|          5|High-Value Customer|   1|\n",
      "|          2| Occasional Shopper|   1|\n",
      "|          1| Occasional Shopper|   2|\n",
      "|          3| Occasional Shopper|   3|\n",
      "|          4|     One-Time Buyer|   1|\n",
      "+-----------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the segmentation thresholds\n",
    "high_value_threshold = 1000\n",
    "occasional_shopper_threshold = 100\n",
    "\n",
    "# Create a new DataFrame with customer segments\n",
    "segmented_customers = df.groupBy(\"customer_id\").agg(sum(\"purchase_amount\").alias(\"total_purchase_amount\"))\n",
    "# segmented_customers.show()\n",
    "\n",
    "segmented_customers = segmented_customers.withColumn(\"segment\",\n",
    "    when(col(\"total_purchase_amount\") > high_value_threshold, \"High-Value Customer\")\n",
    "    .when(col(\"total_purchase_amount\") <= occasional_shopper_threshold, \"One-Time Buyer\")\n",
    "    .otherwise(\"Occasional Shopper\"))\n",
    "\n",
    "window_spec = Window.partitionBy(\"segment\").orderBy(col(\"total_purchase_amount\").desc())\n",
    "segmented_customers = segmented_customers.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "segmented_customers.select(\"customer_id\", \"segment\", \"rank\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf9f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127097269199245312/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127097269199245312%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# There are two ways to handle row duplication in PySpark dataframes. \n",
    "# The distinct() function in PySpark is used to drop/remove duplicate rows (all columns) from a DataFrame, \n",
    "# while dropDuplicates() is used to drop rows based on one or more columns. \n",
    "\n",
    "data = [(\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600), \\\n",
    "    (\"Robert\", \"Sales\", 4100), \\\n",
    "    (\"Maria\", \"Finance\", 3000), \\\n",
    "    (\"James\", \"Sales\", 3000), \\\n",
    "    (\"Scott\", \"Finance\", 3300), \\\n",
    "    (\"Jen\", \"Finance\", 3900), \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000), \\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ]\n",
    "\n",
    "column= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = column)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ed7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct count: 9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "Distinct count: 9\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n",
      "Distinct count of department salary : 8\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|Maria        |Finance   |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Michael      |Sales     |4600  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct\n",
    "distinctDF = df.distinct()\n",
    "print(\"Distinct count: \"+str(distinctDF.count()))\n",
    "distinctDF.show(truncate=False)\n",
    "\n",
    "#Drop_duplicates\n",
    "df2 = df.dropDuplicates()\n",
    "print(\"Distinct count: \"+str(df2.count()))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#Drop_duplicates_on_selected_columns\n",
    "dropDisDF = df.dropDuplicates([\"department\",\"salary\"])\n",
    "print(\"Distinct count of department salary : \"+str(dropDisDF.count()))\n",
    "dropDisDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7126623558360780800/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7126623558360780800%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# # Load the large sample dataset\n",
    "# df = spark.read.csv(\"auto_insurance_claims_large.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Define conditions for potential fraud detection\n",
    "# fraud_detection_result = df.withColumn(\"is_potentially_fraudulent\",\n",
    "#                    when(\n",
    "#                      (col(\"claim_amount\") > 10000) |\n",
    "#                      (col(\"previous_claims\") > 1) |\n",
    "#                      (col(\"insured_age\") < 25) |\n",
    "#                      (col(\"insured_occupation\") == \"Lawyer\") |\n",
    "#                      (col(\"insured_location\") == \"City A\" and col(\"claim_type\") == \"Theft\"),\n",
    "#                      1\n",
    "#                    ).otherwise(0))\n",
    "\n",
    "# fraud_detection_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd69d0f",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7126445296313593856/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7126445296313593856%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "#### Write a SQL query to group the parent-child by the order in the table.  \n",
    "#### So, for example Adult 1 goes with child 1 and Adult 2 goes with child 2& 3 and so on and \n",
    "#### introduce a new column which has parent information. Please see the diagram below for clear understanding. \n",
    "#### Do give it a try and feel free to post your solution the comment section ✳\n",
    "\n",
    "✅Here is the script for you to practice: 👇\n",
    "```\n",
    "    drop table relation;\n",
    "    create table relation (position int , Name varchar(10) , Name_Key int);\n",
    "    insert into relation values (1, 'Adult' , 123);\n",
    "    insert into relation values (2, 'Child' , 555);\n",
    "    insert into relation values (3, 'Child' , 666);\n",
    "    insert into relation values (4, 'Adult' , 456);\n",
    "    insert into relation values (5, 'Child' , 777);\n",
    "    insert into relation values (6, 'Adult' , 789);\n",
    "    insert into relation values (7, 'Child' , 888);\n",
    "    insert into relation values (8, 'Child' , 999);\n",
    "\n",
    "✅Solution-1 (using window function):\n",
    "\n",
    "with assign_0_1 as (\n",
    "    select * , case when Name = 'Adult' then 1 else 0 end as assign_0_1 , \n",
    "    sum(case when Name = 'Adult' then 1 else 0 end) over(order by position) as rolling_assign_0_1\n",
    "    from relation )\n",
    "select b.position , b.name , b.Name_Key , a.Name_Key as parent_Name_key from\n",
    "(select Name_key , rolling_assign_0_1 from assign_0_1 where assign_0_1 = 1) as a inner join assign_0_1 as b\n",
    "on a.rolling_assign_0_1 = b.rolling_assign_0_1;\n",
    "\n",
    "✅Solution-2 (using join):\n",
    "\n",
    "with cte as (\n",
    "    select b.position , b.Name , b.Name_key , max(a.position) as maxPosition from\n",
    "    (select * from relation where Name = 'Adult') as a \n",
    "    inner join relation as b on b.position >= a.position\n",
    "    group by b.position , b.Name , b.Name_key )\n",
    "select a.position , a.Name , a.Name_Key , b.Name_Key as parent_name_key \n",
    "from cte as a inner join relation as b on a.maxPosition = b.position;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48bdea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+\n",
      "|position| Name|Name_Key|\n",
      "+--------+-----+--------+\n",
      "|       1|Adult|     123|\n",
      "|       2|Child|     555|\n",
      "|       3|Child|     666|\n",
      "|       4|Adult|     456|\n",
      "|       5|Child|     777|\n",
      "|       6|Adult|     789|\n",
      "|       7|Child|     888|\n",
      "|       8|Child|     999|\n",
      "+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"position\" , \"Name\" , \"Name_Key\"]\n",
    "\n",
    "data = [(1, 'Adult' , 123),\n",
    "(2, 'Child' , 555),\n",
    "(3, 'Child' , 666),\n",
    "(4, 'Adult' , 456),\n",
    "(5, 'Child' , 777),\n",
    "(6, 'Adult' , 789),\n",
    "(7, 'Child' , 888),\n",
    "(8, 'Child' , 999),]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e6f99ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+\n",
      "|EmpId|Project|Salary|Variable|\n",
      "+-----+-------+------+--------+\n",
      "|  121|     P1|  8000|     500|\n",
      "|  321|     P2| 10000|    1000|\n",
      "|  421|     P1| 12000|       0|\n",
      "+-----+-------+------+--------+\n",
      "\n",
      "+-----+------------+---------+-------------+----------+\n",
      "|EmpId|    FullName|ManagerId|DateOfJoining|      City|\n",
      "+-----+------------+---------+-------------+----------+\n",
      "|  121|   John Snow|      321|   01/31/2019|   Toronto|\n",
      "|  323|Walter White|      986|   01/30/2020|California|\n",
      "|  423|Kuldeep Rana|      876|   27/11/2021| New Delhi|\n",
      "+-----+------------+---------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7125706674941607936/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125706674941607936%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 🎯Table – EmployeeSalary\n",
    "\n",
    "col1 = [\"EmpId\", \"Project\" ,\"Salary\", \"Variable\"]\n",
    "data1 = [(121, \"P1\", 8000, 500),\n",
    "(321, \"P2\", 10000 ,1000),\n",
    "(421, \"P1\", 12000, 0)]\n",
    "\n",
    "# 🎯Table – EmployeeDetails\n",
    "\n",
    "col2 = [\"EmpId\", \"FullName\", \"ManagerId\" ,\"DateOfJoining\" ,\"City\"]\n",
    "data2 = [(121, \"John Snow\", 321, \"01/31/2019\", \"Toronto\"),\n",
    "(323, \"Walter White\", 986, \"01/30/2020\", \"California\"),\n",
    "(423 , \"Kuldeep Rana\", 876 ,\"27/11/2021\", \"New Delhi\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(data2, col2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f068bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"EmployeeSalary\")\n",
    "df2.createOrReplaceTempView(\"EmployeeDetails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f61ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|empid|\n",
      "+-----+\n",
      "|  421|\n",
      "|  321|\n",
      "|  121|\n",
      "|  423|\n",
      "|  323|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔹From the above two tables, fetch me the only emp id columns using single query?\n",
    "spark.sql(\"select distinct empid from EmployeeSalary \\\n",
    "          union \\\n",
    "          select distinct empid from EmployeeDetails\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7cdc98b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|empid|\n",
      "+-----+\n",
      "|  321|\n",
      "|  421|\n",
      "|  323|\n",
      "|  423|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔹Give me the only empid, which are not common in both the tables?\n",
    "spark.sql(\"select empid from EmployeeSalary where empid not in (select empid from EmployeeDetails) \\\n",
    "          union\\\n",
    "          select empid from EmployeeDetails where empid not in (select empid from EmployeeSalary)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb8d049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|     John|    Snow|\n",
      "|   Walter|   White|\n",
      "|  Kuldeep|    Rana|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔹Write two more column from the Full name, and divide it as First name and last name?\n",
    "\n",
    "spark.sql(\"select SUBSTRING_INDEX(FullName,' ',1) AS firstname, \\\n",
    "                            SUBSTRING_INDEX(FullName,' ',-1) AS lastname from EmployeeDetails\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a063f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|Project|maxSalary|\n",
      "+-------+---------+\n",
      "|     P1|    12000|\n",
      "|     P2|    10000|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 🔹Give me the highest salary from different projects in employee salary table.\n",
    "spark.sql(\"select Project, max(Salary) as maxSalary from EmployeeSalary group by Project\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b21055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.linkedin.com/feed/update/urn:li:activity:7125538528997892096/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125538528997892096%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# schema = StructType([\n",
    "#     StructField(\"patient_id\", IntegerType(), True),\n",
    "#     StructField(\"medical_condition\", StringType(), True),\n",
    "#     StructField(\"admission_date\", DateType(), True),\n",
    "#     StructField(\"discharge_date\", DateType(), True),\n",
    "#     StructField(\"readmission_date\", DateType(), True)])\n",
    "\n",
    "# patient_data = spark.read.schema(schema).csv(\"dbfs:/FileStore/raw/patient_data.csv\", header=True)\n",
    "\n",
    "# # Step 1: Calculate days_between_discharge_and_readmission\n",
    "# patient_data = patient_data.withColumn(\"days_between_discharge_and_readmission\", \\\n",
    "#                                        datediff(col(\"readmission_date\"), col(\"discharge_date\")))\n",
    "\n",
    "# # Step 2: Readmission Risk Score\n",
    "# window_spec = Window.partitionBy(\"medical_condition\").orderBy(\"days_between_discharge_and_readmission\")\n",
    "# patient_data = patient_data.withColumn(\"readmission_risk_score\", rank().over(window_spec))\n",
    "# # patient_data = patient_data.withColumn(\"readmission_risk_score\", col(\"readmission_risk_score\").cast(\"integer\"))\n",
    "\n",
    "# # Step 3: Identify High-Risk Patients\n",
    "# high_risk_patients = patient_data.filter(col(\"readmission_risk_score\") == 1)\n",
    "# high_risk_patients = high_risk_patients.select(\"patient_id\", \"medical_condition\", \"readmission_risk_score\")\n",
    "\n",
    "# # Show the results\n",
    "# high_risk_patients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e711942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|Empid|EmpName|\n",
      "+-----+-------+\n",
      "|    1|  Suman|\n",
      "|    2| Sushma|\n",
      "+-----+-------+\n",
      "\n",
      "+-----+-------+----+\n",
      "|Empid|EmpName|Dept|\n",
      "+-----+-------+----+\n",
      "|    3|Akshada|  IT|\n",
      "|    4|    XYZ|  HR|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127574447221260288/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127574447221260288%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema1 = [\"Empid\",\"EmpName\"]\n",
    "data1 = [(1,\"Suman\"), (2,\"Sushma\")]\n",
    " \n",
    "schema2 = [\"Empid\",\"EmpName\",\"Dept\"]\n",
    "data2 = [(3,\"Akshada\",\"IT\"), (4,\"XYZ\",\"HR\")]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d09f7969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----+\n",
      "|Empid|EmpName|Dept|\n",
      "+-----+-------+----+\n",
      "|    1|  Suman|null|\n",
      "|    2| Sushma|null|\n",
      "|    3|Akshada|  IT|\n",
      "|    4|    XYZ|  HR|\n",
      "+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df1.withColumn(\"Dept\", lit(\"null\"))\n",
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cb4d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127518232948654080/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127518232948654080%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Method1:\n",
    "    \n",
    "# SELECT DISTINCT t1.id + 1 AS missing_number\n",
    "# FROM ids t1\n",
    "# WHERE NOT EXISTS (SELECT * FROM ids t2 WHERE t2.id = t1.id + 1)\n",
    "# AND t1.id < (SELECT MAX(id) FROM ids);\n",
    "\n",
    "# drop table ids;\n",
    "\n",
    "# Method 2:\n",
    "# WITH newoutput AS (\n",
    "#   SELECT id, LEAD(id) OVER (ORDER BY id) AS next_id\n",
    "#   FROM ids )\n",
    "# SELECT id + 1 AS missing_number\n",
    "# FROM newoutput\n",
    "# WHERE next_id IS NOT NULL\n",
    "# AND next_id <> id + 1;\n",
    "\n",
    "# data = [(1,), (2,), (3,), (5,), (6,), (7,), (9,)]\n",
    "# df = spark.createDataFrame(data, [\"id\"])\n",
    "# df.show()\n",
    "\n",
    "# with cte as (\n",
    "# select id, case \n",
    "# when lead(id, 1) over(order by id) = id + 1 or lead(id,1) over(order by id) is null then NULL \n",
    "# else id + 1 end as result \n",
    "# from input_tbl ) \n",
    "# select result from cte where result is not null;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a94d13fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n",
      "+-----+------------+-----------+\n",
      "|Seqno|        Name|      name1|\n",
      "+-----+------------+-----------+\n",
      "|    1|  john jones|  JohnJones|\n",
      "|    2|tracey smith|TraceySmith|\n",
      "|    3| amy sanders| AmySanders|\n",
      "+-----+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127187913859104768/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127187913859104768%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)]\n",
    "    return resStr\n",
    "\n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())\n",
    "\n",
    "column = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"), (\"2\", \"tracey smith\"),(\"3\", \"amy sanders\")]\n",
    "df = spark.createDataFrame(data=data,schema=column)\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.withColumn(\"name1\", convertUDF(col(\"Name\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26bb834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Alice’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n",
      "('Adventures', 1)\n",
      "('in', 1)\n",
      "('Wonderland', 1)\n",
      "('Project', 1)\n",
      "('Gutenberg’s', 1)\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127270902743896064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127270902743896064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "records = [\"Project\",\"Gutenberg’s\",\"Alice’s\",\"Adventures\", \n",
    "               \"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\",\"Adventures\", \n",
    "               \"in\",\"Wonderland\",\"Project\",\"Gutenberg’s\"]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(records)\n",
    "\n",
    "# The map() syntax is-map(f, preservesPartitioning=False)\n",
    "# We are adding a new element having value 1 for each element in this PySpark map() example, \n",
    "# and the output of the RDD is PairRDDFunctions, which has key-value pairs, \n",
    "# where we have a word (String type) as Key and 1 (Int type) as Value.\n",
    "\n",
    "rdd2 = rdd.map(lambda x: (x,1))\n",
    "for element in rdd2.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11931fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127343624580739072/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127343624580739072%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# df = spark.read.csv(\"dbfs:/FileStore/raw/orders_uncleaned.csv\",header = True , inferSchema = True)\n",
    "\n",
    "# # Clean missing dates\n",
    "# df = df.withColumn(\"order_date\", when(isnull(col(\"order_date\")), lit(\"2023-01-01\")).otherwise(col(\"order_date\")))\n",
    "\n",
    "# # Clean missing product_id\n",
    "# df = df.withColumn(\"product_id\", when(isnull(col(\"product_id\")), -1).otherwise(col(\"product_id\")))\n",
    "\n",
    "# # Clean missing or inconsistent quantity_sold\n",
    "# df = df.withColumn(\"quantity_sold\", when(isnull(col(\"quantity_sold\")) | (col(\"quantity_sold\") == \"\"), 0)\\\n",
    "#                    .otherwise(col(\"quantity_sold\")).cast(\"int\"))\n",
    "\n",
    "# # Show the cleaned data\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af206d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "# 🔹Calculate the total revenue for each order, considering the quantity_sold and unit_price. \n",
    "# Create a new column named total_revenue.\n",
    "\n",
    "# 🔹Calculate the cumulative revenue for each customer. \n",
    "# Create a new column named cumulative_revenue for each customer, where cumulative revenue is the sum of \n",
    "# the total_revenue for all orders placed by the customer.\n",
    "\n",
    "# 🔹Identify the top-selling product for each month. Create a new column named top_selling_product that \n",
    "# contains the product_id of the product with the highest total quantity sold in each month.\n",
    "\n",
    "# 🔹Calculate the average order value for each customer. Create a new column named average_order_value \n",
    "# for each customer, which is the average of total_revenue for all orders placed by the customer.\n",
    "\n",
    "# 🔹Identify the customers with the highest lifetime value (CLV). CLV is calculated as the sum of \n",
    "# total_revenue for each customer over time. Create a new column named clv to store the CLV for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebffdaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|  S1| 20|\n",
      "|  S2| 23|\n",
      "+----+---+\n",
      "\n",
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "|  S1| 22|\n",
      "|  S4| 27|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127648844800008193/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127648844800008193%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Table_A:\n",
    "col1 = [\"Name\" , \"Age\"]\n",
    "data1 = [(\"S1\"  , 20), (\"S2\"  , 23)]\n",
    "\n",
    "# Table_B:\n",
    "col2 = [\"Name\" , \"Age\"]\n",
    "data2 = [(\"S1\"  , 22), (\"S4\"  , 27)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df1.show()\n",
    "\n",
    "df2 = spark.createDataFrame(data2, col2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedData = df1.join(df2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b769ca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|name|Age_A|Age_B|\n",
      "+----+-----+-----+\n",
      "|  S1|   20|   22|\n",
      "|  S2|   23| null|\n",
      "|  S4| null|   27|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_df = df1.alias(\"A\").join(df2.alias(\"B\"), on='Name', how=\"OUTER\")\n",
    "final_df = join_df.select(\"name\", col(\"A.Age\").alias(\"Age_A\"), col(\"B.Age\").alias(\"Age_B\"))\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "865d6ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+---+\n",
      "|name|Age_A|Age_B|Age|\n",
      "+----+-----+-----+---+\n",
      "|  S1|   20|   22| 22|\n",
      "|  S2|   23| null| 23|\n",
      "|  S4| null|   27| 27|\n",
      "+----+-----+-----+---+\n",
      "\n",
      "+----+---+------+\n",
      "|name|Age|Action|\n",
      "+----+---+------+\n",
      "|  S1| 22|update|\n",
      "|  S2| 23|Delete|\n",
      "|  S4| 27|insert|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Condition for Update, Delete, Insert \"Action\"\n",
    "\n",
    "result = final_df.withColumn(\"Age\", expr(\"case when Age_A is null then Age_B\\\n",
    "                                         when Age_B is null then Age_A\\\n",
    "                                         when Age_A < Age_B then Age_B else Age_A\\\n",
    "                                         end \"))\n",
    "result.show()\n",
    "\n",
    "Result_df = result.withColumn(\"Action\", expr(\"case when Age_A is null then 'insert'\\\n",
    "                                                   when Age_B is null then 'Delete'\\\n",
    "                                                   when Age_A <> Age_B then 'update' else null end\"))                                           \n",
    "                         \n",
    "# Select relevant columns\n",
    "Result_df.select(\"name\", \"Age\", \"Action\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef2b5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+----------+\n",
      "|username|activity| startDate|   endDate|\n",
      "+--------+--------+----------+----------+\n",
      "|   Alice|  Travel|2020-02-12|2020-02-20|\n",
      "|   Alice| Dancing|2020-02-21|2020-02-23|\n",
      "|   Alice|  Travel|2020-02-24|2020-02-28|\n",
      "|     Bob|  Travel|2020-02-11|2020-02-18|\n",
      "+--------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127867344248291328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127867344248291328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 𝐏𝐫𝐨𝐛𝐥𝐞𝐦 𝐒𝐭𝐚𝐭𝐞𝐦𝐞𝐧𝐭:\n",
    "# Write an SQL query to show the 𝐬𝐞𝐜𝐨𝐧𝐝 𝐦𝐨𝐬𝐭 𝐫𝐞𝐜𝐞𝐧𝐭 𝐚𝐜𝐭𝐢𝐯𝐢𝐭𝐲 𝐨𝐟 𝐞𝐚𝐜𝐡 𝐮𝐬𝐞𝐫.\n",
    "# If the user only has one activity, return that one. A user 𝐜𝐚𝐧'𝐭 𝐩𝐞𝐫𝐟𝐨𝐫𝐦 𝐦𝐨𝐫𝐞 𝐭𝐡𝐚𝐧 𝐨𝐧𝐞 𝐚𝐜𝐭𝐢𝐯𝐢𝐭𝐲 𝐚𝐭 𝐭𝐡𝐞 𝐬𝐚𝐦𝐞 𝐭𝐢𝐦𝐞.\n",
    "\n",
    "\n",
    "# table UserActivity\n",
    "schema_ = [\"username\" , \"activity\", \"startDate\"  , \"endDate\" ]\n",
    "\n",
    "data = [\n",
    "('Alice','Travel','2020-02-12','2020-02-20'),\n",
    "('Alice','Dancing','2020-02-21','2020-02-23'),\n",
    "('Alice','Travel','2020-02-24','2020-02-28'),\n",
    "('Bob','Travel','2020-02-11','2020-02-18')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema_)\n",
    "df = df.withColumn(\"startDate\", to_date(\"startDate\")).withColumn(\"endDate\", to_date(\"endDate\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbaf7d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|username|no_of_activities|\n",
      "+--------+----------------+\n",
      "|   Alice|               3|\n",
      "|     Bob|               1|\n",
      "+--------+----------------+\n",
      "\n",
      "+--------+--------+----------+----------+----+\n",
      "|username|activity| startDate|   endDate|drnk|\n",
      "+--------+--------+----------+----------+----+\n",
      "|   Alice|  Travel|2020-02-24|2020-02-28|   1|\n",
      "|   Alice| Dancing|2020-02-21|2020-02-23|   2|\n",
      "|   Alice|  Travel|2020-02-12|2020-02-20|   3|\n",
      "|     Bob|  Travel|2020-02-11|2020-02-18|   1|\n",
      "+--------+--------+----------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(\"username\").agg(count(\"*\").alias(\"no_of_activities\"))\n",
    "df1.show()\n",
    "\n",
    "window_spec = Window.partitionBy(\"username\").orderBy(desc(\"startDate\"))\n",
    "\n",
    "df2 = df.withColumn(\"drnk\", dense_rank().over(window_spec))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23a7716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+----------+----------+----+\n",
      "|username|no_of_activities|activity| startDate|   endDate|drnk|\n",
      "+--------+----------------+--------+----------+----------+----+\n",
      "|   Alice|               3|  Travel|2020-02-24|2020-02-28|   1|\n",
      "|   Alice|               3| Dancing|2020-02-21|2020-02-23|   2|\n",
      "|   Alice|               3|  Travel|2020-02-12|2020-02-20|   3|\n",
      "|     Bob|               1|  Travel|2020-02-11|2020-02-18|   1|\n",
      "+--------+----------------+--------+----------+----------+----+\n",
      "\n",
      "+--------+--------+----------+----------+\n",
      "|username|activity| startDate|   endDate|\n",
      "+--------+--------+----------+----------+\n",
      "|   Alice| Dancing|2020-02-21|2020-02-23|\n",
      "|     Bob|  Travel|2020-02-11|2020-02-18|\n",
      "+--------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedData = df1.join(df2, on=\"username\", how=\"inner\")\n",
    "joinedData.show()\n",
    "\n",
    "filterData = joinedData.filter((col(\"drnk\") == 2) | (col(\"no_of_activities\") == 1))\n",
    "filterData.select(\"username\", \"activity\", \"startDate\", \"endDate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL SOLUTION\n",
    "\n",
    "# WITH cte1 as\n",
    "# (SELECT *, COUNT(1) over(partition by username) as no_of_activities\n",
    "# , dense_rank() over(partition by username ORDER by startdate desc) as drnk\n",
    "# from UserActivity)\n",
    "\n",
    "# SELECT username, activity, startdate, enddate\n",
    "# from cte1\n",
    "# where drnk = 2 OR no_of_activities = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717811f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|name          |cleaned_name|\n",
      "+--------------+------------+\n",
      "|R@hul K#mar   |Rhul Kmar   |\n",
      "|S@m!rtha P@tel|Smrtha Ptel |\n",
      "|M!dhavi S#ngh |Mdhavi Sngh |\n",
      "+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7128353648228675584/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7128353648228675584%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data = [\n",
    "  {\"name\": \"R@hul K#mar\"},\n",
    "  {\"name\": \"S@m!rtha P@tel\"},\n",
    "  {\"name\": \"M!dhavi S#ngh\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "# df.show()\n",
    "\n",
    "# Remove special characters from names using regex\n",
    "df_cleaned = df.withColumn(\"cleaned_name\", regexp_replace(col(\"name\"), \"[^a-zA-Z ]\", \"\"))\n",
    "df_cleaned.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3526a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-------+--------+--------+--------------+---------------+--------+\n",
      "|sensor_id|     sensor_name|rack_no|cp_value|sp_value|senor_pf_value|threshold_value|op_value|\n",
      "+---------+----------------+-------+--------+--------+--------------+---------------+--------+\n",
      "|        1|Porximity Sensor|     14|    10.5|    20.5|          30.5|           40.5|    50.5|\n",
      "|        2|    Level Sensor|     21|    11.5|    21.5|          31.5|           41.5|    51.5|\n",
      "+---------+----------------+-------+--------+--------+--------------+---------------+--------+\n",
      "\n",
      "['cp_value', 'sp_value', 'senor_pf_value', 'threshold_value', 'op_value']\n",
      "+--------+--------+--------------+---------------+--------+\n",
      "|cp_value|sp_value|senor_pf_value|threshold_value|op_value|\n",
      "+--------+--------+--------------+---------------+--------+\n",
      "|    10.5|    20.5|          30.5|           40.5|    50.5|\n",
      "|    11.5|    21.5|          31.5|           41.5|    51.5|\n",
      "+--------+--------+--------------+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7128290390381592576/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7128290390381592576%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# I have a Table named sensor which has around 1000 columns having col names as sensor_id, sensor_name, \n",
    "# cp_value, sp_value, senor_pf_value, threshold_value and so on. Say around 400 columns have “value” included \n",
    "# in column names. Is there any functionality or way that can help me select only those columns \n",
    "# which have “value” as a pattern in them?\n",
    "\n",
    "\n",
    "data = [(1, \"Porximity Sensor\", \"14\", 10.5, 20.5, 30.5, 40.5, 50.5),\n",
    "          (2, \"Level Sensor\", \"21\", 11.5, 21.5, 31.5, 41.5, 51.5)]\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns = [\"sensor_id\", \"sensor_name\", \"rack_no\", \"cp_value\", \"sp_value\", \"senor_pf_value\", \n",
    "              \"threshold_value\", \"op_value\"]\n",
    "\n",
    "# // Create the DataFrame\n",
    "sensorDF = spark.createDataFrame(data, columns)\n",
    "sensorDF.show()\n",
    "\n",
    "# // Get a list of column names containing \"value\"\n",
    "# columnsWithValuesDF = sensorDF.columns.filter(lambda colName : colName.contains(\"value\"))\n",
    "# columnsWithValuesDF.foreach(println)\n",
    "# print(\"\\n\")\n",
    "\n",
    "columnsWithValuesDF = []\n",
    "for i in columns:\n",
    "    if i.endswith(\"value\"):\n",
    "        columnsWithValuesDF.append(i)\n",
    "\n",
    "print(columnsWithValuesDF)\n",
    "   \n",
    "# // Select only the columns with \"value\" in their names\n",
    "selectedColumnsDF = sensorDF.select(columnsWithValuesDF)\n",
    "\n",
    "# // Show the resulting DataFrame\n",
    "selectedColumnsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca5b9e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|In a world full o...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7128161770971496448/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7128161770971496448%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 📊 We have a file *.txt at input path. Find out the frequency of each word exists in text file.\n",
    "\n",
    "line = \"\"\"In a world full of #technology, understanding data is key to success. Data analytics, 'AI', and machine learning are transforming industries. Companies are racing to harness the power of data-driven insights; But,  data is messy and comes in various formats - structured, unstructured, and semi-structured. The challenge is to clean, process, and  analyze this data effectively. There's a growing demand for data scientists, analysts, and engineers who can unlock the value hidden within the data.\"\"\"\n",
    "\n",
    "dataDf = spark.createDataFrame([(line,)], ['text'])\n",
    "dataDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20254bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|      words|counts|\n",
      "+-----------+------+\n",
      "|          a|     2|\n",
      "|   analysts|     1|\n",
      "|  analytics|     1|\n",
      "|    analyze|     1|\n",
      "|        and|     5|\n",
      "|        are|     2|\n",
      "|        ata|     1|\n",
      "|        can|     1|\n",
      "|  challenge|     1|\n",
      "|      clean|     1|\n",
      "|      comes|     1|\n",
      "|       data|     6|\n",
      "|     demand|     1|\n",
      "|     driven|     1|\n",
      "|effectively|     1|\n",
      "|  engineers|     1|\n",
      "|        for|     1|\n",
      "|    formats|     1|\n",
      "|       full|     1|\n",
      "|    growing|     1|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = \"[';.,#*-_]\"\n",
    "replacement = ' '\n",
    "regexDf = dataDf.select(regexp_replace(col('text'), pattern, replacement).alias('words'))\n",
    "splitDf = regexDf.select(explode(split(lower('words'), ' ')).alias('words'))\n",
    "countDf = splitDf.groupBy('words').agg(count('*').alias('counts'))\\\n",
    "                        .filter(col('words') != '').orderBy('words')\n",
    "countDf.show()\n",
    "\n",
    "# new_wordcount_df = dataDf.select(explode(split('text', ' ')).alias('line'))\n",
    "# new_wordcount_df = new_wordcount_df.withColumn(\"line\", regexp_replace(col('line'), \"[,#''.;-]\", \"\"))\n",
    "# new_wordcount_df = new_wordcount_df.filter(col('line') != '').select(lower('line').alias('line'))\n",
    "# count_of_words_df = new_wordcount_df.groupBy('line').agg(count('line').alias('word_count'))\n",
    "\n",
    "# count_of_words_df.show(count_of_words_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ff71e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "fedcba\n",
      "['f', 'e', ',', 'd', '|', 'c', '.', 'b', ',', 'a']\n",
      "fe,d|c.b,a\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7130131940963143681/?updateEntityUrn=urn%3Ali%÷3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130131940963143681%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29#\n",
    "\n",
    "# python question to reverse string without reversing the special symbols\n",
    "\n",
    "def reverse_string_without_special(input_str):\n",
    "  # Step 1: Identify positions of special symbols\n",
    "  special_positions = [i for i, char in enumerate(input_str) if not char.isalnum()]\n",
    "  print(special_positions)\n",
    "\n",
    "  # Step 2: Reverse only alphanumeric characters\n",
    "  alphanumeric_chars = [char for char in input_str if char.isalnum()]\n",
    "  reversed_alphanumeric = ''.join(alphanumeric_chars[::-1])\n",
    "  print(reversed_alphanumeric)\n",
    "\n",
    "  # Step 3: Place special symbols back in their original positions\n",
    "  reversed_string = list(reversed_alphanumeric)\n",
    "  for pos in special_positions:\n",
    "    reversed_string.insert(pos, input_str[pos])\n",
    "  print(reversed_string)\n",
    "\n",
    "  return ''.join(reversed_string)\n",
    "\n",
    "# Example usage\n",
    "input_str = \"ab,c|d.e,f\"\n",
    "output_str = reverse_string_without_special(input_str)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f1f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efe938e1",
   "metadata": {},
   "source": [
    "```\n",
    "Spark = SparkSession.builder\n",
    "    .master()\n",
    "    .appName()\n",
    "    .config()\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "✅ A SparkSession is the entry point to programming Spark with the DataFrame and SQL API. It is the unified interface that combines the functionalities of SQLContext, HiveContext, and StreamingContext\n",
    "\n",
    "✅ builder is an attribute within SparkSession class which initiates Builder class to construct :class:`SparkSession` instances. The Builder class contains methods that allow you to set various configuration options for the SparkSession before creating it.\n",
    "\n",
    "Below are some of the methods within Builder class,\n",
    "\n",
    "❎ master(String master) :- Used to specify the cluster manager URL, determining where the Spark application will run. You can set this value to connect to a standalone cluster, YARN, Mesos, or Kubernetes, depending on your infrastructure\n",
    "\n",
    "❎ appName(String name) :- Sets a name for the application, which will be shown in the Spark web UI. If no application name is set, a randomly generated name will be used.\n",
    "\n",
    "❎ config(SparkConf conf) :- Used to set configuration options for your Spark application. It allows you to specify various Spark properties and control the behavior of your Spark job.\n",
    "\n",
    "❎ getOrCreate() :- Used to either retrieve an existing SparkSession with the specified configuration or create a new one if it doesn't exist.\n",
    "\n",
    "Edited:- GetorCreate ensure instance of Spark session remains one per spark application. In Apache Spark, it is possible to create and use multiple Spark sessions within a single application. Multiple Spark sessions within the same application are typically used in scenarios where you want to isolate different sets of Spark functionality or configurations.\n",
    "\n",
    "❗ Note:- While it's possible to have multiple Spark sessions in a single application, doing so comes with certain trade-offs and potential drawbacks. Each Spark session consumes resources, including memory and CPU. Creating multiple sessions within the same application can lead to increased resource overhead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26381ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+------+\n",
      "|employee_id|employee_name|department|salary|\n",
      "+-----------+-------------+----------+------+\n",
      "|          1|         John|        HR| 50000|\n",
      "|          2|         Jane|        IT| 60000|\n",
      "|          3|          Bob|        HR| 55000|\n",
      "|          4|        Alice|        IT| 65000|\n",
      "|          5|      Charlie|   Finance| 70000|\n",
      "|          6|         Dave|   Finance| 75000|\n",
      "|          7|          Eve|        IT| 62000|\n",
      "|          8|        Frank|        HR| 52000|\n",
      "|          9|        Grace|   Finance| 72000|\n",
      "|         10|         Hank|        IT| 61000|\n",
      "+-----------+-------------+----------+------+\n",
      "\n",
      "+----------+------+\n",
      "|department|budget|\n",
      "+----------+------+\n",
      "|        HR|200000|\n",
      "|        IT|300000|\n",
      "|   Finance|400000|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7129811895481475072/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7129811895481475072%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Question: Given two datasets - one containing information about employees and their departments \n",
    "#     (employees.csv) and another containing information about department budgets (budgets.csv), \n",
    "#     write a PySpark script to find the department with the highest average salary-to-budget ratio. \n",
    "#     The employee dataset has columns: 'employee_id', 'employee_name', 'department', and 'salary', and \n",
    "#     the budget dataset has columns: 'department', 'budget'. Assume that the datasets are large and may \n",
    "#     not fit into memory, so the solution should be scalable.\n",
    "\n",
    "\n",
    "# Emp-Data:\n",
    "col1 = [\"employee_id\",\"employee_name\",\"department\",\"salary\"]\n",
    "\n",
    "data1 = [(1,\"John\",\"HR\",50000),\n",
    "(2,'Jane',\"IT\",60000),\n",
    "(3,\"Bob\",\"HR\",55000),\n",
    "(4,'Alice',\"IT\",65000),\n",
    "(5,\"Charlie\",'Finance',70000),\n",
    "(6,'Dave',\"Finance\",75000),\n",
    "(7,\"Eve\",\"IT\",62000),\n",
    "(8,\"Frank\",\"HR\",52000),\n",
    "(9,\"Grace\",\"Finance\",72000),\n",
    "(10,\"Hank\",\"IT\",61000)]\n",
    "\n",
    "# Budgets:\n",
    "col2 = [\"department\",\"budget\"]\n",
    "\n",
    "data2 = [(\"HR\",200000),\n",
    "(\"IT\",300000),\n",
    "(\"Finance\",400000)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df1.show()\n",
    " \n",
    "df2 = spark.createDataFrame(data2, col2)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "853734b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+------+------+----------------------+\n",
      "|department|employee_id|employee_name|salary|budget|salary_to_budget_ratio|\n",
      "+----------+-----------+-------------+------+------+----------------------+\n",
      "|   Finance|          5|      Charlie| 70000|400000|                 0.175|\n",
      "|   Finance|          6|         Dave| 75000|400000|                0.1875|\n",
      "|   Finance|          9|        Grace| 72000|400000|                  0.18|\n",
      "|        HR|          1|         John| 50000|200000|                  0.25|\n",
      "|        HR|          3|          Bob| 55000|200000|                 0.275|\n",
      "|        HR|          8|        Frank| 52000|200000|                  0.26|\n",
      "|        IT|          2|         Jane| 60000|300000|                   0.2|\n",
      "|        IT|          4|        Alice| 65000|300000|   0.21666666666666667|\n",
      "|        IT|          7|          Eve| 62000|300000|   0.20666666666666667|\n",
      "|        IT|         10|         Hank| 61000|300000|   0.20333333333333334|\n",
      "+----------+-----------+-------------+------+------+----------------------+\n",
      "\n",
      "+----------+--------------------------+\n",
      "|department|avg_salary_to_budget_ratio|\n",
      "+----------+--------------------------+\n",
      "|   Finance|       0.18083333333333332|\n",
      "|        HR|       0.26166666666666666|\n",
      "|        IT|       0.20666666666666667|\n",
      "+----------+--------------------------+\n",
      "\n",
      "The department with the highest average salary-to-budget ratio is: HR\n"
     ]
    }
   ],
   "source": [
    "# Join the datasets on the 'department' column\n",
    "joined_df = df1.join(df2, 'department', \"inner\")\n",
    "\n",
    "# Calculate the salary-to-budget ratio for each employee\n",
    "joined_df = joined_df.withColumn('salary_to_budget_ratio', col('salary') / col('budget'))\n",
    "joined_df.show()\n",
    "\n",
    "# Calculate the average salary-to-budget ratio for each department\n",
    "avg_ratio_df = joined_df.groupBy('department').agg(\n",
    "    avg('salary_to_budget_ratio').alias('avg_salary_to_budget_ratio'))\n",
    "avg_ratio_df.show()\n",
    "\n",
    "# Find the department with the highest average salary-to-budget ratio\n",
    "max_ratio_department = avg_ratio_df.orderBy(desc('avg_salary_to_budget_ratio')).first()\n",
    "\n",
    "# Display the result\n",
    "print(f\"The department with the highest average salary-to-budget ratio is: {max_ratio_department['department']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d44ccdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|user_id|event_name| time|\n",
      "+-------+----------+-----+\n",
      "|      1|     apple|00:01|\n",
      "|      1|    carrot|00:03|\n",
      "|      2|     apple|00:01|\n",
      "|      3|     apple|00:06|\n",
      "|      3|    carrot|00:09|\n",
      "|      3|     grape|00:20|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7130094886023761920/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130094886023761920%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "col = [\"user_id\", \"event_name\", \"time\"]\n",
    "\n",
    "data = [(1 , 'apple' , '00:01'),\n",
    "(1 , 'carrot' , '00:03'),\n",
    "(2 , 'apple' , '00:01'),\n",
    "(3 , 'apple' , '00:06'),\n",
    "(3 , 'carrot' , '00:09'),\n",
    "(3 , 'grape' , '00:20')]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0d57836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+------------+\n",
      "|user_id|event_name| time|       event|\n",
      "+-------+----------+-----+------------+\n",
      "|      1|     apple|00:01| apple:00:01|\n",
      "|      1|    carrot|00:03|carrot:00:03|\n",
      "|      2|     apple|00:01| apple:00:01|\n",
      "|      3|     apple|00:06| apple:00:06|\n",
      "|      3|    carrot|00:09|carrot:00:09|\n",
      "|      3|     grape|00:20| grape:00:20|\n",
      "+-------+----------+-----+------------+\n",
      "\n",
      "+-------+----------------------------------------+-----------------+\n",
      "|user_id|event                                   |most_recent_event|\n",
      "+-------+----------------------------------------+-----------------+\n",
      "|1      |[apple:00:01, carrot:00:03]             |00:03            |\n",
      "|2      |[apple:00:01]                           |00:01            |\n",
      "|3      |[apple:00:06, carrot:00:09, grape:00:20]|00:20            |\n",
      "+-------+----------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"event\", concat_ws(\":\", \"event_name\", \"time\"))\n",
    "df1.show()\n",
    "\n",
    "df2 = df1.groupBy(\"user_id\").agg(collect_list(\"event\").alias(\"event\"), max(\"time\").alias(\"most_recent_event\"))\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e01d64",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7130053191857045504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130053191857045504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "### 1) How do You execute your Spark Job ?\n",
    "`Spark-submit`\n",
    "\n",
    "It has many parameters such as -\n",
    "```\n",
    ">>spark-submit \\\n",
    "        --master yarn \\             resource manager is Yarn\n",
    "        --deploy-mode cluster \\     deploying the spark application in cluster mode\n",
    "        --driver-memory 8g \\        memory to be used by driver jvm\n",
    "        --driver-cores 1 \\          cores to be used by driver jvm\n",
    "        --num-executors 5\\          The total number of executors to be used\n",
    "        --executor-memory 16g \\     Amount of memory to be used ach executors\n",
    "        --executor-cores 2 \\        Number of cores to be used by each executors\n",
    "        --conf \"spark.sql.shuffle.partitions=20000\" \\       \n",
    "        --conf \"spark.executor.memoryOverhead=5244\" \\\n",
    "        --conf \"spark.memory.fraction=0.8\" \\\n",
    "        --conf \"spark.memory.storageFraction=0.2\" \\\n",
    "        --conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" \\\n",
    "        --conf \"spark.sql.files.maxPartitionBytes=168435456\" \\\n",
    "        --conf \"spark.dynamicAllocation.minExecutors=1\" \\\n",
    "        --conf \"spark.dynamicAllocation.maxExecutors=200\" \\\n",
    "        --conf \"spark.dynamicAllocation.enabled=true\" \\\n",
    "        --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" \\ \n",
    "        --jars dependency1.jar, dependency2.jar \\\n",
    "        --class \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e399df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|Original_CC_Number|Hidden_CC_Number|\n",
      "+------------------+----------------+\n",
      "|1234567891234567  |************4567|\n",
      "+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7130826889874604034/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130826889874604034%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample input credit card number\n",
    "input_cc_number = \"1234567891234567\"\n",
    "\n",
    "# Hide all characters except the last four digits\n",
    "hidden_cc_number = \"************\" + input_cc_number[-4:]\n",
    "\n",
    "# Create a DataFrame with the hidden credit card number\n",
    "data = [(input_cc_number, hidden_cc_number)]\n",
    "df = spark.createDataFrame(data, [\"Original_CC_Number\", \"Hidden_CC_Number\"])\n",
    "\n",
    "# Display the result\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fe6c605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+\n",
      "| input_cc_number|          hidden|\n",
      "+----------------+----------------+\n",
      "|1234567891234567|************4567|\n",
      "+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(input_cc_number,)], [\"input_cc_number\"])\n",
    "df2 = df1.withColumn(\"hidden\", expr(\"concat('************',substring(input_cc_number, -4, 4))\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06bcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cbcebff",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7131430811504963585/\n",
    "\n",
    "**Question 26: What are some best practices for tuning Spark applications for optimal performance?**\n",
    "\n",
    "Tuning Spark applications for optimal performance is essential to make the most of your cluster's resources. Here are some best practices for achieving this and an example code sample for each practice:\n",
    "\n",
    "🔹Memory Management : \n",
    "Optimize memory allocation: Set the appropriate memory configurations, such as `spark.driver.memory, spark.executor.memory, and spark.memory.fraction`, to efficiently use available resources.\n",
    "\n",
    "```\n",
    "    from pyspark.sql import SparkSession\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "     .appName(\"MemoryManagementExample\") \\\n",
    "     .config(\"spark.driver.memory\", \"2g\") \\\n",
    "     .config(\"spark.executor.memory\", \"4g\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "🔹Data Serialization:\n",
    "Choose efficient serialization: Use a more efficient serialization format like Avro or Parquet, \n",
    "which can reduce the amount of data transfer and storage.\n",
    "\n",
    "    spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    spark.conf.set(\"spark.kryo.registrator\", \"your.package.YourKryoRegistrator\")\n",
    "\n",
    "🔹Data Partitioning:\n",
    "Optimize data partitioning: Repartition your data using repartition or coalesce to balance data distribution and avoid data skew.\n",
    "\n",
    "    df = df.repartition(100)\n",
    "\n",
    "🔹Caching and Persistence:\n",
    "Cache and persist data: Use cache() or persist() to store frequently used data in memory for faster access.\n",
    "\n",
    "    df = df.cache()\n",
    "\n",
    "🔹Shuffling Optimization:\n",
    "Minimize shuffling: Avoid unnecessary shuffling by using operations like reduceByKey instead of groupByKey.\n",
    "\n",
    "    rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "🔹Broadcasting:\n",
    "Broadcast small data: Broadcast small DataFrames to all nodes to avoid unnecessary data transfer.\n",
    "\n",
    "    broadcast_small_df = spark.sparkContext.broadcast(small_df)\n",
    "\n",
    "🔹Optimized Joins:\n",
    "Use optimized join strategies: Utilize broadcast join, bucketed join, or sort-merge join based on your data distribution.\n",
    "\n",
    "    joined_df = large_df.join(broadcast(small_df), \"id\", \"inner\")\n",
    "\n",
    "🔹Dynamic Resource Allocation:\n",
    "Enable dynamic allocation: Allow Spark to allocate and release resources dynamically based on the workload.\n",
    "    spark.conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888c5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| id|salary|salary|\n",
      "+---+------+------+\n",
      "|  1|    20|    25|\n",
      "|  2|    30|    35|\n",
      "+---+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131150748666523648/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131150748666523648%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "col = [\"id\",\"salary\",\"salary\"]\n",
    "\n",
    "data = [(1,20,25), (2, 30, 35)]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad090b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(id=1, salary=20, salary=25), 0), (Row(id=2, salary=30, salary=35), 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.rdd.zipWithIndex().collect()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28f9a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Row(id=1, salary=20, salary=25), 0)\n",
      "(Row(id=2, salary=30, salary=35), 1)\n"
     ]
    }
   ],
   "source": [
    "df3 = df.rdd.zipWithIndex().map(lambda x : x)\n",
    "# should try this\n",
    "df3.foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47eae6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6edd1c8",
   "metadata": {},
   "source": [
    "```\n",
    "  println(\"\\n Data with Duplicate Column name\")\n",
    "  df.show()\n",
    "   \n",
    "  println(\"\\n withColumnRenamed will rename both columns -- doesn't serve our purpose\")\n",
    "  val withrename = df.withColumnRenamed(\"salary\", \"new_salary\")\n",
    "  withrename.show()\n",
    "\n",
    "  println(\"\\n What zipWithIndex actually does?\")\n",
    "  val sample = df.columns.zipWithIndex.foreach(println)\n",
    "   \n",
    "  println(\"\\n Can use zipWithIndex to resolve this issue\")\n",
    "  val renameCol = df.columns.zipWithIndex .map { \n",
    "   case (\"salary\", 1) => \"old_salary\" \n",
    "   case (\"salary\", 2) => \"new_salary\" \n",
    "   case (col, _) => col\n",
    "  }\n",
    "\n",
    "  val renamedDF = df.toDF(renameCol: _*)\n",
    "  renamedDF.show()     \n",
    "```\n",
    "<img src=\"https://media.licdn.com/dms/image/D5622AQFPaqJ3Vdw-wg/feedshare-shrink_800/0/1700198827008?e=1718841600&v=beta&t=T5x6O31NLcLSeQafU7K7JfoZncHa7dm32MRujkTMSpo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff7af7",
   "metadata": {},
   "source": [
    "# fromhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f4178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/16 15:09:23 WARN Utils: Your hostname, DINESHs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.100 instead (on interface en0)\n",
      "24/05/16 15:09:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/16 15:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported All\n",
      "/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Datasets/\n"
     ]
    }
   ],
   "source": [
    "# Normal Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import col,struct,when, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "print(\"imported\")\n",
    "# Agg\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set, sum, avg, max, countDistinct, count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance, var_samp, var_pop\n",
    "\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.types import MapType, StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "print(\"imported\")\n",
    "# spark context\n",
    "conf = SparkConf().setAppName(\"Spark\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Usage of config()\n",
    "# spark = SparkSession.builder \\\n",
    "#       .master(\"local[1]\") \\\n",
    "#       .appName(\"LearnSpark\") \\\n",
    "#       .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "#       .getOrCreate()\n",
    "\n",
    "print(\"imported All\")\n",
    "\n",
    "Location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Datasets/\"\n",
    "print(Location)\n",
    "\n",
    "image_location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Notebook images\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f7ade9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+-------------+------------+\n",
      "|policyholder_id|claim_id|claim_date|coverage_type|claim_amount|\n",
      "+---------------+--------+----------+-------------+------------+\n",
      "|              1|       1|2023-01-01|   Coverage_A|        5000|\n",
      "|              2|       2|2023-01-05|   Coverage_B|        7000|\n",
      "|              1|       3|2023-02-10|   Coverage_A|        3000|\n",
      "|              3|       4|2023-02-15|   Coverage_C|        4500|\n",
      "|              2|       5|2023-03-03|   Coverage_B|        6000|\n",
      "|              1|       6|2023-03-20|   Coverage_A|        8000|\n",
      "|              1|       7|2023-04-02|   Coverage_A|        5500|\n",
      "|              3|       8|2023-04-10|   Coverage_C|        7000|\n",
      "|              2|       9|2023-05-05|   Coverage_B|        3500|\n",
      "|              1|      10|2023-05-15|   Coverage_A|        9000|\n",
      "|              3|      11|2023-06-01|   Coverage_C|        4200|\n",
      "|              2|      12|2023-06-10|   Coverage_B|        5800|\n",
      "|              1|      13|2023-07-05|   Coverage_A|        7500|\n",
      "|              2|      14|2023-07-20|   Coverage_B|        6200|\n",
      "|              3|      15|2023-08-02|   Coverage_C|        4800|\n",
      "|              1|      16|2023-08-10|   Coverage_A|        5500|\n",
      "|              2|      17|2023-09-01|   Coverage_B|        7000|\n",
      "|              1|      18|2023-09-15|   Coverage_A|        8000|\n",
      "|              3|      19|2023-10-02|   Coverage_C|        4000|\n",
      "|              2|      20|2023-10-10|   Coverage_B|        6500|\n",
      "+---------------+--------+----------+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7130904493835120640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130904493835120640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "col = [\"policyholder_id\",\"claim_id\",\"claim_date\",\"coverage_type\",\"claim_amount\"]\n",
    "\n",
    "data = [(1,1,\"2023-01-01\",\"Coverage_A\",5000),\n",
    "(2,2,\"2023-01-05\",\"Coverage_B\",7000),\n",
    "(1,3,\"2023-02-10\",\"Coverage_A\",3000),\n",
    "(3,4,\"2023-02-15\",\"Coverage_C\",4500),\n",
    "(2,5,\"2023-03-03\",\"Coverage_B\",6000),\n",
    "(1,6,\"2023-03-20\",\"Coverage_A\",8000),\n",
    "(1,7,\"2023-04-02\",\"Coverage_A\",5500),\n",
    "(3,8,\"2023-04-10\",\"Coverage_C\",7000),\n",
    "(2,9,\"2023-05-05\",\"Coverage_B\",3500),\n",
    "(1,10,\"2023-05-15\",\"Coverage_A\",9000),\n",
    "(3,11,\"2023-06-01\",\"Coverage_C\",4200),\n",
    "(2,12,\"2023-06-10\",\"Coverage_B\",5800),\n",
    "(1,13,\"2023-07-05\",\"Coverage_A\",7500),\n",
    "(2,14,\"2023-07-20\",\"Coverage_B\",6200),\n",
    "(3,15,\"2023-08-02\",\"Coverage_C\",4800),\n",
    "(1,16,\"2023-08-10\",\"Coverage_A\",5500),\n",
    "(2,17,\"2023-09-01\",\"Coverage_B\",7000),\n",
    "(1,18,\"2023-09-15\",\"Coverage_A\",8000),\n",
    "(3,19,\"2023-10-02\",\"Coverage_C\",4000),\n",
    "(2,20,\"2023-10-10\",\"Coverage_B\",6500)]\n",
    "\n",
    "df = spark.createDataFrame(data, col)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c02668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task:\n",
    "# 1. Calculate the total payout for each claim, considering the claim_amount. \n",
    "#         Create a new column named total_payout in the DataFrame.\n",
    "# 2. Calculate the cumulative payout for each policyholder. \n",
    "#         Create a new column named cumulative_payout for each policyholder, \n",
    "#         where cumulative payout is the sum of the total_payout for all claims made by the policyholder.\n",
    "# 3. Identify the most frequently claimed type of coverage for each month. \n",
    "#         Create a new column named top_coverage_monthly that contains the coverage type with \n",
    "#         the highest total number of claims in each month.\n",
    "# 4. Calculate the average claim amount for each policyholder.\n",
    "# 5. Identify the policyholders with the highest lifetime value (CLV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f8e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+-------------+------------+\n",
      "|policyholder_id|claim_id|claim_date|coverage_type|claim_amount|\n",
      "+---------------+--------+----------+-------------+------------+\n",
      "|              1|       1|2023-01-01|   Coverage_A|        5000|\n",
      "|              2|       2|2023-01-05|   Coverage_B|        7000|\n",
      "|              1|       3|2023-02-10|   Coverage_A|        3000|\n",
      "|              3|       4|2023-02-15|   Coverage_C|        4500|\n",
      "|              2|       5|2023-03-03|   Coverage_B|        6000|\n",
      "|              1|       6|2023-03-20|   Coverage_A|        8000|\n",
      "|              1|       7|2023-04-02|   Coverage_A|        5500|\n",
      "|              3|       8|2023-04-10|   Coverage_C|        7000|\n",
      "|              2|       9|2023-05-05|   Coverage_B|        3500|\n",
      "|              1|      10|2023-05-15|   Coverage_A|        9000|\n",
      "|              3|      11|2023-06-01|   Coverage_C|        4200|\n",
      "|              2|      12|2023-06-10|   Coverage_B|        5800|\n",
      "|              1|      13|2023-07-05|   Coverage_A|        7500|\n",
      "|              2|      14|2023-07-20|   Coverage_B|        6200|\n",
      "|              3|      15|2023-08-02|   Coverage_C|        4800|\n",
      "|              1|      16|2023-08-10|   Coverage_A|        5500|\n",
      "|              2|      17|2023-09-01|   Coverage_B|        7000|\n",
      "|              1|      18|2023-09-15|   Coverage_A|        8000|\n",
      "|              3|      19|2023-10-02|   Coverage_C|        4000|\n",
      "|              2|      20|2023-10-10|   Coverage_B|        6500|\n",
      "+---------------+--------+----------+-------------+------------+\n",
      "\n",
      "root\n",
      " |-- policyholder_id: long (nullable = true)\n",
      " |-- claim_id: long (nullable = true)\n",
      " |-- claim_date: date (nullable = true)\n",
      " |-- coverage_type: string (nullable = true)\n",
      " |-- claim_amount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"claim_date\", to_date(\"claim_date\"))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ae2a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+-------------+------------+------------+\n",
      "|policyholder_id|claim_id|claim_date|coverage_type|claim_amount|total_payout|\n",
      "+---------------+--------+----------+-------------+------------+------------+\n",
      "|              1|       1|2023-01-01|   Coverage_A|        5000|        5000|\n",
      "|              2|       2|2023-01-05|   Coverage_B|        7000|        7000|\n",
      "|              1|       3|2023-02-10|   Coverage_A|        3000|        3000|\n",
      "|              3|       4|2023-02-15|   Coverage_C|        4500|        4500|\n",
      "|              2|       5|2023-03-03|   Coverage_B|        6000|        6000|\n",
      "|              1|       6|2023-03-20|   Coverage_A|        8000|        8000|\n",
      "|              1|       7|2023-04-02|   Coverage_A|        5500|        5500|\n",
      "|              3|       8|2023-04-10|   Coverage_C|        7000|        7000|\n",
      "|              2|       9|2023-05-05|   Coverage_B|        3500|        3500|\n",
      "|              1|      10|2023-05-15|   Coverage_A|        9000|        9000|\n",
      "|              3|      11|2023-06-01|   Coverage_C|        4200|        4200|\n",
      "|              2|      12|2023-06-10|   Coverage_B|        5800|        5800|\n",
      "|              1|      13|2023-07-05|   Coverage_A|        7500|        7500|\n",
      "|              2|      14|2023-07-20|   Coverage_B|        6200|        6200|\n",
      "|              3|      15|2023-08-02|   Coverage_C|        4800|        4800|\n",
      "|              1|      16|2023-08-10|   Coverage_A|        5500|        5500|\n",
      "|              2|      17|2023-09-01|   Coverage_B|        7000|        7000|\n",
      "|              1|      18|2023-09-15|   Coverage_A|        8000|        8000|\n",
      "|              3|      19|2023-10-02|   Coverage_C|        4000|        4000|\n",
      "|              2|      20|2023-10-10|   Coverage_B|        6500|        6500|\n",
      "+---------------+--------+----------+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate the total payout for each claim, considering the claim_amount. \n",
    "#         Create a new column named total_payout in the DataFrame.\n",
    "\n",
    "# df1 = df.withColumn(\"total_payout\", col('claim_amount'))\n",
    "df1 = df.withColumn(\"total_payout\", expr('claim_amount'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea76a1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+-------------+------------+------------+-----------------+\n",
      "|policyholder_id|claim_id|claim_date|coverage_type|claim_amount|total_payout|cumulative_payout|\n",
      "+---------------+--------+----------+-------------+------------+------------+-----------------+\n",
      "|              1|       1|2023-01-01|   Coverage_A|        5000|        5000|             5000|\n",
      "|              1|       3|2023-02-10|   Coverage_A|        3000|        3000|             8000|\n",
      "|              1|       6|2023-03-20|   Coverage_A|        8000|        8000|            16000|\n",
      "|              1|       7|2023-04-02|   Coverage_A|        5500|        5500|            21500|\n",
      "|              1|      10|2023-05-15|   Coverage_A|        9000|        9000|            30500|\n",
      "|              1|      13|2023-07-05|   Coverage_A|        7500|        7500|            38000|\n",
      "|              1|      16|2023-08-10|   Coverage_A|        5500|        5500|            43500|\n",
      "|              1|      18|2023-09-15|   Coverage_A|        8000|        8000|            51500|\n",
      "|              2|       2|2023-01-05|   Coverage_B|        7000|        7000|             7000|\n",
      "|              2|       5|2023-03-03|   Coverage_B|        6000|        6000|            13000|\n",
      "|              2|       9|2023-05-05|   Coverage_B|        3500|        3500|            16500|\n",
      "|              2|      12|2023-06-10|   Coverage_B|        5800|        5800|            22300|\n",
      "|              2|      14|2023-07-20|   Coverage_B|        6200|        6200|            28500|\n",
      "|              2|      17|2023-09-01|   Coverage_B|        7000|        7000|            35500|\n",
      "|              2|      20|2023-10-10|   Coverage_B|        6500|        6500|            42000|\n",
      "|              3|       4|2023-02-15|   Coverage_C|        4500|        4500|             4500|\n",
      "|              3|       8|2023-04-10|   Coverage_C|        7000|        7000|            11500|\n",
      "|              3|      11|2023-06-01|   Coverage_C|        4200|        4200|            15700|\n",
      "|              3|      15|2023-08-02|   Coverage_C|        4800|        4800|            20500|\n",
      "|              3|      19|2023-10-02|   Coverage_C|        4000|        4000|            24500|\n",
      "+---------------+--------+----------+-------------+------------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Calculate the cumulative payout for each policyholder. \n",
    "#         Create a new column named cumulative_payout for each policyholder, \n",
    "#         where cumulative payout is the sum of the total_payout for all claims made by the policyholder.\n",
    "\n",
    "window = Window.partitionBy(\"policyholder_id\").orderBy(\"claim_date\")\n",
    "df2 = df1.withColumn(\"cumulative_payout\", sum(\"total_payout\").over(window))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248a2748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------------+-----------+\n",
      "|year|month|coverage_type|claim_count|\n",
      "+----+-----+-------------+-----------+\n",
      "|2023|    1|   Coverage_A|          1|\n",
      "|2023|    1|   Coverage_B|          1|\n",
      "|2023|    2|   Coverage_A|          1|\n",
      "|2023|    2|   Coverage_C|          1|\n",
      "|2023|    3|   Coverage_B|          1|\n",
      "|2023|    3|   Coverage_A|          1|\n",
      "|2023|    5|   Coverage_B|          1|\n",
      "|2023|    4|   Coverage_A|          1|\n",
      "|2023|    4|   Coverage_C|          1|\n",
      "|2023|    5|   Coverage_A|          1|\n",
      "|2023|    6|   Coverage_C|          1|\n",
      "|2023|    6|   Coverage_B|          1|\n",
      "|2023|    7|   Coverage_B|          1|\n",
      "|2023|    7|   Coverage_A|          1|\n",
      "|2023|    8|   Coverage_A|          1|\n",
      "|2023|    8|   Coverage_C|          1|\n",
      "|2023|    9|   Coverage_B|          1|\n",
      "|2023|   10|   Coverage_B|          1|\n",
      "|2023|   10|   Coverage_C|          1|\n",
      "|2023|    9|   Coverage_A|          1|\n",
      "+----+-----+-------------+-----------+\n",
      "\n",
      "+----+-----+-------------+-----------+----+\n",
      "|year|month|coverage_type|claim_count|rank|\n",
      "+----+-----+-------------+-----------+----+\n",
      "|2023|    1|   Coverage_A|          1|   1|\n",
      "|2023|    1|   Coverage_B|          1|   1|\n",
      "|2023|    2|   Coverage_A|          1|   1|\n",
      "|2023|    2|   Coverage_C|          1|   1|\n",
      "|2023|    3|   Coverage_B|          1|   1|\n",
      "|2023|    3|   Coverage_A|          1|   1|\n",
      "|2023|    4|   Coverage_A|          1|   1|\n",
      "|2023|    4|   Coverage_C|          1|   1|\n",
      "|2023|    5|   Coverage_B|          1|   1|\n",
      "|2023|    5|   Coverage_A|          1|   1|\n",
      "|2023|    6|   Coverage_C|          1|   1|\n",
      "|2023|    6|   Coverage_B|          1|   1|\n",
      "|2023|    7|   Coverage_B|          1|   1|\n",
      "|2023|    7|   Coverage_A|          1|   1|\n",
      "|2023|    8|   Coverage_A|          1|   1|\n",
      "|2023|    8|   Coverage_C|          1|   1|\n",
      "|2023|    9|   Coverage_B|          1|   1|\n",
      "|2023|    9|   Coverage_A|          1|   1|\n",
      "|2023|   10|   Coverage_B|          1|   1|\n",
      "|2023|   10|   Coverage_C|          1|   1|\n",
      "+----+-----+-------------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Identify the most frequently claimed type of coverage for each month. \n",
    "#         Create a new column named top_coverage_monthly that contains the coverage type with \n",
    "#         the highest total number of claims in each month.\n",
    "monthly_coverage_df = df2.groupBy(year('claim_date').alias('year'), \\\n",
    "            month('claim_date').alias('month'), 'coverage_type').agg(count('claim_id').alias('claim_count'))\n",
    "monthly_coverage_df.show()\n",
    "\n",
    "window_spec = Window.partitionBy('year', 'month').orderBy(desc('claim_count'))\n",
    "top_coverage_df = monthly_coverage_df.withColumn('rank', rank().over(window_spec))\n",
    "top_coverage_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386bb7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|year|month|top_coverage_monthly|\n",
      "+----+-----+--------------------+\n",
      "|2023|    1|          Coverage_A|\n",
      "|2023|    1|          Coverage_B|\n",
      "|2023|    2|          Coverage_A|\n",
      "|2023|    2|          Coverage_C|\n",
      "|2023|    3|          Coverage_B|\n",
      "|2023|    3|          Coverage_A|\n",
      "|2023|    4|          Coverage_A|\n",
      "|2023|    4|          Coverage_C|\n",
      "|2023|    5|          Coverage_B|\n",
      "|2023|    5|          Coverage_A|\n",
      "|2023|    6|          Coverage_C|\n",
      "|2023|    6|          Coverage_B|\n",
      "|2023|    7|          Coverage_B|\n",
      "|2023|    7|          Coverage_A|\n",
      "|2023|    8|          Coverage_A|\n",
      "|2023|    8|          Coverage_C|\n",
      "|2023|    9|          Coverage_B|\n",
      "|2023|    9|          Coverage_A|\n",
      "|2023|   10|          Coverage_B|\n",
      "|2023|   10|          Coverage_C|\n",
      "+----+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_coverage_df_final = top_coverage_df.filter(top_coverage_df.rank == 1)\\\n",
    "                .select('year', 'month', 'coverage_type')\\\n",
    "                .withColumnRenamed('coverage_type', 'top_coverage_monthly')\n",
    "\n",
    "top_coverage_df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e3d912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+-------------+------------+------------+--------------------+\n",
      "|policyholder_id|claim_id|claim_date|coverage_type|claim_amount|total_payout|average_claim_amount|\n",
      "+---------------+--------+----------+-------------+------------+------------+--------------------+\n",
      "|              1|       1|2023-01-01|   Coverage_A|        5000|        5000|              6437.5|\n",
      "|              1|       3|2023-02-10|   Coverage_A|        3000|        3000|              6437.5|\n",
      "|              1|       6|2023-03-20|   Coverage_A|        8000|        8000|              6437.5|\n",
      "|              1|       7|2023-04-02|   Coverage_A|        5500|        5500|              6437.5|\n",
      "|              1|      10|2023-05-15|   Coverage_A|        9000|        9000|              6437.5|\n",
      "|              1|      13|2023-07-05|   Coverage_A|        7500|        7500|              6437.5|\n",
      "|              1|      16|2023-08-10|   Coverage_A|        5500|        5500|              6437.5|\n",
      "|              1|      18|2023-09-15|   Coverage_A|        8000|        8000|              6437.5|\n",
      "|              2|       2|2023-01-05|   Coverage_B|        7000|        7000|              6000.0|\n",
      "|              2|       5|2023-03-03|   Coverage_B|        6000|        6000|              6000.0|\n",
      "|              2|       9|2023-05-05|   Coverage_B|        3500|        3500|              6000.0|\n",
      "|              2|      12|2023-06-10|   Coverage_B|        5800|        5800|              6000.0|\n",
      "|              2|      14|2023-07-20|   Coverage_B|        6200|        6200|              6000.0|\n",
      "|              2|      17|2023-09-01|   Coverage_B|        7000|        7000|              6000.0|\n",
      "|              2|      20|2023-10-10|   Coverage_B|        6500|        6500|              6000.0|\n",
      "|              3|       4|2023-02-15|   Coverage_C|        4500|        4500|              4900.0|\n",
      "|              3|       8|2023-04-10|   Coverage_C|        7000|        7000|              4900.0|\n",
      "|              3|      11|2023-06-01|   Coverage_C|        4200|        4200|              4900.0|\n",
      "|              3|      15|2023-08-02|   Coverage_C|        4800|        4800|              4900.0|\n",
      "|              3|      19|2023-10-02|   Coverage_C|        4000|        4000|              4900.0|\n",
      "+---------------+--------+----------+-------------+------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Calculate the average claim amount for each policyholder.\n",
    "\n",
    "average_claim_amount_window = Window.partitionBy('policyholder_id')\n",
    "insurance_df = df1.withColumn('average_claim_amount', \\\n",
    "                        avg('total_payout').over(average_claim_amount_window))\n",
    "insurance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab440c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+----------+-------------+------------+------------+-----+\n",
      "|policyholder_id|claim_id|claim_date|coverage_type|claim_amount|total_payout|  clv|\n",
      "+---------------+--------+----------+-------------+------------+------------+-----+\n",
      "|              1|       1|2023-01-01|   Coverage_A|        5000|        5000| 5000|\n",
      "|              1|       3|2023-02-10|   Coverage_A|        3000|        3000| 8000|\n",
      "|              1|       6|2023-03-20|   Coverage_A|        8000|        8000|16000|\n",
      "|              1|       7|2023-04-02|   Coverage_A|        5500|        5500|21500|\n",
      "|              1|      10|2023-05-15|   Coverage_A|        9000|        9000|30500|\n",
      "|              1|      13|2023-07-05|   Coverage_A|        7500|        7500|38000|\n",
      "|              1|      16|2023-08-10|   Coverage_A|        5500|        5500|43500|\n",
      "|              1|      18|2023-09-15|   Coverage_A|        8000|        8000|51500|\n",
      "|              2|       2|2023-01-05|   Coverage_B|        7000|        7000| 7000|\n",
      "|              2|       5|2023-03-03|   Coverage_B|        6000|        6000|13000|\n",
      "|              2|       9|2023-05-05|   Coverage_B|        3500|        3500|16500|\n",
      "|              2|      12|2023-06-10|   Coverage_B|        5800|        5800|22300|\n",
      "|              2|      14|2023-07-20|   Coverage_B|        6200|        6200|28500|\n",
      "|              2|      17|2023-09-01|   Coverage_B|        7000|        7000|35500|\n",
      "|              2|      20|2023-10-10|   Coverage_B|        6500|        6500|42000|\n",
      "|              3|       4|2023-02-15|   Coverage_C|        4500|        4500| 4500|\n",
      "|              3|       8|2023-04-10|   Coverage_C|        7000|        7000|11500|\n",
      "|              3|      11|2023-06-01|   Coverage_C|        4200|        4200|15700|\n",
      "|              3|      15|2023-08-02|   Coverage_C|        4800|        4800|20500|\n",
      "|              3|      19|2023-10-02|   Coverage_C|        4000|        4000|24500|\n",
      "+---------------+--------+----------+-------------+------------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Identify the policyholders with the highest lifetime value (CLV)\n",
    "clv_window = Window.partitionBy('policyholder_id').orderBy('claim_date')\n",
    "insurance_df = df1.withColumn('clv', sum('total_payout').over(clv_window))\n",
    "insurance_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7e7d12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col1|col2|col3|\n",
      "+----+----+----+\n",
      "| 100|   2|   1|\n",
      "| 200|   3|   4|\n",
      "| 300|   4|   4|\n",
      "+----+----+----+\n",
      "\n",
      "+-------------+\n",
      "|(col2 > col3)|\n",
      "+-------------+\n",
      "|         true|\n",
      "|        false|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 < col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|         true|\n",
      "|        false|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col2 = col3)|\n",
      "+-------------+\n",
      "|        false|\n",
      "|        false|\n",
      "|         true|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131849051863400448/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131849051863400448%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data=[(100,2,1),(200,3,4),(300,4,4)]\n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\",\"col3\") \n",
    "df.show()\n",
    "#Arthmetic operations\n",
    "\n",
    "df.select(df.col2 > df.col3).show()\n",
    "df.select(df.col2 < df.col3).show()\n",
    "df.select(df.col2 == df.col3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fb346e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          102|\n",
      "|          203|\n",
      "|          304|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 + col2)|\n",
      "+-------------+\n",
      "|          102|\n",
      "|          203|\n",
      "|          304|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 * col2)|\n",
      "+-------------+\n",
      "|          200|\n",
      "|          600|\n",
      "|         1200|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 + df.col2).show()\n",
    "df.select(df.col1 * df.col2).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5aad36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    (col1 / col2)|\n",
      "+-----------------+\n",
      "|             50.0|\n",
      "|66.66666666666667|\n",
      "|             75.0|\n",
      "+-----------------+\n",
      "\n",
      "+-------------+\n",
      "|(col1 % col2)|\n",
      "+-------------+\n",
      "|            0|\n",
      "|            2|\n",
      "|            0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.col1 / df.col2).show()\n",
    "df.select(df.col1 % df.col2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b431730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All substrings of 'hello' are:\n",
      "['h', 'he', 'hel', 'hell', 'hello', 'e', 'el', 'ell', 'ello', 'l', 'll', 'llo', 'l', 'lo', 'o']\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131286673409114112/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131286673409114112%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "#Finding_all_substrings_of_a_given_string_involves_extracting_every \n",
    "#possible_contiguous_sequence_of_characters_within_that_string.\n",
    "\n",
    "def find_substrings(input_string):\n",
    "     substrings = [] # Initialize an empty list to store substrings\n",
    "     n = len(input_string) # Get the length of the input string\n",
    "\n",
    "     # Outer loop for the starting index of substring\n",
    "     for i in range(n):\n",
    "         # Inner loop for the ending index of substring\n",
    "         for j in range(i + 1, n + 1):\n",
    "         # Extract substring from index i to j-1\n",
    "             substring = input_string[i:j]\n",
    "             substrings.append(substring) # Add the extracted substring to the list\n",
    "\n",
    "     return substrings # Return the list containing all substrings\n",
    "\n",
    "# Example usage\n",
    "input_str = \"hello\"\n",
    "result = find_substrings(input_str)\n",
    "print(\"All substrings of '{}' are:\".format(input_str))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bca328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|customer|product_model|\n",
      "+--------+-------------+\n",
      "|       1|            A|\n",
      "|       1|            B|\n",
      "|       2|            A|\n",
      "|       2|            B|\n",
      "|       3|            A|\n",
      "|       3|            B|\n",
      "|       1|            C|\n",
      "|       1|            D|\n",
      "|       1|            E|\n",
      "|       3|            E|\n",
      "|       4|            A|\n",
      "+--------+-------------+\n",
      "\n",
      "+-------------+\n",
      "|product_model|\n",
      "+-------------+\n",
      "|            A|\n",
      "|            B|\n",
      "|            C|\n",
      "|            D|\n",
      "|            E|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131589365658517504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131589365658517504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "#Suppose_you_are_having_some_purchase_data_and_product_data_from_that_solve_the_following_3_questions.\n",
    "\n",
    "# 🔹Find customers who have bought only product A.\n",
    "# 🔹Find customers who upgraded from product B to product E (they might have bought other products as well).\n",
    "# 🔹Find customers who have bought all the models in the new Product Data.\n",
    "\n",
    "purchase_data = spark.createDataFrame([\n",
    " (1, \"A\"), (1, \"B\"),\n",
    " (2, \"A\"), (2, \"B\"),\n",
    " (3, \"A\"), (3, \"B\"),\n",
    " (1, \"C\"), (1, \"D\"),\n",
    " (1, \"E\"), (3, \"E\"),\n",
    " (4, \"A\")], [\"customer\", \"product_model\"])\n",
    "\n",
    "product_data = spark.createDataFrame([\n",
    " (\"A\",), (\"B\",), (\"C\",), (\"D\",), (\"E\",)\n",
    "], [\"product_model\"])\n",
    "\n",
    "purchase_data.show()\n",
    "product_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "497d187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|customer|count|\n",
      "+--------+-----+\n",
      "|       4|    1|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+\n",
      "|customer|\n",
      "+--------+\n",
      "|       4|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find customers who have bought only product A\n",
    "\n",
    "pr1 = purchase_data.groupBy(\"customer\").agg(countDistinct(\"product_model\").alias(\"count\"))\n",
    "pr1 = pr1.filter(pr1[\"count\"] == 1)\n",
    "pr1.show()\n",
    "# pr1.printSchema()\n",
    "\n",
    "purchase_data.createOrReplaceTempView(\"purchase_data\")\n",
    "product_data.createOrReplaceTempView(\"product_data\")\n",
    "\n",
    "only_product_A_customers = spark.sql(\"\"\"\n",
    " SELECT customer\n",
    " FROM purchase_data\n",
    " GROUP BY customer\n",
    " HAVING COUNT(DISTINCT product_model) = 1 \n",
    "\"\"\")\n",
    "only_product_A_customers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad4be58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|customer|\n",
      "+--------+\n",
      "|       1|\n",
      "|       3|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|customer|\n",
      "+--------+\n",
      "|       1|\n",
      "|       3|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find customers who upgraded from product B to product E\n",
    "p1 = purchase_data.alias('p1').withColumnRenamed(\"product_model\", \"product_model1\")\n",
    "p2 = purchase_data.alias('p2').withColumnRenamed(\"product_model\", \"product_model2\")\n",
    "\n",
    "joinedData = p1.join(p2, p1.customer == p2.customer, 'inner')\\\n",
    "                .filter(((p1[\"product_model1\"] == 'E') & (p2[\"product_model2\"] == 'B')))\\\n",
    "                .select(\"p1.customer\")\n",
    "\n",
    "joinedData.show()\n",
    "\n",
    "upgrade_customers = spark.sql(\"\"\"\n",
    " SELECT DISTINCT p1.customer\n",
    " FROM purchase_data p1\n",
    " JOIN purchase_data p2 ON p1.customer = p2.customer\n",
    " WHERE p1.product_model = 'E' AND p2.product_model = 'B'\n",
    "\"\"\")\n",
    "\n",
    "upgrade_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af93305a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|customer|tCount|\n",
      "+--------+------+\n",
      "|       1|     5|\n",
      "|       3|     3|\n",
      "|       2|     2|\n",
      "|       4|     1|\n",
      "+--------+------+\n",
      "\n",
      "+--------+\n",
      "|customer|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|customer|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find customers who have bought all models in the new Product Data\n",
    "\n",
    "allmodels = purchase_data.groupBy(\"customer\").agg(countDistinct(\"product_model\").alias(\"tCount\"))\n",
    "allmodels.show()\n",
    "\n",
    "total_p = product_data.count()\n",
    "allModel = allmodels.filter(allmodels[\"tCount\"] == total_p).select(\"customer\")\n",
    "allModel.show()\n",
    "\n",
    "all_models = spark.sql(\"\"\"\n",
    " SELECT customer\n",
    " FROM purchase_data\n",
    " GROUP BY customer\n",
    " HAVING COUNT(DISTINCT product_model) = (SELECT COUNT(*) FROM product_data)\n",
    "\"\"\")\n",
    "\n",
    "all_models.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7454c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+---------------+\n",
      "|claim_id|policy_id|claim_amount|approval_status|\n",
      "+--------+---------+------------+---------------+\n",
      "|       1|        1|     20000.0|       Approved|\n",
      "|       2|        2|     25000.0|       Approved|\n",
      "|       3|        3|     18000.0|       Approved|\n",
      "|       4|        4|     30000.0|         Denied|\n",
      "|       5|        5|     22000.0|       Approved|\n",
      "|       6|        6|     19000.0|       Approved|\n",
      "|       7|        7|     28000.0|         Denied|\n",
      "|       8|        8|     33000.0|       Approved|\n",
      "|       9|        9|     21000.0|       Approved|\n",
      "|      10|       10|     26000.0|         Denied|\n",
      "+--------+---------+------------+---------------+\n",
      "\n",
      "+---------+-----------+--------------+---------------+\n",
      "|policy_id|customer_id|premium_amount|coverage_amount|\n",
      "+---------+-----------+--------------+---------------+\n",
      "|        1|        101|         500.0|       100000.0|\n",
      "|        2|        102|         800.0|       150000.0|\n",
      "|        3|        103|         600.0|       120000.0|\n",
      "|        4|        104|         700.0|       130000.0|\n",
      "|        5|        105|         900.0|       180000.0|\n",
      "|        6|        106|         550.0|       110000.0|\n",
      "|        7|        107|         750.0|       140000.0|\n",
      "|        8|        108|         950.0|       200000.0|\n",
      "|        9|        109|         620.0|       125000.0|\n",
      "|       10|        110|         850.0|       160000.0|\n",
      "+---------+-----------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131308221222293505/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131308221222293505%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Consider two PySpark DataFrames - 'policy_data' and 'claim_data.' \n",
    "# The 'policy_data' DataFrame has columns: 'policy_id,' 'customer_id,' 'premium_amount,' and 'coverage_amount.' \n",
    "# The 'claim_data' DataFrame has columns: 'claim_id,' 'policy_id,' 'claim_amount,' and 'approval_status.' \n",
    "# Write a PySpark script to analyze these datasets and find the average claim amount per policy for each customer. \n",
    "# Display the customer with the highest average claim amount.\n",
    "\n",
    "# claims_data:\n",
    "schema1 = [\"claim_id\",\"policy_id\",'claim_amount','approval_status']\n",
    "data1 = [(1,1,20000.0,'Approved'),\n",
    "(2,2,25000.0,'Approved'),\n",
    "(3,3,18000.0,'Approved'),\n",
    "(4,4,30000.0,'Denied'),\n",
    "(5,5,22000.0,'Approved'),\n",
    "(6,6,19000.0,'Approved'),\n",
    "(7,7,28000.0,'Denied'),\n",
    "(8,8,33000.0,'Approved'),\n",
    "(9,9,21000.0,'Approved'),\n",
    "(10,10,26000.0,'Denied')]\n",
    "\n",
    "claims_df = spark.createDataFrame(data1, schema1)\n",
    "claims_df.show()\n",
    "\n",
    "# policy_data:\n",
    "schema2 = [\"policy_id\",'customer_id','premium_amount','coverage_amount']\n",
    "data2 = [(1,101,500.0,100000.0),\n",
    "         (2,102,800.0,150000.0),\n",
    "(3,103,600.0,120000.0),\n",
    "(4,104,700.0,130000.0),\n",
    "(5,105,900.0,180000.0),\n",
    "(6,106,550.0,110000.0),\n",
    "(7,107,750.0,140000.0),\n",
    "(8,108,950.0,200000.0),\n",
    "(9,109,620.0,125000.0),\n",
    "(10,110,850.0,160000.0)]\n",
    "\n",
    "policy_df = spark.createDataFrame(data2, schema2)\n",
    "policy_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c56093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------+\n",
      "|customer_id|avg_claim_amount_per_policy|\n",
      "+-----------+---------------------------+\n",
      "|        110|                    26000.0|\n",
      "|        107|                    28000.0|\n",
      "|        103|                    18000.0|\n",
      "|        104|                    30000.0|\n",
      "|        106|                    19000.0|\n",
      "|        105|                    22000.0|\n",
      "|        108|                    33000.0|\n",
      "|        101|                    20000.0|\n",
      "|        102|                    25000.0|\n",
      "|        109|                    21000.0|\n",
      "+-----------+---------------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(customer_id=108, avg_claim_amount_per_policy=33000.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the datasets on the 'policy_id' column\n",
    "joined_df = policy_df.join(claims_df, 'policy_id')\n",
    "\n",
    "# Calculate the claim amount per policy for each claim\n",
    "joined_df = joined_df.withColumn('claim_amount_per_policy', col('claim_amount'))\n",
    "\n",
    "# Calculate the average claim amount per policy for each customer\n",
    "avg_claim_df = joined_df.groupBy('customer_id').agg(\n",
    " avg('claim_amount_per_policy').alias('avg_claim_amount_per_policy')\n",
    ")\n",
    "\n",
    "# Find the customer with the highest average claim amount\n",
    "max_avg_claim_customer = avg_claim_df.orderBy(desc('avg_claim_amount_per_policy')).first()\n",
    "avg_claim_df.show()\n",
    "max_avg_claim_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c5437c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|     numbers|\n",
      "+---+------------+\n",
      "|  1|[10, 20, 30]|\n",
      "|  2|    [40, 50]|\n",
      "|  3|        [60]|\n",
      "+---+------------+\n",
      "\n",
      "+---+------+\n",
      "| id|number|\n",
      "+---+------+\n",
      "|  1|    10|\n",
      "|  1|    20|\n",
      "|  1|    30|\n",
      "|  2|    40|\n",
      "|  2|    50|\n",
      "|  3|    60|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7132352789942255617/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132352789942255617%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample data\n",
    "data = [(1, [10, 20, 30]), (2, [40, 50]), (3, [60])]\n",
    "\n",
    "# Create a DataFrame and showing raw data\n",
    "df = spark.createDataFrame(data, [\"id\", \"numbers\"])\n",
    "df.show()\n",
    "\n",
    "# Use explode for arrays\n",
    "df_expanded = df.select(\"id\", explode(\"numbers\").alias(\"number\"))\n",
    "\n",
    "# Show the result\n",
    "df_expanded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da6f67",
   "metadata": {},
   "source": [
    "🌟 𝑾𝒉𝒚 𝑼𝒔𝒆 `𝒆𝒙𝒑𝒍𝒐𝒅𝒆`?\n",
    "- 𝐹𝑙𝑎𝑡𝑡𝑒𝑛𝑖𝑛𝑔 𝐴𝑟𝑟𝑎𝑦𝑠: Ideal for scenarios where you want to transform arrays into individual rows.\n",
    "- 𝐻𝑎𝑛𝑑𝑙𝑖𝑛𝑔 𝑁𝑒𝑠𝑡𝑒𝑑 𝐷𝑎𝑡𝑎: Perfect for working with complex, nested structures common in real-world datasets.\n",
    "\n",
    "𝑯𝒐𝒘 𝑪𝒂𝒏 𝒀𝒐𝒖 𝑳𝒆𝒗𝒆𝒓𝒂𝒈𝒆 𝑰𝒕?\n",
    "- 𝑁𝑒𝑠𝑡𝑒𝑑 𝐽𝑆𝑂𝑁 𝑃𝑟𝑜𝑐𝑒𝑠𝑠𝑖𝑛𝑔: Unpack nested JSON arrays or maps for analysis.\n",
    "- 𝐷𝑎𝑡𝑎 𝑁𝑜𝑟𝑚𝑎𝑙𝑖𝑧𝑎𝑡𝑖𝑜𝑛: Flatten arrays to simplify downstream processing.\n",
    "- 𝐸𝑥𝑝𝑙𝑜𝑟𝑎𝑡𝑜𝑟𝑦 𝐷𝑎𝑡𝑎 𝐴𝑛𝑎𝑙𝑦𝑠𝑖𝑠: Gain insights from nested structures with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5307985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+-----------+------------+-------------+--------------+\n",
      "|customer_id|age|income|loan_amount|credit_score|loan_approval|previous_loans|\n",
      "+-----------+---+------+-----------+------------+-------------+--------------+\n",
      "|          1| 35| 60000|       5000|         700|          Yes|             2|\n",
      "|          2| 28| 45000|       8000|         650|          Yes|             1|\n",
      "|          3| 45| 80000|      12000|         750|           No|             0|\n",
      "|          4| 32| 55000|       6000|         680|          Yes|             3|\n",
      "|          5| 40| 70000|      10000|         720|           No|             1|\n",
      "|          6| 27| 40000|       7000|         630|          Yes|             2|\n",
      "|          7| 38| 75000|      15000|         780|           No|             0|\n",
      "|          8| 31| 50000|       9000|         690|          Yes|             2|\n",
      "|          9| 42| 85000|      11000|         760|          Yes|             4|\n",
      "|         10| 29| 48000|       7500|         670|           No|             1|\n",
      "+-----------+---+------+-----------+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7132377874984022016/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132377874984022016%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema1 = ['customer_id','age','income','loan_amount','credit_score','loan_approval','previous_loans']\n",
    "\n",
    "data1 = [\n",
    "(1,35,60000,5000,700,'Yes',2),\n",
    "(2,28,45000,8000,650,'Yes',1),\n",
    "(3,45,80000,12000,750,'No',0),\n",
    "(4,32,55000,6000,680,'Yes',3),\n",
    "(5,40,70000,10000,720,'No',1),\n",
    "(6,27,40000,7000,630,'Yes',2),\n",
    "(7,38,75000,15000,780,'No',0),\n",
    "(8,31,50000,9000,690,'Yes',2),\n",
    "(9,42,85000,11000,760,'Yes',4),\n",
    "(10,29,48000,7500,670,'No',1)]\n",
    "\n",
    "df = spark.createDataFrame(data1, schema1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe78dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+-----------+------------+-------------+--------------+-----------+\n",
      "|customer_id|age|income|loan_amount|credit_score|loan_approval|previous_loans|credit_rank|\n",
      "+-----------+---+------+-----------+------------+-------------+--------------+-----------+\n",
      "|          6| 27| 40000|       7000|         630|          Yes|             2|          1|\n",
      "|          2| 28| 45000|       8000|         650|          Yes|             1|          1|\n",
      "|         10| 29| 48000|       7500|         670|           No|             1|          1|\n",
      "|          8| 31| 50000|       9000|         690|          Yes|             2|          1|\n",
      "|          4| 32| 55000|       6000|         680|          Yes|             3|          1|\n",
      "|          1| 35| 60000|       5000|         700|          Yes|             2|          1|\n",
      "|          7| 38| 75000|      15000|         780|           No|             0|          1|\n",
      "|          5| 40| 70000|      10000|         720|           No|             1|          1|\n",
      "|          9| 42| 85000|      11000|         760|          Yes|             4|          1|\n",
      "|          3| 45| 80000|      12000|         750|           No|             0|          1|\n",
      "+-----------+---+------+-----------+------------+-------------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate the rank based on the credit score for each age group\n",
    "window_spec = Window.partitionBy(\"age\").orderBy(\"credit_score\")\n",
    "credit_data = df.withColumn(\"credit_rank\", rank().over(window_spec))\n",
    "credit_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6f252a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+------+-----------+------------+-------------+--------------+\n",
      "|customer_id|age|income|loan_amount|credit_score|loan_approval|previous_loans|\n",
      "+-----------+---+------+-----------+------------+-------------+--------------+\n",
      "|          6| 27| 40000|       7000|         630|          Yes|             2|\n",
      "|          2| 28| 45000|       8000|         650|          Yes|             1|\n",
      "|         10| 29| 48000|       7500|         670|           No|             1|\n",
      "|          8| 31| 50000|       9000|         690|          Yes|             2|\n",
      "|          4| 32| 55000|       6000|         680|          Yes|             3|\n",
      "|          1| 35| 60000|       5000|         700|          Yes|             2|\n",
      "|          7| 38| 75000|      15000|         780|           No|             0|\n",
      "|          5| 40| 70000|      10000|         720|           No|             1|\n",
      "|          9| 42| 85000|      11000|         760|          Yes|             4|\n",
      "|          3| 45| 80000|      12000|         750|           No|             0|\n",
      "+-----------+---+------+-----------+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Identify High Credit Risk Customers (Rank 1)\n",
    "high_credit_risk_customers = credit_data.filter(col(\"credit_rank\") == 1)\n",
    "high_credit_risk_customers = high_credit_risk_customers.select(\n",
    " \"customer_id\", \"age\", \"income\", \"loan_amount\", \"credit_score\", \"loan_approval\", \"previous_loans\")\n",
    "\n",
    "high_credit_risk_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b9f84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0354a87",
   "metadata": {},
   "source": [
    "###### https://www.linkedin.com/feed/update/urn:li:activity:7132233433853886465/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132233433853886465%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "### Exploring_Essential_DateTime_Functions_in_PySpark! \n",
    "\n",
    "In PySpark, we have a powerful toolkit of datetime functions to help us manipulate and analyze temporal data. Let's dive into some fundamental datetime functions and their usage with examples:\n",
    "\n",
    "1️⃣ to_date():\n",
    "This function converts a string representation of a date to a DateType. It's great for filtering or grouping by date.\n",
    "```\n",
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn(\"date_column\", to_date(\"date_string_column\", \"yyyy-MM-dd\"))\n",
    "```\n",
    "\n",
    "2️⃣ date_add() and date_sub():\n",
    "These functions allow you to add or subtract days from a date.\n",
    "```\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "df = df.withColumn(\"future_date\", date_add(\"date_column\", 7))\n",
    "df = df.withColumn(\"past_date\", date_sub(\"date_column\", 3))\n",
    "```\n",
    "3️⃣ datediff():\n",
    "Calculates the difference in days between two dates.\n",
    "```\n",
    "from pyspark.sql.functions import datediff\n",
    "df = df.withColumn(\"date_difference\", datediff(\"date2\", \"date1\"))\n",
    "```\n",
    "4️⃣ date_format():\n",
    "This function converts a date to a string with a specified format.\n",
    "```\n",
    "from pyspark.sql.functions import date_format\n",
    "df = df.withColumn(\"formatted_date\", date_format(\"date_column\", \"dd-MM-yyyy\"))\n",
    "```\n",
    "5️⃣ trunc():\n",
    "Truncates a date to a specified level (day, month, year).\n",
    "```\n",
    "from pyspark.sql.functions import trunc\n",
    "df = df.withColumn(\"year\", trunc(\"date_column\", \"yyyy\"))\n",
    "df = df.withColumn(\"month\", trunc(\"date_column\", \"MM\"))\n",
    "```\n",
    "These functions empower PySpark users to handle datetime data efficiently, making it easier to perform tasks like data aggregation, filtering, and feature engineering. Harnessing the power of datetime functions is a key step in any data project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52a1e662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---------------+----+-----+\n",
      "| id|student_name|department_name|year|marks|\n",
      "+---+------------+---------------+----+-----+\n",
      "|  1|         ram|            EEE|2023|   80|\n",
      "|  2|        shiv|            CSE|2023|   86|\n",
      "|  4|      Anusha|            CSE|2023| null|\n",
      "|  5|       geeta|            ECE|2023| null|\n",
      "+---+------------+---------------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131250476611768321/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131250476611768321%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data1=[(1,\"ram\",\"EEE\",\"2023\",80),(2,\"shiv\",\"CSE\",\"2023\",86)]\n",
    "df1 = spark.createDataFrame(data1).toDF(\"id\",\"student_name\",\"department_name\",\"year\",\"marks\")\n",
    "\n",
    "\n",
    "data2=[(4,\"Anusha\",\"CSE\",\"2023\"),(5,\"geeta\",\"ECE\",\"2023\")]\n",
    "df2 = spark.createDataFrame(data2).toDF(\"id\",\"student_name\",\"department_name\",\"year\")\n",
    "\n",
    "df3=df2.withColumn(\"Marks\", lit(\"null\"))\n",
    "# Merged Dataframe\n",
    "mergeDf=df1.union(df3)\n",
    "mergeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40253f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+----------+\n",
      "|employee_id|employee_name|salary|manager_id|\n",
      "+-----------+-------------+------+----------+\n",
      "|          1|         John| 50000|         2|\n",
      "|          2|        Alice| 60000|         3|\n",
      "|          3|          Bob| 55000|         5|\n",
      "|          4|      Micheal| 45000|         2|\n",
      "|          5|        Sarah| 40000|         6|\n",
      "|          6|       Robert| 35000|      None|\n",
      "+-----------+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7132210496119734273/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132210496119734273%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 𝐅𝐢𝐧𝐝 𝐭𝐡𝐞 𝐞𝐦𝐩𝐥𝐨𝐲𝐞𝐞𝐬 𝐰𝐡𝐨 𝐞𝐚𝐫𝐧 𝐦𝐨𝐫𝐞 𝐭𝐡𝐚𝐧 𝐭𝐡𝐞𝐢𝐫 𝐫𝐞𝐬𝐩𝐞𝐜𝐭𝐢𝐯𝐞 𝐦𝐚𝐧𝐚𝐠𝐞𝐫𝐬\n",
    "data = [\n",
    " (\"1\", \"John\", \"50000\", \"2\"),\n",
    " (\"2\", \"Alice\", \"60000\", \"3\"),\n",
    " (\"3\", \"Bob\", \"55000\", \"5\"),\n",
    " (\"4\", \"Micheal\", \"45000\", \"2\"),\n",
    " (\"5\", \"Sarah\", \"40000\", \"6\"),\n",
    " (\"6\", \"Robert\", \"35000\", \"None\")]\n",
    "\n",
    "schema = [\"employee_id\", \"employee_name\", \"salary\", \"manager_id\"]\n",
    "employe_df=spark.createDataFrame(data).toDF(*schema)\n",
    "employe_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "576cd9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+------------+-----------+\n",
      "|employee_id|employee_name|salary|manager_name|manager_sal|\n",
      "+-----------+-------------+------+------------+-----------+\n",
      "|          2|        Alice| 60000|         Bob|      55000|\n",
      "|          3|          Bob| 55000|       Sarah|      40000|\n",
      "|          5|        Sarah| 40000|      Robert|      35000|\n",
      "+-----------+-------------+------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = employe_df.alias('e1').join(employe_df.alias('e2'), col('e1.manager_id') == col('e2.employee_id'), 'inner')\\\n",
    "                .filter(col('e1.salary') > col('e2.salary'))\\\n",
    "                .selectExpr('e1.employee_id', 'e1.employee_name', 'e1.salary', \\\n",
    "                            'e2.employee_name as manager_name', 'e2.salary as manager_sal' )\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "902557f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------+------------+-----------+--------------+\n",
      "|policy_id|insurance_type|claim_date|claim_amount|insured_age|insured_gender|\n",
      "+---------+--------------+----------+------------+-----------+--------------+\n",
      "|        1|          Auto|2023-01-01|        5000|         35|          Male|\n",
      "|        1|          Auto|2023-02-15|        7000|         35|          Male|\n",
      "|        1|          Home|2023-03-10|        3000|         35|          Male|\n",
      "|        2|          Auto|2023-01-05|        4500|         40|        FeMale|\n",
      "|        2|          Auto|2023-02-20|        6000|         40|        FeMale|\n",
      "|        2|          Home|2023-03-15|        3500|         40|        FeMale|\n",
      "|        3|        Health|2023-01-10|        2000|         28|          Male|\n",
      "|        3|        Health|2023-02-25|        3000|         28|          Male|\n",
      "|        4|          Auto|2023-01-15|        5500|         45|        FeMale|\n",
      "|        4|          Auto|2023-02-28|        8000|         45|        FeMale|\n",
      "|        4|          Home|2023-03-20|        4000|         45|        FeMale|\n",
      "|        5|        Health|2023-01-20|        2500|         33|        FeMale|\n",
      "|        5|        Health|2023-03-05|        3500|         33|        FeMale|\n",
      "|        6|          Auto|2023-01-25|        6000|         50|          Male|\n",
      "|        6|          Auto|2023-03-10|        7500|         50|          Male|\n",
      "|        6|          Home|2023-03-25|        4500|         50|          Male|\n",
      "|        7|        Health|2023-02-01|        1800|         28|        FeMale|\n",
      "|        7|        Health|2023-03-15|        2800|         28|        FeMale|\n",
      "|        8|          Home|2023-02-05|        4000|         38|          Male|\n",
      "|        8|          Home|2023-03-20|        5500|         38|          Male|\n",
      "+---------+--------------+----------+------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7131677453747429376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131677453747429376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema = ['policy_id','insurance_type','claim_date','claim_amount','insured_age','insured_gender']\n",
    "data = [(1,'Auto','2023-01-01',5000,35,'Male'),\n",
    "(1,'Auto','2023-02-15',7000,35,'Male'),\n",
    "(1,'Home','2023-03-10',3000,35,'Male'),\n",
    "(2,'Auto','2023-01-05',4500,40,'FeMale'),\n",
    "(2,'Auto','2023-02-20',6000,40,'FeMale'),\n",
    "(2,'Home','2023-03-15',3500,40,'FeMale'),\n",
    "(3,'Health','2023-01-10',2000,28,'Male'),\n",
    "(3,'Health','2023-02-25',3000,28,'Male'),\n",
    "(4,'Auto','2023-01-15',5500,45,'FeMale'),\n",
    "(4,'Auto','2023-02-28',8000,45,'FeMale'),\n",
    "(4,'Home','2023-03-20',4000,45,'FeMale'),\n",
    "(5,'Health','2023-01-20',2500,33,'FeMale'),\n",
    "(5,'Health','2023-03-05',3500,33,'FeMale'),\n",
    "(6,'Auto','2023-01-25',6000,50,'Male'),\n",
    "(6,'Auto','2023-03-10',7500,50,'Male'),\n",
    "(6,'Home','2023-03-25',4500,50,'Male'),\n",
    "(7,'Health','2023-02-01',1800,28,'FeMale'),\n",
    "(7,'Health','2023-03-15',2800,28,'FeMale'),\n",
    "(8,'Home','2023-02-05',4000,38,'Male'),\n",
    "(8,'Home','2023-03-20',5500,38,'Male')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a95c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158a28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8ca72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d554e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cef1e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e0c9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c074a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b573c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdf283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133439078947037184/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133439078947037184%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0a16b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133300632303587328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133300632303587328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9655588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133650440394616832/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133650440394616832%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92edfcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133038914390392835/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133038914390392835%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "11/1/1023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132887915352784896/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132887915352784896%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b41310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132784170765885440/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132784170765885440%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132872813538910208/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132872813538910208%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66119fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133612690916184064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133612690916184064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133257864789327873/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133257864789327873%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133272955505033218/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133272955505033218%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb7316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-spark-activity-7132377874984022016-m2kx/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133125938602491904/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133125938602491904%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135466730939682816/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135466730939682816%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "𝐒𝐡𝐚𝐫𝐞𝐝 𝐕𝐚𝐫𝐢𝐚𝐛𝐥𝐞𝐬 𝐢𝐧 𝐒𝐩𝐚𝐫𝐤\n",
    "=================================\n",
    "There are 2 type of shared variables\n",
    "i)Broadcast variable\n",
    "ii) Accumulator variable\n",
    "\n",
    "𝐁𝐫𝐨𝐚𝐝𝐜𝐚𝐬𝐭 𝐕𝐚𝐫𝐢𝐚𝐛𝐥𝐞 :\n",
    "-> Broadcast variables are read-only, distributed data structures used to efficiently share data across worker nodes in a Spark cluster\n",
    "-> Broadcast Variables are used to cache a value or object on each node in the cluster, which can then be used across multiple tasks.\n",
    "-> They are loaded onto every worker node so that they can access them.\n",
    "\n",
    " 𝐔𝐬𝐞 𝐜𝐚𝐬𝐞 : You can use broadcast to distribute lookup tables or reference data to each worker node, used in a join operation or filter operation.\n",
    "Ex : val joinedDf=largeDf.join(broadcast(smallDf))\n",
    "=======================================\n",
    "𝐀𝐜𝐜𝐮𝐦𝐮𝐥𝐚𝐭𝐨𝐫 :\n",
    "->Accumulator variable is a shared copy kept in driver node and all the executors will have access to it.\n",
    "->Executors update the variable in Driver node.\n",
    "->Executors only have an access to update the variable they do not have access to read the variable.\n",
    "\n",
    "𝐔𝐬𝐞 𝐜𝐚𝐬𝐞 :\n",
    "you can use an accumulator to keep track of the count of a particular event, sum of values, or any other kind of accumulation that you need to do across all the worker nodes\n",
    "Ex : val accum = sc.longAccumulator(\"Sum Accumulator\")\n",
    "val rdd=sc.parallelize(Array(1, 2, 3)).foreach(x => accum.add(x))\n",
    "println(accum.value)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c9975",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130236033287274497/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130236033287274497%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134163826815418368/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134163826815418368%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "12/1/1023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842bd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133831650236411904/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133831650236411904%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfad444",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_databricks-pyspark-spark-activity-7134594743274995712-4LwC/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc62bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134960321202446336/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134960321202446336%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03773a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134964193274843139/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134964193274843139%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88957854",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134766136801263617/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134766136801263617%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134424091536572416/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134424091536572416%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f6ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133878542810951680/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133878542810951680%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805bbcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136989323773493248/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136989323773493248%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133310348861861888/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133310348861861888%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277439c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135618539335389185/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135618539335389185%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2743da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135657443992248321/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135657443992248321%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecbc198",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136950876237144064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136950876237144064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136722175230689280/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136722175230689280%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "project: https://github.com/Pavanpawar2705/Ingesting-Real-time-Logistics-Data-in-MongoDB-with-Kafka-and-Python                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136421513045434368/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136421513045434368%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14257148",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7137026016887140352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7137026016887140352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135663369507803136/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135663369507803136%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7138387159870296064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7138387159870296064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57eff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135475274451664896/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135475274451664896%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1391a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7137478380425285632/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7137478380425285632%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a03c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7138499356214665218/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7138499356214665218%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "https://www.montecarlodata.com/blog-pyspark-data-quality-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7138583044248035328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7138583044248035328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e07015",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7141326742601863168/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7141326742601863168%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "Deloitte Data Engineer Interview question and Round :\n",
    "soultuion:\n",
    "Q:- How would you rename 100 column in pyspark?\n",
    "Ans:-\n",
    "from functools import reduce\n",
    "def rename_cols(df, old_columns, new_columns):\n",
    "  for old_col, new_col in zip(old_columns, new_columns):\n",
    "    df = df.withColumnRenamed(old_col, new_col)\n",
    "  return df\n",
    "\n",
    "old_columns = ['old_name1', 'old_name2', ..., 'old_name100']\n",
    "new_columns = ['new_name1', 'new_name2', ..., 'new_name100']\n",
    "\n",
    "df_renamed = rename_cols(df, old_columns, new_columns)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140921123524837376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140921123524837376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "🛑 Features of PySpark :\n",
    "===================\n",
    "\n",
    "1. In memory computation\n",
    "2. Distributed processing using parallelism\n",
    "3. Fault - tolerance\n",
    "4. Immutable\n",
    "5. Lazy Evaluation\n",
    "6. Cache & Persistence\n",
    "7. Inbuild optimization using DataFrame\n",
    "\n",
    "🛑 Advantage of PySpark\n",
    "==================\n",
    "\n",
    "1. It is a general purpose in-memory distributed processing engine that allows to process data efficiently in a distributed manner.\n",
    "\n",
    "2. Application running on spark is 100x faster than traditional one (i.e Hadoop ecosystem)\n",
    "\n",
    "3. We can process data from external data resource like HDFS, AWS, S3 & many more file system.\n",
    "\n",
    "4. It has Mlib & graph libraries as well for processing.\n",
    "\n",
    "5. By spark we can process real-time data through streaming & Kafka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd9449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140407880188428288/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140407880188428288%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "Data Engineering Interview Question:\n",
    "===========================\n",
    "\n",
    "Spark map() vs mapPartitions():\n",
    "========================\n",
    "\n",
    "map() –\n",
    "\n",
    "Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.\n",
    "\n",
    "\n",
    "mapPartitions() –\n",
    "\n",
    "This is exactly the same as map(); the difference being, Spark mapPartitions() provides a facility to do heavy initializations (for example Database connection) once for each partition instead of doing it on every DataFrame row. This helps the performance of the job when you dealing with heavy-weighted initialization on larger datasets.\n",
    "\n",
    "map vs flatmap :\n",
    "=============\n",
    "\n",
    "The key difference between map and flatMap is map returns only one row/element for every input, while flatMap() can return a list of rows/elements.                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd099f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140257445376937984/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140257445376937984%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "Data Engineering Interview question:\n",
    "===========================\n",
    "\n",
    "distinct() vs dropDuplicates() :\n",
    "\n",
    "\n",
    "distinct() and dropDuplicates() in Spark are used to remove duplicate rows, but there is a subtle difference. distinct() considers all columns when identifying duplicates, while dropDuplicates() allowing you to specify a subset of columns to determine uniqueness.\n",
    "\n",
    "Please comment on which one is better if we consider we need to remove duplicate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec976c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140395946852429824/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140395946852429824%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca0a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7141653171134787584/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7141653171134787584%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a5ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143547646807355393/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143547646807355393%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26628350",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7142294899235696640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7142294899235696640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143112135022133249/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143112135022133249%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf45491",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143210451340804097/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143210451340804097%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f33803",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143268521010008067/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143268521010008067%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd64232",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144726008561041408/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144726008561041408%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b430a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143508594397724672/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143508594397724672%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd148e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144663579512791040/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144663579512791040%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143656501138673667/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143656501138673667%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76948a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144167235375280128/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144167235375280128%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1821c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7142271906786725888/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7142271906786725888%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5be613a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144922987287240704/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144922987287240704%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7206cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143818058900324352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143818058900324352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "\n",
    "see the pdf and see                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747d9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7145061670602678272/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7145061670602678272%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d35c9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "This is the best explanation to differentiate🤯.. Let’s learn…\n",
    "\n",
    "1️⃣ 𝗗𝗮𝘁𝗮 𝗪𝗮𝗿𝗲𝗵𝗼𝘂𝘀𝗲: A data warehouse is a centralized repository of data that is used for analysis and reporting. It is typically used to store historical data, but can also store real-time data. Data warehouses are typically structured, meaning that the data is organized in a way that makes it easy to query and analyze.🏢\n",
    "\n",
    "2️⃣ 𝗗𝗮𝘁𝗮 𝗠𝗮𝗿𝘁: A data mart is a subset of a data warehouse. It's designed to cater to the needs of a specific business unit or team. Think of it as a department store within the larger shopping mall (the data warehouse).🏪\n",
    "\n",
    "3️⃣ 𝗗𝗮𝘁𝗮 𝗟𝗮𝗸𝗲: A data lake is a repository for all of an organization's data, both structured and unstructured. It is a more flexible and scalable solution than a data warehouse, as it can store any type of data, regardless of its structure. Data lakes are typically used for big data analytics, as they can store and process large amounts of data quickly and efficiently..🌊\n",
    "\n",
    "4️⃣ 𝗗𝗲𝗹𝘁𝗮 𝗟𝗮𝗸𝗲: Delta Lake is a data lake storage layer that provides a unified view of data across multiple data lakes. It is a relatively new technology, but it has quickly become popular due to its ability to provide a consistent view of data in a data lake.\n",
    "\n",
    "• ACID transactions\n",
    "• Data versioning\n",
    "• Automatic compaction🛠️\n",
    "\n",
    "5️⃣ 𝗗𝗮𝘁𝗮 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: A data pipeline is the process of moving data from one location to another, often involving steps for extracting, transforming, and loading data (ETL). It's like the conveyer belt in a factory, ensuring that the data gets to where it's needed in a usable form.🚀\n",
    "\n",
    "6️⃣ 𝗗𝗮𝘁𝗮 𝗠𝗲𝘀𝗵: Data Mesh is an architectural paradigm that treats data as a product. It decentralizes data ownership and architecture, allowing teams to develop, govern, and operate their own data domains. It's a shift from monolithic, centralized data management to a more distributed approach.🕸️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144217757201358849/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144217757201358849%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7fdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146341572584816640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146341572584816640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59238446",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146286978206949376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146286978206949376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146478784010006529/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146478784010006529%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755dfb79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e54b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319d558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac32cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41967e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c273a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43a3d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2ca5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e796f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc2275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482d7f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-databricks-activity-7116845920289234944-9Zvn/?utm_source=share&utm_medium=member_desktop\n",
    "    \n",
    "links to previous questions    \n",
    "\n",
    "do all the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118563608191533056/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118563608191533056%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "do in last        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60219daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120828695124074497/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120828695124074497%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-spark-activity-7126262755782799360-zsjS/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91facfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "40\n",
    "\n",
    "https://www.linkedin.com/feed/update/urn:li:activity:7131308221222293505/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131308221222293505%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131113063897800705/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131113063897800705%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d171bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130992927283052544/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130992927283052544%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58793d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136270124465999872/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136270124465999872%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "try all this                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2688a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "project\n",
    "\n",
    "https://github.com/MrSachinGoyal/gcp-dataproc-pyspark-analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b5fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7145633755364229120/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7145633755364229120%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146764370142724097/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146764370142724097%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "5 SQL projects to build your strong Analytics portfolio 🚀\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb75bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/imadhaka/PyLeetProblems/tree/master/spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646569a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7121832036981501952/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121832036981501952%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4602256280705183/69616450320427/8093088653480119/latest.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e065611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/mattmartin14/dream_machine/tree/main/deltalake/docker/dl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b313abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7123284641020579843/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123284641020579843%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 𝐓𝐨 𝐜𝐚𝐥𝐜𝐮𝐥𝐚𝐭𝐞 𝐚 𝐦𝐨𝐯𝐢𝐧𝐠 𝐚𝐯𝐞𝐫𝐚𝐠𝐞 𝐢𝐧 𝐏𝐲𝐒𝐩𝐚𝐫𝐤 𝐨𝐧 𝐚 𝐬𝐚𝐥𝐞𝐬 𝐭𝐚𝐛𝐥𝐞?\n",
    "\n",
    "data = [{\"date\": \"2023-10-01\", \"sales_amount\": 100},\n",
    "        {\"date\": \"2023-10-02\", \"sales_amount\": 150},\n",
    "        {\"date\": \"2023-10-03\", \"sales_amount\": 200},\n",
    "        {\"date\": \"2023-10-04\", \"sales_amount\": 250},\n",
    "        {\"date\": \"2023-10-05\", \"sales_amount\": 300},\n",
    "        {\"date\": \"2023-10-06\", \"sales_amount\": 350}]\n",
    "df= spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16804050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7125014408900083713/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125014408900083713%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# 𝟭 𝗺𝗶𝗻 𝘀𝗵𝗼𝗿𝘁 𝘃𝗶𝗱𝗲𝗼 𝗳𝗼𝗿 𝗠𝗼𝘀𝘁𝗹𝘆 𝗮𝘀𝗸𝗲𝗱 𝗦𝗽𝗮𝗿𝗸 𝗶𝗻𝘁𝗲𝗿𝘃𝗶𝗲𝘄 𝗤𝘂𝗲𝘀\n",
    "# watch all the questions                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14827ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110 + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7127242879789330432/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127242879789330432%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# https://www.linkedin.com/feed/update/urn:li:activity:7129973716570017792/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7129973716570017792%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a25ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e977be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a8f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
