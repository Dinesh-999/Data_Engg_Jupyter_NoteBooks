{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88745d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "# Normal Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType \n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"imported\")\n",
    "\n",
    "location = \"/Users/dinesh/Desktop/DINESH H R/Programming/Data Engineering/Course  Contents/scenarios_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e6e6bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/20 16:45:31 WARN Utils: Your hostname, DINESHs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.15 instead (on interface en0)\n",
      "23/12/20 16:45:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/20 16:45:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/20 16:45:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n"
     ]
    }
   ],
   "source": [
    "# spark context\n",
    "conf = SparkConf().setAppName(\"Linkedin\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('Linkedin scenario').getOrCreate()\n",
    "\n",
    "print(\"imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b62756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|  fruit| id|\n",
      "+-------+---+\n",
      "|  Apple|  1|\n",
      "|Bananna|  2|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{\"id\": 1, \"Fruit\": 'Apple'},\n",
    "{\"id\": 2, \"Fruit\": 'Bananna'}]\n",
    "\n",
    "spark.createDataFrame(data,schema= ['fruit','id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324d0512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "| katy| 32|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "| katy| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Alice\", 25),(\"katy\",32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Drop duplicates based on 'Name' and 'Age'\n",
    "deduplicated_df = df.dropDuplicates(subset=[\"Name\", \"Age\"])\n",
    "deduplicated_df.show()\n",
    "# 2. Using df.distinct()\n",
    "\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db77c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|text                         |\n",
      "+-----------------------------+\n",
      "|Hello world and hello PySpark|\n",
      "|This is a PySpark example    |\n",
      "|Let's count unique words     |\n",
      "+-----------------------------+\n",
      "\n",
      "+-----------------------------+-------+\n",
      "|text                         |word   |\n",
      "+-----------------------------+-------+\n",
      "|Hello world and hello PySpark|Hello  |\n",
      "|Hello world and hello PySpark|world  |\n",
      "|Hello world and hello PySpark|and    |\n",
      "|Hello world and hello PySpark|hello  |\n",
      "|Hello world and hello PySpark|PySpark|\n",
      "|This is a PySpark example    |This   |\n",
      "|This is a PySpark example    |is     |\n",
      "|This is a PySpark example    |a      |\n",
      "|This is a PySpark example    |PySpark|\n",
      "|This is a PySpark example    |example|\n",
      "|Let's count unique words     |Let's  |\n",
      "|Let's count unique words     |count  |\n",
      "|Let's count unique words     |unique |\n",
      "|Let's count unique words     |words  |\n",
      "+-----------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Hello world and hello PySpark\",),\n",
    "    (\"This is a PySpark example\",),\n",
    "    (\"Let's count unique words\",)]\n",
    "\n",
    "schema = [\"text\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Split the text column into words and explode to create one row per word\n",
    "words_df = df.withColumn(\"word\", explode(split(df[\"text\"], \" \")))\n",
    "words_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79c4f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT word)|\n",
      "+--------------------+\n",
      "|                  13|\n",
      "+--------------------+\n",
      "\n",
      "13\n",
      "Total number of unique words: 13\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of unique words\n",
    "\n",
    "words_df.select(countDistinct(\"word\")).show()\n",
    "\n",
    "unique_word_count = words_df.select(countDistinct(\"word\")).collect()[0][0]\n",
    "print(unique_word_count)\n",
    "\n",
    "# Show the total number of unique words\n",
    "print(\"Total number of unique words:\", unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ed681",
   "metadata": {},
   "source": [
    "1. https://www.linkedin.com/feed/update/urn:li:activity:7101496064259604480/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7101496064259604480%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc4d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7101496064259604480/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7101496064259604480%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29 -- \n",
    "\n",
    "columns = [\"user_id\", \"timestamp\"]\n",
    "\n",
    "# data= [(\"user1\", \"2023-08-21 10:00:00\"),\n",
    "# (\"user2\", \"2023-08-21 11:30:00\"),\n",
    "# (\"user1\", \"2023-08-21 12:15:00\"),\n",
    "# (\"user3\", \"2023-08-21 13:45:00\"),\n",
    "# (\"user2\", \"2023-08-21 14:30:00\"),\n",
    "# (\"user1\", \"2023-08-21 15:00:00\")]\n",
    "\n",
    "data = [\n",
    "(\"user1\", \"2023-08-21 10:09:09\"),\n",
    "(\"user2\", \"2023-08-21 11:30:00\"),\n",
    "(\"user1\", \"2023-08-21 12:15:00\"),\n",
    "(\"user3\", \"2023-08-21 13:45:09\"),\n",
    "(\"user2\", \"2023-08-21 14:30:00\"),\n",
    "(\"user1\", \"2023-08-21 15:09:09\")\n",
    "]\n",
    "\n",
    "# Questions:\n",
    "# ğŸ“Œ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğğšğ«ğ¥ğ¢ğğ¬ğ­ ğšğ§ğ ğ¥ğšğ­ğğ¬ğ­ ğ­ğ¢ğ¦ğğ¬ğ­ğšğ¦ğ©ğ¬ ğ¢ğ§ ğ­ğ¡ğ ğğšğ­ğšğ¬ğğ­.\n",
    "# ğŸ“Œ ğ‚ğ¨ğ®ğ§ğ­ ğ­ğ¡ğ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğšğœğ­ğ¢ğ¯ğ¢ğ­ğ¢ğğ¬ ğ©ğğ« ğ®ğ¬ğğ«.\n",
    "# ğŸ“Œ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¢ğ¦ğ ğğ®ğ«ğšğ­ğ¢ğ¨ğ§ ğ›ğğ­ğ°ğğğ§ ğœğ¨ğ§ğ¬ğğœğ®ğ­ğ¢ğ¯ğ ğšğœğ­ğ¢ğ¯ğ¢ğ­ğ¢ğğ¬ ğŸğ¨ğ« ğğšğœğ¡ ğ®ğ¬ğğ«."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f9a8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|          timestamp|\n",
      "+-------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|\n",
      "|  user2|2023-08-21 11:30:00|\n",
      "|  user1|2023-08-21 12:15:00|\n",
      "|  user3|2023-08-21 13:45:09|\n",
      "|  user2|2023-08-21 14:30:00|\n",
      "|  user1|2023-08-21 15:09:09|\n",
      "+-------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame(data, schema=columns)\n",
    "data.withColumn(\"timestamp1\", to_timestamp(data.timestamp))\n",
    "data.show()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7748b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest\n",
      "+-------+-------------------+\n",
      "|user_id|          timestamp|\n",
      "+-------+-------------------+\n",
      "|  user1|2023-08-21 15:09:09|\n",
      "+-------+-------------------+\n",
      "\n",
      "Earliest\n",
      "+-------+-------------------+\n",
      "|user_id|          timestamp|\n",
      "+-------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# method 1\n",
    "# ğŸ“Œ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğğšğ«ğ¥ğ¢ğğ¬ğ­ ğšğ§ğ ğ¥ğšğ­ğğ¬ğ­ ğ­ğ¢ğ¦ğğ¬ğ­ğšğ¦ğ©ğ¬ ğ¢ğ§ ğ­ğ¡ğ ğğšğ­ğšğ¬ğğ­.\n",
    "ordered_desc = data.orderBy(desc(data.timestamp))\n",
    "print(\"Latest\")\n",
    "ordered_desc.limit(1).show()\n",
    "\n",
    "ordered_asc = data.orderBy(data.timestamp)\n",
    "print(\"Earliest\")\n",
    "ordered_asc.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0b8da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|          timestamp|\n",
      "+-------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|\n",
      "|  user2|2023-08-21 11:30:00|\n",
      "|  user1|2023-08-21 12:15:00|\n",
      "|  user3|2023-08-21 13:45:09|\n",
      "|  user2|2023-08-21 14:30:00|\n",
      "|  user1|2023-08-21 15:09:09|\n",
      "+-------+-------------------+\n",
      "\n",
      "2023-08-21 15:09:09\n",
      "2023-08-21 10:09:09\n"
     ]
    }
   ],
   "source": [
    "# method 2\n",
    "# ğŸ“Œ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğğšğ«ğ¥ğ¢ğğ¬ğ­ ğšğ§ğ ğ¥ğšğ­ğğ¬ğ­ ğ­ğ¢ğ¦ğğ¬ğ­ğšğ¦ğ©ğ¬ ğ¢ğ§ ğ­ğ¡ğ ğğšğ­ğšğ¬ğğ­.\n",
    "max_time_stamp = data.withColumn(\"timestamp\", to_timestamp(data.timestamp))\n",
    "max_time_stamp.show()\n",
    "\n",
    "max_time = max_time_stamp.selectExpr(\"max(timestamp)\").first()[0]\n",
    "print(max_time)\n",
    "\n",
    "min_time = max_time_stamp.selectExpr(\"min(timestamp)\").first()[0]\n",
    "print(min_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "682f08b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|user_id|activity_count|\n",
      "+-------+--------------+\n",
      "|  user1|             3|\n",
      "|  user2|             2|\n",
      "|  user3|             1|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ ğ‚ğ¨ğ®ğ§ğ­ ğ­ğ¡ğ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğšğœğ­ğ¢ğ¯ğ¢ğ­ğ¢ğğ¬ ğ©ğğ« ğ®ğ¬ğğ«.\n",
    "\n",
    "count_data = data.groupBy(data.user_id).agg(count(\"*\").alias(\"activity_count\"))\n",
    "count_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37430a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|user_id|          timestamp|     next_timestamp|\n",
      "+-------+-------------------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|2023-08-21 12:15:00|\n",
      "|  user1|2023-08-21 12:15:00|2023-08-21 15:09:09|\n",
      "|  user1|2023-08-21 15:09:09|               null|\n",
      "|  user2|2023-08-21 11:30:00|2023-08-21 14:30:00|\n",
      "|  user2|2023-08-21 14:30:00|               null|\n",
      "|  user3|2023-08-21 13:45:09|               null|\n",
      "+-------+-------------------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- next_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Œ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¢ğ¦ğ ğğ®ğ«ğšğ­ğ¢ğ¨ğ§ ğ›ğğ­ğ°ğğğ§ ğœğ¨ğ§ğ¬ğğœğ®ğ­ğ¢ğ¯ğ ğšğœğ­ğ¢ğ¯ğ¢ğ­ğ¢ğğ¬ ğŸğ¨ğ« ğğšğœğ¡ ğ®ğ¬ğğ«.\n",
    "\n",
    "windowspec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "windowdata = data.withColumn(\"next_timestamp\", lead(\"timestamp\").over(windowspec))\n",
    "windowdata.show()\n",
    "\n",
    "windowdata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df7d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+----------+\n",
      "|user_id|          timestamp|     next_timestamp|difference|\n",
      "+-------+-------------------+-------------------+----------+\n",
      "|  user1|2023-08-21 10:09:09|2023-08-21 12:15:00|      null|\n",
      "|  user1|2023-08-21 12:15:00|2023-08-21 15:09:09|      null|\n",
      "|  user1|2023-08-21 15:09:09|               null|      null|\n",
      "|  user2|2023-08-21 11:30:00|2023-08-21 14:30:00|      null|\n",
      "|  user2|2023-08-21 14:30:00|               null|      null|\n",
      "|  user3|2023-08-21 13:45:09|               null|      null|\n",
      "+-------+-------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# windowdata.withColumn(\"difference\", minute(col(\"next_timestamp\"))- minute(col(\"timestamp\"))).show()\n",
    "\n",
    "windowdata.withColumn(\"difference\", col(\"timestamp\").cast(\"Long\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa5ab555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|user_id|          timestamp|\n",
      "+-------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|\n",
      "|  user2|2023-08-21 11:30:00|\n",
      "|  user1|2023-08-21 12:15:00|\n",
      "|  user3|2023-08-21 13:45:09|\n",
      "|  user2|2023-08-21 14:30:00|\n",
      "|  user1|2023-08-21 15:09:09|\n",
      "+-------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+-------+-------------------+\n",
      "|user_id|          timestamp|\n",
      "+-------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|\n",
      "|  user2|2023-08-21 11:30:00|\n",
      "|  user1|2023-08-21 12:15:00|\n",
      "|  user3|2023-08-21 13:45:09|\n",
      "|  user2|2023-08-21 14:30:00|\n",
      "|  user1|2023-08-21 15:09:09|\n",
      "+-------+-------------------+\n",
      "\n",
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Linked in code for above code.\n",
    "\n",
    "schema = [\"user_id\",\"timestamp\"]\n",
    "data=[\n",
    "(\"user1\", \"2023-08-21 10:09:09\"),\n",
    "(\"user2\", \"2023-08-21 11:30:00\"),\n",
    "(\"user1\", \"2023-08-21 12:15:00\"),\n",
    "(\"user3\", \"2023-08-21 13:45:09\"),\n",
    "(\"user2\", \"2023-08-21 14:30:00\"),\n",
    "(\"user1\", \"2023-08-21 15:09:09\")]\n",
    "\n",
    "# Create a Dataframe from the data and schema\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fd6dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max timestamp: 2023-08-21 15:09:09\n",
      "min_timestamp 2023-08-21 10:09:09\n",
      "+-------+--------------+\n",
      "|user_id|activity_count|\n",
      "+-------+--------------+\n",
      "|  user1|             3|\n",
      "|  user2|             2|\n",
      "|  user3|             1|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the earliest and latest timestamps in the dataset.\n",
    "max_timestamp=df.select(max(\"timestamp\")).first()[0]\n",
    "print(\"max timestamp:\",max_timestamp)\n",
    "min_timestamp=df.select(min(\"timestamp\")).first()[0]\n",
    "print(\"min_timestamp\",min_timestamp)\n",
    " \n",
    "# Count the number of activities per user.\n",
    "no_of_activities = df.groupBy(\"user_id\").agg(count(\"*\").alias(\"activity_count\"))\n",
    "no_of_activities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5433d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|user_id|          timestamp|     next_timestamp|\n",
      "+-------+-------------------+-------------------+\n",
      "|  user1|2023-08-21 10:09:09|2023-08-21 12:15:00|\n",
      "|  user1|2023-08-21 12:15:00|2023-08-21 15:09:09|\n",
      "|  user1|2023-08-21 15:09:09|               null|\n",
      "|  user2|2023-08-21 11:30:00|2023-08-21 14:30:00|\n",
      "|  user2|2023-08-21 14:30:00|               null|\n",
      "|  user3|2023-08-21 13:45:09|               null|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the time duration between consecutive activities for each user.\n",
    "window_spec = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "df_with_next_timestamp = df.withColumn(\"next_timestamp\", lead(\"timestamp\").over(window_spec))\n",
    " \n",
    "df_with_next_timestamp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e11308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+--------+\n",
      "|user_id|          timestamp|     next_timestamp|duration|\n",
      "+-------+-------------------+-------------------+--------+\n",
      "|  user1|2023-08-21 10:09:09|2023-08-21 12:15:00|    7551|\n",
      "|  user1|2023-08-21 12:15:00|2023-08-21 15:09:09|   10449|\n",
      "|  user1|2023-08-21 15:09:09|               null|    null|\n",
      "|  user2|2023-08-21 11:30:00|2023-08-21 14:30:00|   10800|\n",
      "|  user2|2023-08-21 14:30:00|               null|    null|\n",
      "|  user3|2023-08-21 13:45:09|               null|    null|\n",
      "+-------+-------------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Calculate the time duration between consecutive actions\n",
    "df_with_duration = df_with_next_timestamp.withColumn(\"duration\", (col(\"next_timestamp\").cast(\"long\") - col(\"timestamp\").cast(\"long\")))\n",
    "# Show the final DataFrame\n",
    "df_with_duration.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad714741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5953353",
   "metadata": {},
   "source": [
    "2. https://www.linkedin.com/feed/update/urn:li:activity:7105024721942933504?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7105024721942933504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0fbf5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7101175506905022464/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7101175506905022464%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# student_id, subject, score.\n",
    "\n",
    "# Sample Dataset :\n",
    "\n",
    "# student_id,subject,score\n",
    "# 1,Math,85\n",
    "# 2,Science,92\n",
    "# 3,Math,78\n",
    "# 4,English,88\n",
    "# 5,Science,95\n",
    "# 6,Math,90\n",
    "\n",
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ ğŸğ¨ğ« ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "# ğŸ”¹ ğˆğğğ§ğ­ğ¢ğŸğ² ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ¬ğœğ¨ğ«ğ ğšğ§ğ ğ¢ğ­ğ¬ ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğ¬ğ­ğ®ğğğ§ğ­ ğŸğ¨ğ« ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ¬ğ­ğ®ğğğ§ğ­ğ¬ ğ°ğ¡ğ¨ ğ­ğ¨ğ¨ğ¤ ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "# ğŸ”¹ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğ¬ğ®ğ›ğ£ğğœğ­(ğ¬) ğ°ğ¢ğ­ğ¡ ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "372779eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+\n",
      "|student_id|subject|score|\n",
      "+----------+-------+-----+\n",
      "|         1|   Math|   85|\n",
      "|         2|Science|   92|\n",
      "|         3|   Math|   78|\n",
      "|         4|English|   88|\n",
      "|         5|Science|   95|\n",
      "|         6|   Math|   90|\n",
      "+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"student_id\",\"subject\",\"score\"]\n",
    "data = [(1,\"Math\",85),\n",
    "(2,\"Science\",92),\n",
    "(3,\"Math\",78),\n",
    "(4,\"English\",88),\n",
    "(5,\"Science\",95),\n",
    "(6,\"Math\",90)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd2fccfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|subject|        avg_score|\n",
      "+-------+-----------------+\n",
      "|   Math|84.33333333333333|\n",
      "|Science|             93.5|\n",
      "|English|             88.0|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ ğŸğ¨ğ« ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "df1 = df.groupBy(\"subject\").agg(avg(\"score\").alias(\"avg_score\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a2d132c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|subject|max_score|\n",
      "+-------+---------+\n",
      "|   Math|       90|\n",
      "|Science|       95|\n",
      "|English|       88|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğˆğğğ§ğ­ğ¢ğŸğ² ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ¬ğœğ¨ğ«ğ ğšğ§ğ ğ¢ğ­ğ¬ ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğ¬ğ­ğ®ğğğ§ğ­ ğŸğ¨ğ« ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "df2 = df.groupBy(\"subject\").agg(max(\"score\").alias(\"max_score\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5806ffee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|subject|total_students|\n",
      "+-------+--------------+\n",
      "|   Math|             3|\n",
      "|Science|             2|\n",
      "|English|             1|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ¬ğ­ğ®ğğğ§ğ­ğ¬ ğ°ğ¡ğ¨ ğ­ğ¨ğ¨ğ¤ ğğšğœğ¡ ğ¬ğ®ğ›ğ£ğğœğ­.\n",
    "df3 = df.groupBy(\"subject\").agg(count(\"*\").alias(\"total_students\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "199bf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.0\n",
      "+----------+-------+-----+\n",
      "|student_id|subject|score|\n",
      "+----------+-------+-----+\n",
      "|         2|Science|   92|\n",
      "|         5|Science|   95|\n",
      "|         6|   Math|   90|\n",
      "+----------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğ¬ğ®ğ›ğ£ğğœğ­(ğ¬) ğ°ğ¢ğ­ğ¡ ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ.\n",
    "avger = df.selectExpr(\"avg(score)\").collect()[0][0]\n",
    "print(avger)\n",
    "df.filter(df.score > avger).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d498f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ”¹ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğ¬ğ®ğ›ğ£ğğœğ­(ğ¬) ğ°ğ¢ğ­ğ¡ ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğšğ¯ğğ«ğšğ ğ ğ¬ğœğ¨ğ«ğ.\n",
    "\n",
    "# df1.orderBy(desc(\"avg_score\")).limit(1).show()\n",
    "df1.orderBy(col(\"avg_score\").desc()).limit(1).collect()[0][1]\n",
    "# df1.orderBy(desc(\"avg_score\")).limit(1).collect()[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8cb618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb67bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7099432968850472960/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7099432968850472960%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Dataset: The dataset contains the following columns: order_id, customer_id, order_date, total_amount\n",
    "\n",
    "# order_id,customer_id,order_date,total_amount\n",
    "# 1,C101,2023-07-01,150\n",
    "# 2,C102,2023-07-02,200\n",
    "# 3,C101,2023-07-02,100\n",
    "# 4,C103,2023-07-03,300\n",
    "# 5,C102,2023-07-04,250\n",
    "# 6,C101,2023-07-05,120\n",
    "\n",
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ«ğğ¯ğğ§ğ®ğ ğ ğğ§ğğ«ğšğ­ğğ ğŸğ«ğ¨ğ¦ ğšğ¥ğ¥ ğ¨ğ«ğğğ«ğ¬.\n",
    "# ğŸ”¹ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğšğ¯ğğ«ğšğ ğ ğ¨ğ«ğğğ« ğšğ¦ğ¨ğ®ğ§ğ­.\n",
    "# ğŸ”¹ ğˆğğğ§ğ­ğ¢ğŸğ² ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ­ğ¨ğ­ğšğ¥ ğ¨ğ«ğğğ« ğšğ¦ğ¨ğ®ğ§ğ­ ğšğ§ğ ğ¢ğ­ğ¬ ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğœğ®ğ¬ğ­ğ¨ğ¦ğğ«.\n",
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ¨ğ«ğğğ«ğ¬ ğŸğ¨ğ« ğğšğœğ¡ ğœğ®ğ¬ğ­ğ¨ğ¦ğğ«."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57446a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+------------+\n",
      "|order_id|customer_id|order_date|total_amount|\n",
      "+--------+-----------+----------+------------+\n",
      "|       1|       C101|2023-07-01|         150|\n",
      "|       2|       C102|2023-07-02|         200|\n",
      "|       3|       C101|2023-07-02|         100|\n",
      "|       4|       C103|2023-07-03|         300|\n",
      "|       5|       C102|2023-07-04|         250|\n",
      "|       6|       C101|2023-07-05|         120|\n",
      "+--------+-----------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"order_id\",\"customer_id\",\"order_date\",\"total_amount\"]\n",
    "\n",
    "data = [(1,\"C101\",\"2023-07-01\",150),\n",
    "(2,\"C102\",\"2023-07-02\",200),\n",
    "(3,\"C101\",\"2023-07-02\",100),\n",
    "(4,\"C103\",\"2023-07-03\",300),\n",
    "(5,\"C102\",\"2023-07-04\",250),\n",
    "(6,\"C101\",\"2023-07-05\",120)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51815602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| sum|\n",
      "+----+\n",
      "|1120|\n",
      "+----+\n",
      "\n",
      "+------------------+\n",
      "|               avg|\n",
      "+------------------+\n",
      "|186.66666666666666|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ«ğğ¯ğğ§ğ®ğ ğ ğğ§ğğ«ğšğ­ğğ ğŸğ«ğ¨ğ¦ ğšğ¥ğ¥ ğ¨ğ«ğğğ«ğ¬\n",
    "df.selectExpr(\"sum(total_amount) as sum\").show()\n",
    "\n",
    "# ğŸ”¹ ğ…ğ¢ğ§ğ ğ­ğ¡ğ ğšğ¯ğğ«ğšğ ğ ğ¨ğ«ğğğ« ğšğ¦ğ¨ğ®ğ§ğ­.\n",
    "df.selectExpr(\"avg(total_amount) as avg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87d6741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_amount|\n",
      "+-----------+------------+\n",
      "|       C103|         300|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğˆğğğ§ğ­ğ¢ğŸğ² ğ­ğ¡ğ ğ¡ğ¢ğ ğ¡ğğ¬ğ­ ğ­ğ¨ğ­ğšğ¥ ğ¨ğ«ğğğ« ğšğ¦ğ¨ğ®ğ§ğ­ ğšğ§ğ ğ¢ğ­ğ¬ ğœğ¨ğ«ğ«ğğ¬ğ©ğ¨ğ§ğğ¢ğ§ğ  ğœğ®ğ¬ğ­ğ¨ğ¦ğğ«.\n",
    "# df.selectExpr(\"max(total_amount)\").show()\n",
    "\n",
    "df.orderBy(desc(\"total_amount\")).select(\"customer_id\", \"total_amount\").limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2697a4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|customer_id|count|\n",
      "+-----------+-----+\n",
      "|       C101|    3|\n",
      "|       C102|    2|\n",
      "|       C103|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¹ ğ‚ğšğ¥ğœğ®ğ¥ğšğ­ğ ğ­ğ¡ğ ğ­ğ¨ğ­ğšğ¥ ğ§ğ®ğ¦ğ›ğğ« ğ¨ğŸ ğ¨ğ«ğğğ«ğ¬ ğŸğ¨ğ« ğğšğœğ¡ ğœğ®ğ¬ğ­ğ¨ğ¦ğğ«.\n",
    "df.groupBy(\"customer_id\").agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0de8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68617282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7098861202759450624/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7098861202759450624%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Dataset: The dataset contains the following columns: employee_id, employee_name, department, salary.\n",
    "# Sample Dataset:\n",
    "# employee_id,employee_name,department,salary\n",
    "# 1,John Doe,Engineering,90000\n",
    "# 2,Jane Smith,Marketing,75000\n",
    "# 3,Michael Johnson,Engineering,105000\n",
    "# 4,Emily Davis,Marketing,80000\n",
    "# 5,Robert Brown,Engineering,95000\n",
    "# 6,Linda Wilson,HR,60000\n",
    "\n",
    "# Questions:\n",
    "# âº Calculate the total payroll cost for the company.\n",
    "# âº Find the average salary for each department.\n",
    "# âº Identify the highest-paid employee and their department.\n",
    "# âº Calculate the total number of employees in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3efebed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------+------+\n",
      "|employee_id|  employee_name| department|salary|\n",
      "+-----------+---------------+-----------+------+\n",
      "|          1|       John Doe|Engineering| 90000|\n",
      "|          2|     Jane Smith|  Marketing| 75000|\n",
      "|          3|Michael Johnson|Engineering|105000|\n",
      "|          4|    Emily Davis|  Marketing| 80000|\n",
      "|          5|   Robert Brown|Engineering| 95000|\n",
      "|          6|   Linda Wilson|         HR| 60000|\n",
      "+-----------+---------------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"employee_id\",\"employee_name\",\"department\",\"salary\"]\n",
    "data = [(1,\"John Doe\",\"Engineering\",90000),\n",
    "(2,\"Jane Smith\",\"Marketing\",75000),\n",
    "(3,\"Michael Johnson\",\"Engineering\",105000),\n",
    "(4,\"Emily Davis\",\"Marketing\",80000),\n",
    "(5,\"Robert Brown\",\"Engineering\",95000),\n",
    "(6,\"Linda Wilson\",\"HR\",60000)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65906a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|   sum|\n",
      "+------+\n",
      "|505000|\n",
      "+------+\n",
      "\n",
      "+-----------+-----------------+\n",
      "| department|              avg|\n",
      "+-----------+-----------------+\n",
      "|Engineering|96666.66666666667|\n",
      "|  Marketing|          77500.0|\n",
      "|         HR|          60000.0|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âº Calculate the total payroll cost for the company.\n",
    "df.select(sum(\"salary\").alias(\"sum\")).show()\n",
    "# âº Find the average salary for each department.\n",
    "df.groupBy(\"department\").agg(avg(\"salary\").alias(\"avg\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaf76f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|  employee_name| department|\n",
      "+---------------+-----------+\n",
      "|Michael Johnson|Engineering|\n",
      "+---------------+-----------+\n",
      "\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|Engineering|    3|\n",
      "|  Marketing|    2|\n",
      "|         HR|    1|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# âº Identify the highest-paid employee and their department.\n",
    "df.orderBy(desc(\"salary\")).select(\"employee_name\", \"department\").limit(1).show()\n",
    "\n",
    "# âº Calculate the total number of employees in each department.\n",
    "df.groupBy(\"department\").agg(count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcdd833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2ac8798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7098006304308297729/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7098006304308297729%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# The dataset contains the following columns: product_id, product_name, category, price, quantity_sold.\n",
    "\n",
    "# product_id,product_name,category,price,quantity_sold\n",
    "# 1,Product A,Electronics,500,100\n",
    "# 2,Product B,Clothing,50,200\n",
    "# 3,Product C,Electronics,800,50\n",
    "# 4,Product D,Beauty,30,300\n",
    "# 5,Product E,Clothing,75,150\n",
    "\n",
    "# ğŸ”¸ Calculate the total revenue generated from all sales.\n",
    "# ğŸ”¸ Find the top 5 best-selling products based on the quantity sold.\n",
    "# ğŸ”¸ Calculate the average price of products in each category.\n",
    "# ğŸ”¸ Identify the category with the highest total revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c65284a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n",
      "|product_id|product_name|   category|price|quantity_sold|\n",
      "+----------+------------+-----------+-----+-------------+\n",
      "|         1|   Product A|Electronics|  500|          100|\n",
      "|         2|   Product B|   Clothing|   50|          200|\n",
      "|         3|   Product C|Electronics|  800|           50|\n",
      "|         4|   Product D|     Beauty|   30|          300|\n",
      "|         5|   Product E|   Clothing|   75|          150|\n",
      "+----------+------------+-----------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"product_id\",\"product_name\",\"category\",\"price\",\"quantity_sold\"]\n",
    "data = [\n",
    "(1,\"Product A\",\"Electronics\",500,100),\n",
    "(2,\"Product B\",\"Clothing\",50,200),\n",
    "(3,\"Product C\",\"Electronics\",800,50),\n",
    "(4,\"Product D\",'Beauty',30,300),\n",
    "(5,\"Product E\",\"Clothing\",75,150)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1de321c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+-------+\n",
      "|product_id|product_name|   category|price|quantity_sold|revenue|\n",
      "+----------+------------+-----------+-----+-------------+-------+\n",
      "|         1|   Product A|Electronics|  500|          100|    600|\n",
      "|         2|   Product B|   Clothing|   50|          200|    250|\n",
      "|         3|   Product C|Electronics|  800|           50|    850|\n",
      "|         4|   Product D|     Beauty|   30|          300|    330|\n",
      "|         5|   Product E|   Clothing|   75|          150|    225|\n",
      "+----------+------------+-----------+-----+-------------+-------+\n",
      "\n",
      "2255\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¸ Calculate the total revenue generated from all sales.\n",
    "\n",
    "# revenue = df.withColumn(\"revenue\", (df.price + df.quantity_sold)).agg(sum(col(\"revenue\"))).collect()[0][0]\n",
    "# print(revenue)\n",
    "\n",
    "revenue1 = df.withColumn(\"revenue\", (df.price + df.quantity_sold))\n",
    "revenue1.show()\n",
    "revenue2 = revenue1.selectExpr(\"sum(revenue)\").collect()[0][0]\n",
    "print(revenue2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65800ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----+-------------+\n",
      "|product_id|product_name|   category|price|quantity_sold|\n",
      "+----------+------------+-----------+-----+-------------+\n",
      "|         4|   Product D|     Beauty|   30|          300|\n",
      "|         2|   Product B|   Clothing|   50|          200|\n",
      "|         5|   Product E|   Clothing|   75|          150|\n",
      "|         1|   Product A|Electronics|  500|          100|\n",
      "|         3|   Product C|Electronics|  800|           50|\n",
      "+----------+------------+-----------+-----+-------------+\n",
      "\n",
      "+-----------+---------+\n",
      "|   category|avg_price|\n",
      "+-----------+---------+\n",
      "|Electronics|    650.0|\n",
      "|   Clothing|     62.5|\n",
      "|     Beauty|     30.0|\n",
      "+-----------+---------+\n",
      "\n",
      "+-----------+\n",
      "|   category|\n",
      "+-----------+\n",
      "|Electronics|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¸ Find the top 5 best-selling products based on the quantity sold.\n",
    "df.sort(desc(df.quantity_sold)).limit(5).show()\n",
    "\n",
    "# ğŸ”¸ Calculate the average price of products in each category.\n",
    "df.groupBy(df.category).agg(avg(df.price).alias(\"avg_price\")).show()\n",
    "\n",
    "# ğŸ”¸ Identify the category with the highest total revenue.\n",
    "revenue1.sort(desc(revenue1.revenue)).select(\"category\").limit(1).show()\n",
    "# print(revenue1.sort(desc(revenue1.revenue)).select(\"category\").first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b63b5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7097033199851081728/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7097033199851081728%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Sample CSV file named customer_purchases.csv with the following columns:\n",
    "\n",
    "# customer_id,purchase_amount,purchase_date\n",
    "# 1,100,2023-01-15\n",
    "# 2,150,2023-02-20\n",
    "# 1,200,2023-03-10\n",
    "# 3,50,2023-04-05\n",
    "# 2,120,2023-05-15\n",
    "# 1,300,2023-06-25\n",
    "\n",
    "# Find the customer with the highest total purchase amount.â”\n",
    "# Practise this question and boost your pyspark skill. ğŸ’¡ğŸ’¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5347847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|customer_id|purchase_amount|purchase_date|\n",
      "+-----------+---------------+-------------+\n",
      "|          1|            100|   2023-01-15|\n",
      "|          2|            150|   2023-02-20|\n",
      "|          1|            200|   2023-03-10|\n",
      "|          3|             50|   2023-04-05|\n",
      "|          2|            120|   2023-05-15|\n",
      "|          1|            300|   2023-06-25|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = [\"customer_id\",\"purchase_amount\",\"purchase_date\"]\n",
    "data = [(1,100,\"2023-01-15\"),\n",
    "(2,150,\"2023-02-20\"),\n",
    "(1,200,\"2023-03-10\"),\n",
    "(3,50,\"2023-04-05\"),\n",
    "(2,120,\"2023-05-15\"),\n",
    "(1,300,\"2023-06-25\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ced574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|max_amount|\n",
      "+-----------+----------+\n",
      "|          1|       300|\n",
      "+-----------+----------+\n",
      "\n",
      "+-----------+---------------+-------------+\n",
      "|customer_id|purchase_amount|purchase_date|\n",
      "+-----------+---------------+-------------+\n",
      "|          1|            300|   2023-06-25|\n",
      "+-----------+---------------+-------------+\n",
      "\n",
      "+-----------+---------------+-------------+\n",
      "|customer_id|purchase_amount|purchase_date|\n",
      "+-----------+---------------+-------------+\n",
      "|          1|            300|   2023-06-25|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the customer with the highest total purchase amount.â”\n",
    "df.groupBy(df.customer_id).agg(max(df.purchase_amount).alias(\"max_amount\")).sort(desc(col(\"max_amount\"))).limit(1).show()\n",
    "\n",
    "df.orderBy(desc(df.purchase_amount)).limit(1).show()\n",
    "\n",
    "row_value = df.orderBy(desc(df.purchase_amount)).limit(1)\n",
    "row_value.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8369add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7107898245263065088/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7107898245263065088%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Question : How can you create a DataFrame?\n",
    "# You can create a DataFrame from an existing RDD using the createDataFrame method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "120e8111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "|Alice| 25|\n",
      "| katy| 32|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "| katy| 32|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "| katy| 32|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "|Alice| 25|\n",
      "|  Bob| 30|\n",
      "| katy| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7108381435144671232/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7108381435144671232%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Question : handling row duplication in PySpark DataFrames! ğŸ“Š\n",
    "\n",
    "# 1. Dropping Duplicates:\n",
    "# The simplest approach is to drop duplicate rows based on a subset of columns. This can be done using the dropDuplicates() method. Here's how:\n",
    "\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Alice\", 25),(\"katy\",32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Drop duplicates based on 'Name' and 'Age'\n",
    "deduplicated_df = df.dropDuplicates(subset=[\"Name\", \"Age\"])\n",
    "deduplicated_df.show()\n",
    "\n",
    "# 2. Using df.distinct()\n",
    "\n",
    "df.distinct().show()\n",
    "\n",
    "# 3. Using Window Functions:\n",
    "# You can also leverage window functions to assign row numbers and then filter out duplicate rows.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "window_spec = Window.partitionBy(\"Name\", \"Age\").orderBy(\"Name\")\n",
    "\n",
    "# Add a row number column\n",
    "df_with_row_number = df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter out duplicate rows\n",
    "deduplicated_windowfn_df = df_with_row_number.filter(df_with_row_number.row_number == 1).drop(\"row_number\")\n",
    "\n",
    "# Show result\n",
    "deduplicated_windowfn_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfd753d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7108623025624809472/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7108623025624809472%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28ca55b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|       City|Temperature_F|\n",
      "+-----------+-------------+\n",
      "|   New York|         95.0|\n",
      "|Los Angeles|         78.5|\n",
      "|    Chicago|         82.0|\n",
      "+-----------+-------------+\n",
      "\n",
      "+-----------+-------------+-------------+\n",
      "|       City|Temperature_F|Temperature_C|\n",
      "+-----------+-------------+-------------+\n",
      "|   New York|         95.0|         35.0|\n",
      "|Los Angeles|         78.5|        25.83|\n",
      "|    Chicago|         82.0|        27.78|\n",
      "+-----------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7108743821303193600/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7108743821303193600%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# PySpark UDF with an example.\n",
    "# PySpark User-Defined Functions (UDFs) are your secret weapon for custom transformations on DataFrame columns.\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "data = [(\"New York\", 95.0), (\"Los Angeles\", 78.5), (\"Chicago\", 82.0)]\n",
    "columns = [\"City\", \"Temperature_F\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "def fahrenheit_to_celsius(fahrenheit_temp):\n",
    "  celsius_temp = ((fahrenheit_temp - 32) * 5 / 9)\n",
    "  return round(celsius_temp, 2)\n",
    "\n",
    "# Register UDF\n",
    "convert_to_celsius_udf = udf(fahrenheit_to_celsius, FloatType())\n",
    "\n",
    "# Apply UDF to create a new column\n",
    "# df_with_celsius = df.withColumn(\"Temperature_C\", convert_to_celsius_udf(df.Temperature_F))\n",
    "\n",
    "# df_with_celsius = df.selectExpr(\"City\", \"Temperature_F\", \"((Temperature_F - 32) * 5 / 9) as Temperature_C\")\n",
    "df_with_celsius = df.withColumn(\"Temperature_C\", round(((df.Temperature_F - 32) * 5 / 9), 2))\n",
    "\n",
    "df_with_celsius.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c7418f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n",
      "| id|   name| birthdate|salary|\n",
      "+---+-------+----------+------+\n",
      "|  1|  Alice|2021-01-15|   100|\n",
      "|  2|    Bob|2021-03-20|   200|\n",
      "|  3|Charlie|2021-02-10|   150|\n",
      "+---+-------+----------+------+\n",
      "\n",
      "+---------+--------------+\n",
      "|full_name|updated_salary|\n",
      "+---------+--------------+\n",
      "|    Alice|         110.0|\n",
      "|      Bob|         220.0|\n",
      "|  Charlie|         165.0|\n",
      "+---------+--------------+\n",
      "\n",
      "+------+----------------+\n",
      "|salary|increased_salary|\n",
      "+------+----------------+\n",
      "|   100|           150.0|\n",
      "|   200|           300.0|\n",
      "|   150|           225.0|\n",
      "+------+----------------+\n",
      "\n",
      "+-------+----------+--------------+\n",
      "|   name|birth_year|     name_year|\n",
      "+-------+----------+--------------+\n",
      "|  Alice|      2021|  Alice - 2021|\n",
      "|    Bob|      2021|    Bob - 2021|\n",
      "|Charlie|      2021|Charlie - 2021|\n",
      "+-------+----------+--------------+\n",
      "\n",
      "+-------+---------------+\n",
      "|   name|salary_category|\n",
      "+-------+---------------+\n",
      "|  Alice|     Low Salary|\n",
      "|    Bob|    High Salary|\n",
      "|Charlie|     Low Salary|\n",
      "+-------+---------------+\n",
      "\n",
      "+-------+-------------+\n",
      "|   name|double_salary|\n",
      "+-------+-------------+\n",
      "|  Alice|        100.0|\n",
      "|    Bob|        200.0|\n",
      "|Charlie|        150.0|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7109430095672479744/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7109430095672479744%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "#\n",
    "\n",
    "data = [(1, \"Alice\", \"2021-01-15\", 100),\n",
    "  (2, \"Bob\", \"2021-03-20\", 200),\n",
    "  (3, \"Charlie\", \"2021-02-10\", 150)]\n",
    "\n",
    "columns = [\"id\", \"name\", \"birthdate\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# ğŸ”¹Selecting Columns with Alias:\n",
    "df.selectExpr(\"name AS full_name\", \"salary * 1.1 AS updated_salary\").show()\n",
    "\n",
    "# ğŸ”¹Mathematical Transformations:\n",
    "df.selectExpr(\"salary\", \"salary * 1.5 AS increased_salary\").show()\n",
    "\n",
    "# ğŸ”¹String Manipulation:\n",
    "df.selectExpr(\"name\", \"substring(birthdate, 1, 4) AS birth_year\", \"concat(name, ' - ', birth_year) AS name_year\").show()\n",
    "\n",
    "# ğŸ”¹Conditional Expressions:\n",
    "df.selectExpr(\"name\", \"CASE WHEN salary > 150 THEN 'High Salary' ELSE 'Low Salary' END AS salary_category\").show()\n",
    "\n",
    "# ğŸ”¹Type Casting:\n",
    "df.selectExpr(\"name\", \"cast(salary AS double) AS double_salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4fcf1679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16, 25]\n",
      "[('B', 13), ('C', 15), ('A', 30)]\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7099953413865582592/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7099953413865582592%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# The map function in Spark is your go-to tool for transforming your data. It takes an input RDD (Resilient Distributed Dataset) and applies a transformation function to each element, producing a new RDD. Think of it as a way to modify, clean, or reshape your data to fit your analysis needs. Whether you're changing data types, extracting features, or even just performing calculations, map is your Swiss Army knife.\n",
    "\n",
    "# # Spark Context Setup\n",
    "# from pyspark import SparkContext\n",
    "# sc = SparkContext(\"local\", \"MapFunctionExample\")\n",
    "\n",
    "# # Sample RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# # Applying the Map Transformation\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# # Collecting Results\n",
    "result = squared_rdd.collect()\n",
    "print(result) # Output: [1, 4, 9, 16, 25]\n",
    "\n",
    "# ğŸŸ¢ ReduceByKey Function: Aggregating with Style\n",
    "# When it comes to aggregating data, the reduceByKey function is your ace in the hole. It's perfect for performing operations on key-value pairs, where values for the same key are grouped together and combined using a provided function. This can be incredibly useful for tasks like summing values for each key or finding maximum values.\n",
    "\n",
    "# # Sample Key-Value RDD\n",
    "data = [(\"A\", 10), (\"B\", 5), (\"A\", 20), (\"B\", 8), (\"C\", 15)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# # Applying the ReduceByKey Transformation\n",
    "sum_by_key = rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# # Collecting Results\n",
    "result = sum_by_key.collect()\n",
    "print(result) # Output: [('A', 30), ('B', 13), ('C', 15)]\n",
    "\n",
    "\n",
    "# ğŸ’¡ Pro Tip: Remember that both map and reduceByKey functions are part of Spark's resilient distributed processing model, allowing you to efficiently handle large datasets across a cluster of machines.\n",
    "# So there you have it, the dynamic duo of Spark functionsâ€”map for flexible data transformation and reduceByKey for effective aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "843e3688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+-----------------+\n",
      "|product_id|quantity_sold|quantity_returned|\n",
      "+----------+-------------+-----------------+\n",
      "|         1|         10.0|              2.0|\n",
      "|         2|          8.0|              NaN|\n",
      "|         3|         12.0|              3.0|\n",
      "|         4|          NaN|              5.0|\n",
      "+----------+-------------+-----------------+\n",
      "\n",
      "+----------+-------------------------+-----------------------------+-----------------+\n",
      "|product_id|quantity_sold_withoutNull|quantity_returned_withoutNull|net_quantity_sold|\n",
      "+----------+-------------------------+-----------------------------+-----------------+\n",
      "|         1|                     10.0|                          2.0|              8.0|\n",
      "|         2|                      8.0|                          0.0|              8.0|\n",
      "|         3|                     12.0|                          3.0|              9.0|\n",
      "|         4|                      0.0|                          5.0|             -5.0|\n",
      "+----------+-------------------------+-----------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110200923343261696/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110200923343261696%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Create a DataFrame with sales data\n",
    "data = [(1,10.0, 2.0), (2,8.0, float('nan')), (3,12.0, 3.0), (4,float('nan'), 5.0)]\n",
    "df = spark.createDataFrame(data, [\"product_id\",\"quantity_sold\", \"quantity_returned\"])\n",
    "df.show()\n",
    "\n",
    "# Calculate net quantity sold, handling NaN values\n",
    "net_sales_df = df.withColumn(\"quantity_sold_withoutNull\", nanvl(df[\"quantity_sold\"], lit(0.0)))\\\n",
    "    .withColumn(\"quantity_returned_withoutNull\", nanvl(df[\"quantity_returned\"], lit(0.0)))\n",
    "    \n",
    "result_df = net_sales_df.withColumn(\"net_quantity_sold\", \\\n",
    "                        net_sales_df[\"quantity_sold_withoutNull\"] - net_sales_df[\"quantity_returned_withoutNull\"])\n",
    "\n",
    "# Show the result\n",
    "result_df.drop('quantity_sold','quantity_returned').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f7239a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110228203385884672/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110228203385884672%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Write a pyspark query to \n",
    "# 1. convert product_name in lowercase without leading or trailing white spaces.\n",
    "# 2. set sale_date in the format ('YYYY-MM').\n",
    "# 3. calculate total the number of times the product was sold in this month.\n",
    "# Return the result table ordered by product_name in ascending order. In case of a tie, order it by sale_date in ascending order.\n",
    "\n",
    "# The query result format is in the following example.\n",
    "\n",
    "# Sales\n",
    "# +---------+--------------+------------+\n",
    "# | sale_id | product_name | sale_date  |\n",
    "# +---------+--------------+------------+\n",
    "# | 1       | LCPHONE      | 2000-01-16 |\n",
    "# | 2       | LCPhone      | 2000-01-17 |\n",
    "# | 3       | LcPhOnE      | 2000-02-18 |\n",
    "# | 4       | LCKeyCHAiN   | 2000-02-19 |\n",
    "# | 5       | LCKeyChain   | 2000-02-28 |\n",
    "# | 6       | Matryoshka   | 2000-03-31 |\n",
    "# +---------+--------------+------------+\n",
    "\n",
    "# Result table:\n",
    "# +--------------+-----------+-------+\n",
    "# | product_name | sale_date | total |\n",
    "# +--------------+-----------+-------+\n",
    "# | lckeychain   | 2000-02   | 2     |\n",
    "# | lcphone      | 2000-01   | 2     |\n",
    "# | lcphone      | 2000-02   | 1     |\n",
    "# | matryoshka   | 2000-03   | 1     |\n",
    "# +--------------+-----------+-------+\n",
    "\n",
    "# Pyspark Solution:\n",
    "\n",
    "# #Transform the product_name & sale_date columns\n",
    "# sales_df = sales_df.select(\"sale_id\", trim(lower(col(\"product_name\"))).alias(\"product_name\"),date_format(col(\"sale_date\"), \"YYYY-MM\").alias(\"sale_date\"))\n",
    "\n",
    "# # Calculate the total number of times the product was sold using a window function\n",
    "# saleWindow = Window.partitionBy(\"product_name\", \"sale_date\")\n",
    "# sales_df_final = sales_df.withColumn(\"saleCount\", count(\"sale_id\").over(saleWindow))\n",
    "\n",
    "# # Select and show the result columns\n",
    "# result_df = sales_df_final.select(\"product_name\", \"sale_date\", \"saleCount\") \\\n",
    "#     .orderBy(\"product_name\", \"sale_date\")\n",
    "\n",
    "# result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0dc1e640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|   LCPHONE  |2000-01-16|\n",
      "|      2|   LCPhone  |2000-01-17|\n",
      "|      3|   LcPhOnE  |2000-02-18|\n",
      "|      4| LCKeyCHAiN |2000-02-19|\n",
      "|      5|  LCKeyChain|2000-02-28|\n",
      "|      6|  Matryoshka|2000-03-31|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"sale_id\" , \"product_name\" , \"sale_date\"]\n",
    "\n",
    "data = [(1, \"LCPHONE  \",\"2000-01-16\"),\n",
    "(2,\"LCPhone  \",\"2000-01-17\"),\n",
    "(3,\"LcPhOnE  \",\"2000-02-18\"),\n",
    "(4,\"LCKeyCHAiN \",\"2000-02-19\"),\n",
    "(5,\"LCKeyChain\",\"2000-02-28\"),\n",
    "(6,\"Matryoshka\",\"2000-03-31\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "789ea90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|     lcphone|2000-01-16|\n",
      "|      2|     lcphone|2000-01-17|\n",
      "|      3|     lcphone|2000-02-18|\n",
      "|      4|  lckeychain|2000-02-19|\n",
      "|      5|  lckeychain|2000-02-28|\n",
      "|      6|  matryoshka|2000-03-31|\n",
      "+-------+------------+----------+\n",
      "\n",
      "+-------+------------+----------+----------+\n",
      "|sale_id|product_name| sale_date|sales_data|\n",
      "+-------+------------+----------+----------+\n",
      "|      1|     lcphone|2000-01-16|   2000-01|\n",
      "|      2|     lcphone|2000-01-17|   2000-01|\n",
      "|      3|     lcphone|2000-02-18|   2000-02|\n",
      "|      4|  lckeychain|2000-02-19|   2000-02|\n",
      "|      5|  lckeychain|2000-02-28|   2000-02|\n",
      "|      6|  matryoshka|2000-03-31|   2000-03|\n",
      "+-------+------------+----------+----------+\n",
      "\n",
      "+-------+------------+----------+----------+----------+\n",
      "|sale_id|product_name| sale_date|sales_data|salesCount|\n",
      "+-------+------------+----------+----------+----------+\n",
      "|      4|  lckeychain|2000-02-19|   2000-02|         2|\n",
      "|      5|  lckeychain|2000-02-28|   2000-02|         2|\n",
      "|      1|     lcphone|2000-01-16|   2000-01|         2|\n",
      "|      2|     lcphone|2000-01-17|   2000-01|         2|\n",
      "|      3|     lcphone|2000-02-18|   2000-02|         1|\n",
      "|      6|  matryoshka|2000-03-31|   2000-03|         1|\n",
      "+-------+------------+----------+----------+----------+\n",
      "\n",
      "+------------+----------+----------+\n",
      "|product_name| sale_date|salesCount|\n",
      "+------------+----------+----------+\n",
      "|  lckeychain|2000-02-19|         2|\n",
      "|  lckeychain|2000-02-28|         2|\n",
      "|     lcphone|2000-01-16|         2|\n",
      "|     lcphone|2000-01-17|         2|\n",
      "|     lcphone|2000-02-18|         1|\n",
      "|  matryoshka|2000-03-31|         1|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. convert product_name in lowercase without leading or trailing white spaces.\n",
    "\n",
    "trimdata = df.withColumn(\"product_name\", trim(lower(df.product_name)))\n",
    "trimdata.show()\n",
    "\n",
    "# 2. set sale_date in the format ('YYYY-MM').\n",
    "datadf = trimdata.withColumn(\"sales_data\", date_format(col(\"sale_date\"), \"yyyy-MM\"))\n",
    "# trimdata.selectExpr(\"date_format(sale_date, 'YYYY-MM')\")\n",
    "datadf.show()\n",
    "\n",
    "# 3. calculate total the number of times the product was sold in this month.\n",
    "window = Window.partitionBy(\"product_name\", \"sales_data\")\n",
    "sales_final = datadf.withColumn(\"salesCount\", count(\"sale_id\").over(window)).orderBy(\"product_name\", \"sale_date\")\n",
    "sales_final.show()\n",
    "\n",
    "result_df = sales_final.select(\"product_name\", \"sale_date\", \"salesCount\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30e6a12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+\n",
      "|  name|dept_name|Salary|\n",
      "+------+---------+------+\n",
      "| James|    Sales|  2000|\n",
      "|  sofy|    Sales|  3000|\n",
      "| Laren|    Sales|  4000|\n",
      "|  Kiku|    Sales|  5000|\n",
      "|   Sam|  Finance|  6000|\n",
      "|Samuel|  Finance|  7000|\n",
      "|  Yash|  Finance|  8000|\n",
      "| Rabin|  Finance|  9000|\n",
      "|Lukasz|Marketing| 10000|\n",
      "| Jolly|Marketing| 11000|\n",
      "|Mausam|Marketing| 12000|\n",
      "| Lamba|Marketing| 13000|\n",
      "|Jogesh|       HR| 14000|\n",
      "| Mannu|       HR| 15000|\n",
      "|Sylvia|       HR| 16000|\n",
      "|  Sama|       HR| 17000|\n",
      "+------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110465175123632128/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110465175123632128%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Cummulative is nothing but a \"Running Sum\" of salary column.\n",
    "\n",
    "data= [(\"James\", \"Sales\", 2000),(\"sofy\", \"Sales\", 3000),(\"Laren\", \"Sales\", 4000),(\"Kiku\", \"Sales\", 5000),\n",
    "       (\"Sam\", \"Finance\", 6000),(\"Samuel\", \"Finance\", 7000),(\"Yash\", \"Finance\", 8000),\n",
    "       (\"Rabin\", \"Finance\", 9000),(\"Lukasz\", \"Marketing\", 10000),(\"Jolly\", \"Marketing\", 11000),\n",
    "       (\"Mausam\", \"Marketing\", 12000),(\"Lamba\", \"Marketing\", 13000),(\"Jogesh\", \"HR\", 14000),\n",
    "       (\"Mannu\", \"HR\", 15000),(\"Sylvia\", \"HR\", 16000),(\"Sama\", \"HR\", 17000)]\n",
    "\n",
    "#Schema\n",
    "# emp_schema = StructType([StructField(\"name\",StringType()), StructField(\"dept_name\",StringType()),StructField(\"Salary\", StringType())])\n",
    "\n",
    "emp_schema = [\"name\", \"dept_name\", \"Salary\"]\n",
    "employees_Salary_df = spark.createDataFrame(data,emp_schema)\n",
    "\n",
    "# employees_Salary_df.show(truncate=False)\n",
    "employees_Salary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8c1cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+-------+\n",
      "|name  |dept_name|Salary|cum_sum|\n",
      "+------+---------+------+-------+\n",
      "|Sam   |Finance  |6000  |6000   |\n",
      "|Samuel|Finance  |7000  |13000  |\n",
      "|Yash  |Finance  |8000  |21000  |\n",
      "|Rabin |Finance  |9000  |30000  |\n",
      "|Jogesh|HR       |14000 |14000  |\n",
      "|Mannu |HR       |15000 |29000  |\n",
      "|Sylvia|HR       |16000 |45000  |\n",
      "|Sama  |HR       |17000 |62000  |\n",
      "|Lukasz|Marketing|10000 |10000  |\n",
      "|Jolly |Marketing|11000 |21000  |\n",
      "|Mausam|Marketing|12000 |33000  |\n",
      "|Lamba |Marketing|13000 |46000  |\n",
      "|James |Sales    |2000  |2000   |\n",
      "|sofy  |Sales    |3000  |5000   |\n",
      "|Laren |Sales    |4000  |9000   |\n",
      "|Kiku  |Sales    |5000  |14000  |\n",
      "+------+---------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = (Window.partitionBy(\"dept_name\").orderBy(\"salary\"))\n",
    "# window_spec = (Window.partitionBy(\"dept_name\").orderBy(\"salary\").rowsBetween(Window.unboundedPreceding,0))\n",
    "\n",
    "#Cummulative salary\n",
    "cum_sum_df = employees_Salary_df.withColumn('cum_sum',sum(col(\"Salary\").cast(\"Integer\")).over(window_spec))\n",
    "\n",
    "cum_sum_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90025847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110563318309810176/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110563318309810176%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# when to use broadcast function in pyspark?\n",
    "\n",
    "# # Load the sales_data and customer_info DataFrames\n",
    "# sales_data = spark.read.csv(\"dbfs:/FileStore/sales_data.csv\", header=True, inferSchema=True)\n",
    "# customer_info = spark.read.csv(\"dbfs:/FileStore/customer_info.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# # Optimize the join by broadcasting the smaller DataFrame (customer_info)\n",
    "# enriched_sales_data = sales_data.join(broadcast(customer_info), \"customer_id\", \"left\")\n",
    "\n",
    "# # Show the resulting DataFrame\n",
    "# enriched_sales_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9276f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110963608590708737/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110963608590708737%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Assume you have a DataFrame named 'employee' with columns 'emp_id', 'name', 'dept_id', and 'salary'\n",
    "# Read your data or replace this with your actual DataFrame creation\n",
    "\n",
    "# # Add a row number partitioned by department and ordered by salary in descending order\n",
    "\n",
    "# window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
    "# employee_with_rank = employee.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# # employee_with_rank = employee.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "\n",
    "# # Filter for 10th highest salary\n",
    "# tenth_highest_salary = employee_with_rank.filter(col(\"rank\") == 10)\n",
    "\n",
    "# # Filter for top 6 salaries\n",
    "# top_six_salaries = employee_with_rank.filter(col(\"rank\").between(1, 6))\n",
    "\n",
    "# # Show the results\n",
    "# print(\"10th Highest Salary in each department:\")\n",
    "# tenth_highest_salary.show()\n",
    "\n",
    "# print(\"Top 6 Salaries in each department:\")\n",
    "# top_six_salaries.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f42dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| ID| Name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2| null|\n",
      "|  3|  Bob|\n",
      "+---+-----+\n",
      "\n",
      "+---+-------+\n",
      "| ID|   Name|\n",
      "+---+-------+\n",
      "|  1|  Alice|\n",
      "|  2|Unknown|\n",
      "|  3|    Bob|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7111012314748719104/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7111012314748719104%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "# 1. **`na.fill()` - PySpark (Spark DataFrame):**\n",
    "\n",
    "data = [(1, \"Alice\"), (2, None), (3, \"Bob\")]\n",
    "columns = [\"ID\", \"Name\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Fill null values in the \"Name\" column with \"Unknown\"\n",
    "df.na.fill(\"Unknown\", subset=[\"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc409aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|    name|product_id|units|\n",
      "+--------+----------+-----+\n",
      "|LCHouse1|         1|    1|\n",
      "|LCHouse1|         2|   10|\n",
      "|LCHouse1|         3|    5|\n",
      "|LCHouse2|         1|    2|\n",
      "|LCHouse2|         2|    2|\n",
      "|LCHouse3|         4|    1|\n",
      "+--------+----------+-----+\n",
      "\n",
      "+----------+------------+-----+------+------+\n",
      "|product_id|product_name|Width|Length|Height|\n",
      "+----------+------------+-----+------+------+\n",
      "|         1|       LC-TV|    5|    50|    40|\n",
      "|         2| LC-KeyChain|    5|     5|     5|\n",
      "|         3|    LC-Phone|    2|    10|    10|\n",
      "|         4|  LC-T-Shirt|    4|    10|    20|\n",
      "+----------+------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110958019600912385/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110958019600912385%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema1 = [\"name\",\"product_id\",\"units\"]\n",
    "data1 = [(\"LCHouse1\",1,1),\n",
    "(\"LCHouse1\",2,10),\n",
    "(\"LCHouse1\",3,5),\n",
    "(\"LCHouse2\",1,2),\n",
    "(\"LCHouse2\",2,2),\n",
    "(\"LCHouse3\",4,1)]\n",
    "\n",
    "Warehouse = spark.createDataFrame(data1, schema=schema1)\n",
    "Warehouse.show()\n",
    "\n",
    "schema2 = [\"product_id\",\"product_name\",\"Width\",\"Length\",\"Height\"]\n",
    "\n",
    "data2 = [(1,\"LC-TV\",5,50,40),\n",
    "(2,\"LC-KeyChain\",5,5,5),\n",
    "(3,\"LC-Phone\",2,10,10),\n",
    "(4,\"LC-T-Shirt\",4,10,20)]\n",
    "\n",
    "Products = spark.createDataFrame(data2, schema=schema2)\n",
    "Products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62423434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+------+------+------+\n",
      "|product_id|product_name|Width|Length|Height|volume|\n",
      "+----------+------------+-----+------+------+------+\n",
      "|         1|       LC-TV|    5|    50|    40| 10000|\n",
      "|         2| LC-KeyChain|    5|     5|     5|   125|\n",
      "|         3|    LC-Phone|    2|    10|    10|   200|\n",
      "|         4|  LC-T-Shirt|    4|    10|    20|   800|\n",
      "+----------+------------+-----+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "volumedata = Products.withColumn(\"volume\", col(\"Width\")*col(\"Length\")*col(\"Height\"))\n",
    "volumedata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8d67edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+----------+------------+-----+------+------+------+\n",
      "|    name|product_id|units|product_id|product_name|Width|Length|Height|volume|\n",
      "+--------+----------+-----+----------+------------+-----+------+------+------+\n",
      "|LCHouse1|         1|    1|         1|       LC-TV|    5|    50|    40| 10000|\n",
      "|LCHouse2|         1|    2|         1|       LC-TV|    5|    50|    40| 10000|\n",
      "|LCHouse1|         2|   10|         2| LC-KeyChain|    5|     5|     5|   125|\n",
      "|LCHouse2|         2|    2|         2| LC-KeyChain|    5|     5|     5|   125|\n",
      "|LCHouse1|         3|    5|         3|    LC-Phone|    2|    10|    10|   200|\n",
      "|LCHouse3|         4|    1|         4|  LC-T-Shirt|    4|    10|    20|   800|\n",
      "+--------+----------+-----+----------+------------+-----+------+------+------+\n",
      "\n",
      "+--------+------+\n",
      "|    name|volume|\n",
      "+--------+------+\n",
      "|LCHouse1| 10325|\n",
      "|LCHouse2| 10125|\n",
      "|LCHouse3|   800|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = Warehouse.join(volumedata, Warehouse.product_id == volumedata.product_id, \"inner\")\n",
    "joined_data.show()\n",
    "\n",
    "finaldf = joined_data.groupBy(\"name\").agg(sum(col(\"volume\")).alias(\"volume\")).orderBy(\"name\").select(\"name\", \"volume\")\n",
    "finaldf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a8377e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da575b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7106478906605535232/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7106478906605535232%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# How to choose number of executors required for our cluster ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5a9b8df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|category|value|\n",
      "+--------+-----+\n",
      "|       A|   10|\n",
      "|       B|   20|\n",
      "|       A|   30|\n",
      "|       B|   40|\n",
      "|       A|   50|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7111703324025880576/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7111703324025880576%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data = [(\"A\", 10),\n",
    "    (\"B\", 20),\n",
    "    (\"A\", 30),\n",
    "    (\"B\", 40),\n",
    "    (\"A\", 50)]\n",
    "\n",
    "columns = [\"category\", \"value\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14a455d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|            10|\n",
      "|       A|   30|            40|\n",
      "|       A|   50|            90|\n",
      "|       B|   20|            20|\n",
      "|       B|   40|            60|\n",
      "+--------+-----+--------------+\n",
      "\n",
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|            90|\n",
      "|       A|   30|            80|\n",
      "|       A|   50|            50|\n",
      "|       B|   20|            60|\n",
      "|       B|   40|            40|\n",
      "+--------+-----+--------------+\n",
      "\n",
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|            90|\n",
      "|       A|   30|            90|\n",
      "|       A|   50|            90|\n",
      "|       B|   20|            60|\n",
      "|       B|   40|            60|\n",
      "+--------+-----+--------------+\n",
      "\n",
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|            40|\n",
      "|       A|   30|            80|\n",
      "|       A|   50|            50|\n",
      "|       B|   20|            60|\n",
      "|       B|   40|            40|\n",
      "+--------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"value\")\n",
    "\n",
    "# Calculate a cumulative sum considering all rows from the start of the partition up to the current row\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))).show()\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.currentRow, Window.unboundedFollowing))).show()\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))).show()\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.currentRow,1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "726236a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|            10|\n",
      "|       B|   20|            30|\n",
      "|       A|   30|            60|\n",
      "|       B|   40|           100|\n",
      "|       A|   50|           150|\n",
      "+--------+-----+--------------+\n",
      "\n",
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|           150|\n",
      "|       B|   20|           140|\n",
      "|       A|   30|           120|\n",
      "|       B|   40|            90|\n",
      "|       A|   50|            50|\n",
      "+--------+-----+--------------+\n",
      "\n",
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|           150|\n",
      "|       B|   20|           150|\n",
      "|       A|   30|           150|\n",
      "|       B|   40|           150|\n",
      "|       A|   50|           150|\n",
      "+--------+-----+--------------+\n",
      "\n",
      "+--------+-----+--------------+\n",
      "|category|value|cumulative_sum|\n",
      "+--------+-----+--------------+\n",
      "|       A|   10|            30|\n",
      "|       B|   20|            50|\n",
      "|       A|   30|            70|\n",
      "|       B|   40|            90|\n",
      "|       A|   50|            50|\n",
      "+--------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a window specification\n",
    "window_spec = Window.orderBy(\"value\")\n",
    "\n",
    "# Calculate a cumulative sum considering all rows from the start of the partition up to the current row\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))).show()\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.currentRow, Window.unboundedFollowing))).show()\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))).show()\n",
    "df.withColumn(\"cumulative_sum\", sum(\"value\").over(window_spec.rowsBetween(Window.currentRow,1))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61a3dbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inferSchema=True\n",
    "\n",
    "p = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",True)\\\n",
    "    .load(location + \"p.csv\")\n",
    "# p.show()\n",
    "p.printSchema()\n",
    "\n",
    "\n",
    "# inferSchema=True, by default inferschema is false\n",
    "p = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\",False)\\\n",
    "    .load(location + \"p.csv\")\n",
    "p.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0738effd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3b2c2a2\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7111570839325589504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7111570839325589504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "# 1.Given a String and return its transformed form such that it is formed using the count of continuous occurrence of characters.\n",
    "\n",
    "# Sample Input 1: aaabbcca\n",
    "# Sample Output 1: a3b2c2a1\n",
    "\n",
    "# Sample Input 2: aabbbccaa\n",
    "# Sample Output 2: a2b3c2a2\n",
    "\n",
    "# Sample Input 3: abcd\n",
    "# Sample Output 3: a1b1c1d1\n",
    "    \n",
    "#     Code:-\n",
    "def transform_string(input_str):\n",
    "  result = \"\"\n",
    "  i = 0\n",
    "   \n",
    "  while i < len(input_str):\n",
    "    count = 1\n",
    "    \n",
    "    # Count continuous occurrences of the current character\n",
    "    while i + 1 < len(input_str) and input_str[i] == input_str[i + 1]:\n",
    "      count += 1\n",
    "      i += 1\n",
    "        \n",
    "    # Append the character and its count to the result\n",
    "    result += input_str[i] + str(count)\n",
    "    i += 1\n",
    "   \n",
    "  return result\n",
    "print(transform_string(\"aaabbccaa\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5b9d0496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|        d1|\n",
      "+----------+\n",
      "|2015-04-08|\n",
      "|2015-04-09|\n",
      "|2015-04-10|\n",
      "|2015-04-11|\n",
      "|2015-04-12|\n",
      "|2015-04-13|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- d1: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|        d1|        d2|\n",
      "+----------+----------+\n",
      "|2015-04-08|2023-12-20|\n",
      "|2015-04-09|2023-12-20|\n",
      "|2015-04-10|2023-12-20|\n",
      "|2015-04-11|2023-12-20|\n",
      "|2015-04-12|2023-12-20|\n",
      "|2015-04-13|2023-12-20|\n",
      "+----------+----------+\n",
      "\n",
      "+----+\n",
      "|diff|\n",
      "+----+\n",
      "|3178|\n",
      "|3177|\n",
      "|3176|\n",
      "|3175|\n",
      "|3174|\n",
      "|3173|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7111884516591357954/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7111884516591357954%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Explain how and when to use datediff() in pyspark?\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "data = [\n",
    "  ('2015-04-08',),\n",
    "  ('2015-04-09',),\n",
    "  ('2015-04-10',),\n",
    "  ('2015-04-11',),\n",
    "  ('2015-04-12',),\n",
    "  ('2015-04-13',)\n",
    "]\n",
    "columns = ['d1']\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"d1\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "# Convert 'd1' to DateType\n",
    "df = df.withColumn(\"d1\", df[\"d1\"].cast(DateType()))\n",
    "\n",
    "# Create a new DataFrame with the current date in 'd2'\n",
    "df_with_current_date = df.withColumn(\"d2\", lit(current_date()))\n",
    "df_with_current_date.show()\n",
    "\n",
    "# Calculate the difference in days\n",
    "df_with_current_date. select(datediff(df_with_current_date.d2, df_with_current_date.d1).alias('diff')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e77e81d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+\n",
      "|      full_name|user_id|\n",
      "+---------------+-------+\n",
      "|       John Doe|      1|\n",
      "|     Jane Smith|      2|\n",
      "|Michael Johnson|      3|\n",
      "+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110106658030141441/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110106658030141441%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "data = [  {\"user_id\": 1, \"full_name\": \"John Doe\"},  {\"user_id\": 2, \"full_name\": \"Jane Smith\"},  {\"user_id\": 3, \"full_name\": \"Michael Johnson\"}]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "67335762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------+-------+-------+\n",
      "|      full_name|user_id|  first|   last|\n",
      "+---------------+-------+-------+-------+\n",
      "|       John Doe|      1|   John|    Doe|\n",
      "|     Jane Smith|      2|   Jane|  Smith|\n",
      "|Michael Johnson|      3|Michael|Johnson|\n",
      "+---------------+-------+-------+-------+\n",
      "\n",
      "+---------------+-------+-------+-------+------------+\n",
      "|      full_name|user_id|  first|   last|name_initial|\n",
      "+---------------+-------+-------+-------+------------+\n",
      "|       John Doe|      1|   John|    Doe|          JD|\n",
      "|     Jane Smith|      2|   Jane|  Smith|          JS|\n",
      "|Michael Johnson|      3|Michael|Johnson|          MJ|\n",
      "+---------------+-------+-------+-------+------------+\n",
      "\n",
      "+---------------+-------+------------+\n",
      "|      full_name|user_id|name_initial|\n",
      "+---------------+-------+------------+\n",
      "|       John Doe|      1|          JD|\n",
      "|     Jane Smith|      2|          JS|\n",
      "|Michael Johnson|      3|          MJ|\n",
      "+---------------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitdata = df.withColumn(\"first\", split(\"full_name\", \" \")[0]).withColumn(\"last\", split(\"full_name\", \" \")[1])\n",
    "splitdata.show()\n",
    "\n",
    "initial_data = splitdata.withColumn(\"name_initial\", expr(\"concat(substring(first, 1, 1), substring(last, 1, 1))\"))\n",
    "initial_data.show()\n",
    "initial_data.select(\"full_name\", \"user_id\", \"name_initial\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9261aa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               email|\n",
      "+--------------------+\n",
      "|  john.doe@email.com|\n",
      "|jane.smith@email.com|\n",
      "|alice.wonder@emai...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7110482297321062402/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110482297321062402%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "#  Extract the first and last names from these emails, which are structured with dots as delimiters. \n",
    "data = [\n",
    "  {\"email\": \"john.doe@email.com\"},\n",
    "  {\"email\": \"jane.smith@email.com\"},\n",
    "  {\"email\": \"alice.wonder@email.com\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n",
    "\n",
    "# # Extract first and last names from email addresses\n",
    "# df_extracted = df.withColumn(\"first_name\", F.split(F.col(\"email\"), \"\\\\.\")[0])\n",
    "# df_extracted = df_extracted.withColumn(\"last_name\", F.split(F.col(\"email\"), \"\\\\.\")[1])\n",
    "# df_extracted.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8053f962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|               email|        name|\n",
      "+--------------------+------------+\n",
      "|  john.doe@email.com|    john.doe|\n",
      "|jane.smith@email.com|  jane.smith|\n",
      "|alice.wonder@emai...|alice.wonder|\n",
      "+--------------------+------------+\n",
      "\n",
      "+--------------------+------------+-----+------+\n",
      "|               email|        name|first|  last|\n",
      "+--------------------+------------+-----+------+\n",
      "|  john.doe@email.com|    john.doe| john|   doe|\n",
      "|jane.smith@email.com|  jane.smith| jane| smith|\n",
      "|alice.wonder@emai...|alice.wonder|alice|wonder|\n",
      "+--------------------+------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "namedf = df.withColumn(\"name\", split(\"email\", \"@\")[0])\n",
    "namedf.show()\n",
    "\n",
    "namedf.withColumn(\"first\", split(\"name\", \"\\\\.\")[0]).withColumn(\"last\", split(\"name\", \"\\\\.\")[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6e84a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7107967859015770112/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7107967859015770112%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "330874d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7111946335590744064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7111946335590744064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# â“Write a query to get ğ‘»ğ’‰ğ’† ğ’…ğ’Šğ’‡ğ’‡ğ’†ğ’“ğ’†ğ’ğ’„ğ’† ğ’ğ’‡ ğ’‚ğ’ğ’ğ’–ğ’ğ’• ğ’ƒğ’†ğ’•ğ’˜ğ’†ğ’†ğ’ ğ’‚ğ’‘ğ’‘ğ’ğ’†ğ’” & ğ’ğ’“ğ’‚ğ’ğ’ˆğ’†ğ’” ğ’‡ğ’ğ’“ ğ’†ğ’‚ğ’„ğ’‰ ğ’…ğ’‚ğ’š.\n",
    "\n",
    "\n",
    "# ğŸ“Œ DDL Script for Table creation & loading the data\n",
    "# CREATE TABLE NamasteSQL.Sales_tbl(\n",
    "# sales_date DATE,\n",
    "# fruits VARCHAR(25),\n",
    "# sold_num SMALLINT\n",
    "# );\n",
    "\n",
    "# INSERT INTO NamasteSQL.Sales_tbl(sales_date, fruits, sold_num) VALUES\n",
    "# ('2020-05-01', 'apples', 10),\n",
    "# ('2020-05-01', 'oranges', 8),\n",
    "# ('2020-05-02', 'apples', 15),\n",
    "# ('2020-05-02', 'oranges', 15),\n",
    "# ('2020-05-03', 'apples', 20),\n",
    "# ('2020-05-03', 'oranges', 0),\n",
    "# ('2020-05-04', 'apples', 15),\n",
    "# ('2020-05-04', 'oranges', 16);\n",
    "\n",
    "# Select sales_date, diff\n",
    "# From (Select \n",
    "# sales_date, sold_num, Lead(sold_num,1) Over (partition by sales_date order by sales_date) As new_sale_num, (sold_num - Lead(sold_num,1) Over (partition by sales_date order by sales_date)) As diff\n",
    "# From \n",
    "# Sales_tbl) x\n",
    "# where diff Is not Null;\n",
    "\n",
    "# select sales_date, \n",
    "# (min(case when fruits = 'apples' then sold_num end)- min(case when fruits = 'oranges' then sold_num end)) as diff\n",
    "# from Sales_tbl\n",
    "# group by sales_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7890abb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n",
      "|sales_date| fruits|sold_num|\n",
      "+----------+-------+--------+\n",
      "|2020-05-01| apples|      10|\n",
      "|2020-05-01|oranges|       8|\n",
      "|2020-05-02| apples|      15|\n",
      "|2020-05-02|oranges|      15|\n",
      "|2020-05-03| apples|      20|\n",
      "|2020-05-03|oranges|       0|\n",
      "|2020-05-04| apples|      15|\n",
      "|2020-05-04|oranges|      16|\n",
      "+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"sales_date\", \"fruits\", \"sold_num\"]\n",
    "data = [('2020-05-01', 'apples', 10),\n",
    "('2020-05-01', 'oranges', 8),\n",
    "('2020-05-02', 'apples', 15),\n",
    "('2020-05-02', 'oranges', 15),\n",
    "('2020-05-03', 'apples', 20),\n",
    "('2020-05-03', 'oranges', 0),\n",
    "('2020-05-04', 'apples', 15),\n",
    "('2020-05-04', 'oranges', 16)]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df1 = df.orderBy(\"sales_date\", \"fruits\") # for safer side\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ab4fec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n",
      "|sales_date|diff|\n",
      "+----------+----+\n",
      "|2020-05-01|   2|\n",
      "|2020-05-02|   0|\n",
      "|2020-05-03|  20|\n",
      "|2020-05-04|  -1|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using Sql\n",
    "df.createOrReplaceTempView(\"Sales_tbl\")\n",
    "spark.sql(\"select sales_date, (min(case when fruits = 'apples' then sold_num end)- min(case when fruits = 'oranges' then sold_num end)) as diff from Sales_tbl group by sales_date;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21a2f4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+----+\n",
      "|sales_date|fruits|sold_num|lead|\n",
      "+----------+------+--------+----+\n",
      "|2020-05-01|apples|      10|   8|\n",
      "|2020-05-02|apples|      15|  15|\n",
      "|2020-05-03|apples|      20|   0|\n",
      "|2020-05-04|apples|      15|  16|\n",
      "+----------+------+--------+----+\n",
      "\n",
      "+----------+----+\n",
      "|sales_date|diff|\n",
      "+----------+----+\n",
      "|2020-05-01|   2|\n",
      "|2020-05-02|   0|\n",
      "|2020-05-03|  20|\n",
      "|2020-05-04|  -1|\n",
      "+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using DSL\n",
    "windowSpec = Window.partitionBy(\"sales_date\").orderBy(\"fruits\")\n",
    "\n",
    "lead_data = df.withColumn(\"lead\", lead(\"sold_num\").over(windowSpec)).filter(col(\"lead\").isNotNull())\n",
    "lead_data.show()\n",
    "\n",
    "lead_data.withColumn(\"diff\", col(\"sold_num\")-col(\"lead\")).select(\"sales_date\", \"diff\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04764311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7112005314522083328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7112005314522083328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Unlock the real-time potential of your data. Explore Apache Spark's Structured Streaming today!\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# import tempfile\n",
    "# import time\n",
    "\n",
    "# with tempfile.TemporaryDirectory() as d, tempfile.TemporaryDirectory() as cp:\n",
    "#  df = spark.readStream.format(\"rate\").load()\n",
    "#  q = df.writeStream.format(\"csv\").option(\"checkpointLocation\", cp).start(d)\n",
    "#  time.sleep(10)\n",
    "#  q.stop()\n",
    "#  spark.read.schema(\"timestamp TIMESTAMP, value STRING\").csv(d).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1cff80d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|log_id|user_id|action|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|    101| login|2023-09-05 08:30:00|\n",
      "|     2|    102| click|2023-09-06 12:45:00|\n",
      "|     3|    101| click|2023-09-07 14:15:00|\n",
      "|     4|    103| login|2023-09-08 09:00:00|\n",
      "|     5|    102|logout|2023-09-09 17:30:00|\n",
      "|     6|    101| click|2023-09-10 11:20:00|\n",
      "|     7|    103| click|2023-09-11 10:15:00|\n",
      "|     8|    102| click|2023-09-12 13:10:00|\n",
      "+------+-------+------+-------------------+\n",
      "\n",
      "+-------+-------------+\n",
      "|user_id|actions_count|\n",
      "+-------+-------------+\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7112246904075276289/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7112246904075276289%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "data = [\n",
    "  (1, 101, 'login', '2023-09-05 08:30:00'),\n",
    "  (2, 102, 'click', '2023-09-06 12:45:00'),\n",
    "  (3, 101, 'click', '2023-09-07 14:15:00'),\n",
    "  (4, 103, 'login', '2023-09-08 09:00:00'),\n",
    "  (5, 102, 'logout', '2023-09-09 17:30:00'),\n",
    "  (6, 101, 'click', '2023-09-10 11:20:00'),\n",
    "  (7, 103, 'click', '2023-09-11 10:15:00'),\n",
    "  (8, 102, 'click', '2023-09-12 13:10:00')\n",
    "]\n",
    "\n",
    "columns = [\"log_id\", \"user_id\", \"action\", \"timestamp\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Calculate the number of actions performed by each user in the last 7 days\n",
    "result = df.select(\"user_id\", \"timestamp\") \\\n",
    "  .filter(datediff(current_date(), col(\"timestamp\")) <= 60) \\\n",
    "  .groupBy(\"user_id\") \\\n",
    "  .count() \\\n",
    "  .withColumnRenamed(\"count\", \"actions_count\")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef3175e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+\n",
      "|sales_rep|      date|sales_amount|\n",
      "+---------+----------+------------+\n",
      "|     John|2023-07-01|        5000|\n",
      "|     Jane|2023-07-01|        7000|\n",
      "|     John|2023-08-01|        6000|\n",
      "|     Jane|2023-08-01|        8000|\n",
      "|     John|2023-09-01|        5500|\n",
      "|     Jane|2023-09-01|        7500|\n",
      "+---------+----------+------------+\n",
      "\n",
      "+---------+----------+------------+--------------+--------------------+----------------+\n",
      "|sales_rep|date      |sales_amount|quarterly_rank|previous_month_sales|next_month_sales|\n",
      "+---------+----------+------------+--------------+--------------------+----------------+\n",
      "|Jane     |2023-07-01|7000        |1             |null                |8000            |\n",
      "|Jane     |2023-08-01|8000        |2             |7000                |7500            |\n",
      "|Jane     |2023-09-01|7500        |3             |8000                |null            |\n",
      "|John     |2023-07-01|5000        |1             |null                |6000            |\n",
      "|John     |2023-08-01|6000        |2             |5000                |5500            |\n",
      "|John     |2023-09-01|5500        |3             |6000                |null            |\n",
      "+---------+----------+------------+--------------+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7107214877533835264/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7107214877533835264%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "    \n",
    "data = [(\"John\", \"2023-07-01\", 5000),(\"Jane\", \"2023-07-01\", 7000),(\"John\", \"2023-08-01\", 6000),\n",
    "        (\"Jane\", \"2023-08-01\", 8000),(\"John\", \"2023-09-01\", 5500),(\"Jane\", \"2023-09-01\", 7500)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"sales_rep\", \"date\", \"sales_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"sales_rep\").orderBy(\"date\")\n",
    "\n",
    "# Apply the PySpark Dense_RANK, LAG, and LEAD functions\n",
    "df_ranked = df.withColumn(\"quarterly_rank\", dense_rank().over(window_spec))\n",
    "df_analyzed = df_ranked.withColumn(\"previous_month_sales\", lag(\"sales_amount\").over(window_spec))\n",
    "df_analyzed = df_analyzed.withColumn(\"next_month_sales\", lead(\"sales_amount\").over(window_spec))\n",
    "df_analyzed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "431c5a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|                 bio|               posts|user_id|\n",
      "+--------------------+--------------------+-------+\n",
      "|Contact John at j...|[Received an emai...|      1|\n",
      "|Email: support@ex...|[Join our mailing...|      2|\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7109377885852868608/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7109377885852868608%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# ğŸ” Navigating PySpark: Parsing Nested Strings for Data!âš™ï¸\n",
    "# ğŸŒ± Example: Extracting Valuable Email Addresses from Complex Text ğŸš€\n",
    "\n",
    "\n",
    "# Sample data with nested strings\n",
    "data = [\n",
    "  {\n",
    "    \"user_id\": 1,\n",
    "    \"bio\": \"Contact John at john@email.com for inquiries.\",\n",
    "    \"posts\": [\n",
    "      \"Received an email from jane@email.com today.\",\n",
    "      \"Check out our website: www.example.com.\"\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"user_id\": 2,\n",
    "    \"bio\": \"Email: support@example.com\",\n",
    "    \"posts\": [\n",
    "      \"Join our mailing list for updates: news@example.com\",\n",
    "      \"Follow us on Twitter: @example\"\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "582c6bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------------------------------------------------------------------------+-------+----------------------------------------------------------------------------------------------------+\n",
      "|bio                                          |posts                                                                                  |user_id|all_emails                                                                                          |\n",
      "+---------------------------------------------+---------------------------------------------------------------------------------------+-------+----------------------------------------------------------------------------------------------------+\n",
      "|Contact John at john@email.com for inquiries.|[Received an email from jane@email.com today., Check out our website: www.example.com.]|1      |[[Received, an, email, from, jane@email.com, today.], [Check, out, our, website:, www.example.com.]]|\n",
      "|Email: support@example.com                   |[Join our mailing list for updates: news@example.com, Follow us on Twitter: @example]  |2      |[[Join, our, mailing, list, for, updates:, news@example.com], [Follow, us, on, Twitter:, @example]] |\n",
      "+---------------------------------------------+---------------------------------------------------------------------------------------+-------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF to extract emails using regex\n",
    "def extract_emails(text):\n",
    "  return re.findall(r'\\S+@\\S+', text)\n",
    "\n",
    "# Extract emails from nested strings\n",
    "df_extracted = df.withColumn(\"all_emails\", expr(\"TRANSFORM(posts, x -> split(x, ' '))\"))\n",
    "# df_extracted = df_extracted.withColumn(\"emails\", expr(\"TRANSFORM(all_emails, x -> TRANSFORM(filter(extract_emails(x), e -> e LIKE '%@%'), y -> regexp_replace(y, '[^a-zA-Z0-9@.]', '')) )\"))\n",
    "\n",
    "df_extracted.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8c21bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+\n",
      "|customer_id|purchase_date|purchase_amount|\n",
      "+-----------+-------------+---------------+\n",
      "|         C1|   2023-07-01|          150.0|\n",
      "|         C1|   2023-07-05|          200.0|\n",
      "|         C2|   2023-07-02|          120.0|\n",
      "|         C2|   2023-07-03|           80.0|\n",
      "+-----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/iamanuragsharma17_onestepanalytics-pyspark-customerbehavioranalysis-activity-7123526446731386880-5QpO\n",
    "\n",
    "data = [(\"C1\", \"2023-07-01\", 150.00),\n",
    " (\"C1\", \"2023-07-05\", 200.00),\n",
    " (\"C2\", \"2023-07-02\", 120.00),\n",
    " (\"C2\", \"2023-07-03\", 80.00)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"customer_id\", \"purchase_date\", \"purchase_amount\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3366cf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------------+---------------+-------------------+--------------------+\n",
      "|customer_id|purchase_date|purchase_amount|total_purchases|first_purchase_date|recent_purchase_date|\n",
      "+-----------+-------------+---------------+---------------+-------------------+--------------------+\n",
      "|C1         |2023-07-01   |150.0          |150.0          |2023-07-01         |2023-07-01          |\n",
      "|C1         |2023-07-05   |200.0          |350.0          |2023-07-01         |2023-07-05          |\n",
      "|C2         |2023-07-02   |120.0          |120.0          |2023-07-02         |2023-07-02          |\n",
      "|C2         |2023-07-03   |80.0           |200.0          |2023-07-02         |2023-07-03          |\n",
      "+-----------+-------------+---------------+---------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"purchase_date\")\n",
    "\n",
    "# Apply Aggregate and First functions\n",
    "df_with_aggregate = df.withColumn(\"total_purchases\", sum(\"purchase_amount\").over(window_spec))\n",
    "df_with_aggregate = df_with_aggregate.withColumn(\"first_purchase_date\", first(\"purchase_date\").over(window_spec))\n",
    "df_with_aggregate = df_with_aggregate.withColumn(\"recent_purchase_date\", last(\"purchase_date\").over(window_spec))\n",
    "\n",
    "df_with_aggregate.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52d23e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|employee_id|team_id|\n",
      "+-----------+-------+\n",
      "|          1|      8|\n",
      "|          2|      8|\n",
      "|          3|      8|\n",
      "|          4|      7|\n",
      "|          5|      9|\n",
      "|          6|      9|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7113486107366854656/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7113486107366854656%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "columns = [\"employee_id\",\"team_id\"]\n",
    "data = [(1,8), (2,8), (3,8), (4,7), (5,9), (6,9)]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "afc40498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|team_id|employee_id's|\n",
      "+-------+-------------+\n",
      "|      8|            3|\n",
      "|      7|            1|\n",
      "|      9|            2|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = df.groupBy(\"team_id\").agg(count(\"employee_id\").alias(\"employee_id's\"))\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a941b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|team_id1|sum|\n",
      "+--------+---+\n",
      "|       8| 24|\n",
      "|       7|  7|\n",
      "|       9| 18|\n",
      "+--------+---+\n",
      "\n",
      "+-----------+-------+--------+---+\n",
      "|employee_id|team_id|team_id1|sum|\n",
      "+-----------+-------+--------+---+\n",
      "|          1|      8|       8| 24|\n",
      "|          2|      8|       8| 24|\n",
      "|          3|      8|       8| 24|\n",
      "|          4|      7|       7|  7|\n",
      "|          5|      9|       9| 18|\n",
      "|          6|      9|       9| 18|\n",
      "+-----------+-------+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.groupBy(\"team_id\").agg(sum('team_id').alias(\"sum\")).withColumnRenamed(\"team_id\", \"team_id1\")\n",
    "df1.show()\n",
    "\n",
    "joined_data = df.join(df1, df.team_id == df1.team_id1, \"inner\")\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "040b8099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+--------+---+---------+\n",
      "|employee_id|team_id|team_id1|sum|team_size|\n",
      "+-----------+-------+--------+---+---------+\n",
      "|          1|      8|       8| 24|        3|\n",
      "|          2|      8|       8| 24|        3|\n",
      "|          3|      8|       8| 24|        3|\n",
      "|          4|      7|       7|  7|        1|\n",
      "|          5|      9|       9| 18|        2|\n",
      "|          6|      9|       9| 18|        2|\n",
      "+-----------+-------+--------+---+---------+\n",
      "\n",
      "+-----------+---------+\n",
      "|employee_id|team_size|\n",
      "+-----------+---------+\n",
      "|          1|        3|\n",
      "|          2|        3|\n",
      "|          3|        3|\n",
      "|          4|        1|\n",
      "|          5|        2|\n",
      "|          6|        2|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jj = joined_data.withColumn(\"team_size\", (joined_data.sum/joined_data.team_id).cast(\"integer\"))\n",
    "jj.show()\n",
    "\n",
    "jj.select(\"employee_id\", \"team_size\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92367bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e6b8f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_ = [\"cust_id\",\"first_name\",\"last_name\",\"cust_order\",\"cust_status\",\"full_name\",\"net_salary\",\"age\"]\n",
    "data = [\n",
    "(12,\"Emma\",\"Wilson\",8,\"Inactive\",\"Emma Wilson\",10024,31),\n",
    "(14,\"Isabella\",\"Thomas\" ,7,\"Inactive\" ,\"Isabella Thomas\",10021,35),\n",
    "(9,\"James\",\"Miller\",6,\"Active\",\"James Miller\",10018,37),\n",
    "(19,\"Daniel\",\"Clark\",1,\"Active\",\"Daniel Clark\",10009,37),\n",
    "(4,\"Emily\",\"Williams\" ,2,\"Active\",\"Emily Williams\",10014,38),\n",
    "(5,\"William\" ,\"Brown\",7,\"Inactive\",\"William Brown\" ,10001,40),\n",
    "(8,\"Sarah\",\"Davis\",9,\"Inactive\",\"Sarah Davis\",10017,44),\n",
    "(10,\"Olivial\",\"Moore\",5,\"Inactive\",\"Olivia Moore\",10026,44),\n",
    "(18,\"Ava\",\"Lee\",6,\"Inactive\",\"Ava Lee\",10022,47),\n",
    "(11,\"John\" ,\"Doe\",5,\"Active\",\"John Doe\",10019,48),\n",
    "(13,\"Matthew\",\"Evans\" ,3,\"Active\",\"Matthew Evans\" ,10006,48),\n",
    "(15,\"Joseph\" ,\"Harris\",10,'Active',\"Joseph Harris\",10017,48),\n",
    "(21,\"Jane\",\"Smith\",8,\"Inactive\",\"Jane Smith\",10009,49)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dace8e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+----------+-----------+---------------+----------+---+\n",
      "|cust_id|first_name|last_name|cust_order|cust_status|      full_name|net_salary|age|\n",
      "+-------+----------+---------+----------+-----------+---------------+----------+---+\n",
      "|     12|      Emma|   Wilson|         8|   Inactive|    Emma Wilson|     10024| 31|\n",
      "|     14|  Isabella|   Thomas|         7|   Inactive|Isabella Thomas|     10021| 35|\n",
      "|      9|     James|   Miller|         6|     Active|   James Miller|     10018| 37|\n",
      "|     19|    Daniel|    Clark|         1|     Active|   Daniel Clark|     10009| 37|\n",
      "|      4|     Emily| Williams|         2|     Active| Emily Williams|     10014| 38|\n",
      "|      5|   William|    Brown|         7|   Inactive|  William Brown|     10001| 40|\n",
      "|      8|     Sarah|    Davis|         9|   Inactive|    Sarah Davis|     10017| 44|\n",
      "|     10|   Olivial|    Moore|         5|   Inactive|   Olivia Moore|     10026| 44|\n",
      "|     18|       Ava|      Lee|         6|   Inactive|        Ava Lee|     10022| 47|\n",
      "|     11|      John|      Doe|         5|     Active|       John Doe|     10019| 48|\n",
      "|     13|   Matthew|    Evans|         3|     Active|  Matthew Evans|     10006| 48|\n",
      "|     15|    Joseph|   Harris|        10|     Active|  Joseph Harris|     10017| 48|\n",
      "|     21|      Jane|    Smith|         8|   Inactive|     Jane Smith|     10009| 49|\n",
      "+-------+----------+---------+----------+-----------+---------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7112367704598085632/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7112367704598085632%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Define the external source and target paths\n",
    "source_path = 'your_path' # Update with your actual source file path\n",
    "target_path = \"your_output_path\" # Update with your desired target file path\n",
    "\n",
    "# Extract: Read data from an external CSV file\n",
    "# df = spark.read.csv(source_path, header=True, schema = 'cust_id int, first_name string,last_name string,cust_order int,cust_status string')\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()\n",
    "\n",
    "# Transformation 1: Concatenate First and Last Names\n",
    "df = df.withColumn(\"full_name\", concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")))\n",
    "\n",
    "# Transformation 2: Calculate Net Salary (subtract 10% as taxes)\n",
    "df = df.withColumn(\"net_salary\", floor(lit(10000) + rand() * lit(50)) )\n",
    "#adding age column\n",
    "df = df.withColumn(\"age\", floor(lit(20) + rand() * lit(31)))\n",
    "\n",
    "# Transformation 3: Filter by Age (age >= 30)\n",
    "df = df.filter(col(\"age\") >= 30)\n",
    "\n",
    "# Transformation 4: Group by Age and Calculate Average Salary\n",
    "avg_salary_by_age = df.groupBy(\"age\").agg({\"net_salary\": \"avg\"}).withColumnRenamed(\"avg(salary)\", \"avg_salary\")\n",
    "# avg_salary_by_age.show()\n",
    "\n",
    "# Transformation 5: Sort by Age\n",
    "df = df.orderBy(\"age\")\n",
    "# df.show()\n",
    "\n",
    "# # Save the transformed data to an external CSV file\n",
    "# df.write.csv(target_path, mode=\"overwrite\", header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8a65dd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.linkedin.com/feed/update/urn:li:activity:7112971681165438976/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7112971681165438976%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "# ğŸ”¹LAG Function:\n",
    "# The LAG function allows you to access the value of a column from the previous row within the current row's result set.\n",
    "# It is useful for calculating differences between consecutive rows.\n",
    "\n",
    "# Example:\n",
    "# SELECT OrderDate, SalesAmount,\n",
    "#  LAG(SalesAmount) OVER (ORDER BY OrderDate) AS PreviousSales\n",
    "#  FROM SalesData;\n",
    "\n",
    "# ğŸ”¹LEAD Function:\n",
    "# The LEAD function is similar to LAG but allows you to access the value of a column from the next row within the current row's result set.\n",
    "\n",
    "# SELECT\n",
    "#  OrderDate, SalesAmount,\n",
    "#  LEAD(SalesAmount) OVER (ORDER BY OrderDate) AS NextSales\n",
    "#  FROM SalesData;\n",
    "\n",
    "# ğŸ”¹RANK and DENSE_RANK Functions:\n",
    "# The RANK and DENSE_RANK functions assign a ranking to each row based on the specified ordering within the result set.\n",
    "# RANK assigns the same rank to rows with equal values, leaving gaps if there are duplicate values.\n",
    "# DENSE_RANK assigns the same rank to rows with equal values but doesn't leave gaps for duplicates.\n",
    "\n",
    "# SELECT\n",
    "#  EmployeeName, Salary,\n",
    "#  RANK() OVER (ORDER BY Salary DESC) AS SalaryRank,\n",
    "#  DENSE_RANK() OVER (ORDER BY Salary DESC) AS DenseRank\n",
    "#  FROM Employees;\n",
    "\n",
    "# ğŸ”¹ROW_NUMBER Function:\n",
    "# The ROW_NUMBER function assigns a unique sequential number to each row within the result set, without regard to the values in the columns being ordered.\n",
    "\n",
    "# SELECT\n",
    "#  ProductName, Price,\n",
    "#  ROW_NUMBER() OVER (ORDER BY ProductName) AS RowNum\n",
    "#  FROM Products;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "21998245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------------+\n",
      "|stock_symbol|      date|closing_price|\n",
      "+------------+----------+-------------+\n",
      "|        HDFC|2023-07-01|        150.0|\n",
      "|        HDFC|2023-07-02|        155.0|\n",
      "|        HDFC|2023-07-03|        160.0|\n",
      "+------------+----------+-------------+\n",
      "\n",
      "+------------+----------+-------------+------------------+------------+\n",
      "|stock_symbol|date      |closing_price|previous_day_price|price_change|\n",
      "+------------+----------+-------------+------------------+------------+\n",
      "|HDFC        |2023-07-01|150.0        |null              |null        |\n",
      "|HDFC        |2023-07-02|155.0        |150.0             |5.0         |\n",
      "|HDFC        |2023-07-03|160.0        |155.0             |5.0         |\n",
      "+------------+----------+-------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7113036095340052480/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7113036095340052480%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample stock data\n",
    "data = [(\"HDFC\", \"2023-07-01\", 150.00),\n",
    "    (\"HDFC\", \"2023-07-02\", 155.00),\n",
    "    (\"HDFC\", \"2023-07-03\", 160.00)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"stock_symbol\", \"date\", \"closing_price\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"stock_symbol\").orderBy(\"date\")\n",
    "\n",
    "# Apply the PySpark LAG function\n",
    "df_with_lag = df.withColumn(\"previous_day_price\", lag(\"closing_price\").over(window_spec))\n",
    "df_with_lag = df_with_lag.withColumn(\"price_change\", col(\"closing_price\") - col(\"previous_day_price\"))\n",
    "df_with_lag.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13544701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68c7f6f2",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7110661306436759552/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110661306436759552%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Handling NULLS With Pyspark:\n",
    "\n",
    "In PySpark, the `na` attribute provides various methods to deal with missing or null values in a DataFrame. Here are some common ways to use the `na` attribute:\n",
    "\n",
    "1. **Dropping Rows with Null Values:**\n",
    "  - `na.drop()`: Removes rows containing any null values in any column.\n",
    "  - `na.drop(\"all\")`: Removes rows where all columns have null values.\n",
    "  - `na.drop(subset=[\"column_name1\", \"column_name2\"])`: Removes rows with null values in specific columns.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  df.na.drop()\n",
    "  df.na.drop(\"all\")\n",
    "  df.na.drop(subset=[\"column_name\"])\n",
    "  ```\n",
    "\n",
    "2. **Filling Null Values:**\n",
    "  - `na.fill(value, subset=[\"column_name\"])`: Replaces null values in specific columns with the specified value.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  df.na.fill(\"replacement_value\", subset=[\"column_name\"])\n",
    "  ```\n",
    "\n",
    "3. **Replacing Values Conditionally:**\n",
    "  - `na.replace(map)`: Replaces values in specified columns using a map of replacement values.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  replace_map = {\"column_name1\": \"new_value1\", \"column_name2\": \"new_value2\"}\n",
    "  df.na.replace(replace_map)\n",
    "  ```\n",
    "\n",
    "4. **Filling Null Values with Mean/Median:**\n",
    "  - You can calculate the mean or median of a column and use it to fill null values.\n",
    "\n",
    "  Example (Filling with Mean):\n",
    "  ```python\n",
    "  from pyspark.sql.functions import mean\n",
    "\n",
    "  mean_value = df.select(mean(\"column_name\")).collect()[0][0]\n",
    "  df.na.fill(mean_value, subset=[\"column_name\"])\n",
    "  ```\n",
    "\n",
    "5. **Dropping Columns with Null Values:**\n",
    "  - `na.drop(\"all\", subset=[\"column_name\"])`: Drops columns where all values are null.\n",
    "\n",
    "  Example:\n",
    "  ```python\n",
    "  df.na.drop(\"all\", subset=[\"column_name\"])\n",
    "  ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc72997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95422bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n",
      "|employee_id|department|salary|\n",
      "+-----------+----------+------+\n",
      "|          1|       cse| 25000|\n",
      "|          3|       cse| 29000|\n",
      "|          5|       ece| 22000|\n",
      "|          2|       ece| 40000|\n",
      "|          6|        me| 12000|\n",
      "|          8|       ece| 32000|\n",
      "+-----------+----------+------+\n",
      "\n",
      "+-----------+----------+---------+----------+-----+-------+\n",
      "|employee_id|first_name|last_name|       DOB|state|country|\n",
      "+-----------+----------+---------+----------+-----+-------+\n",
      "|          1|    dinesh|      H R|13-04-1995|   KA|  INDIA|\n",
      "|          2|   bharath|    kumar|12-06-1999|   KA|  INDIA|\n",
      "|          3|     shiva|    kumar|15-02-1993|   KA|  INDIA|\n",
      "|          5|   dhivesh|      CTS|22-05-1999|   KA|  INDIA|\n",
      "|          6|  santhosh|      CTS|08-08-1998|   KA|  INDIA|\n",
      "|          8|  bassappa|      CTS|07-07-2000|   KA|  INDIA|\n",
      "+-----------+----------+---------+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7114176918945476609/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114176918945476609%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema1 = [\"employee_id\", \"department\", \"salary\"]\n",
    "data1 = [(1,\"cse\",25000),\n",
    "(3,\"cse\",29000),\n",
    "(5,\"ece\",22000),\n",
    "(2,\"ece\",40000),\n",
    "(6,\"me\",12000),\n",
    "(8,\"ece\",32000)]\n",
    "\n",
    "schema2 = [\"employee_id\", \"first_name\", \"last_name\", \"DOB\", \"state\", \"country\"]\n",
    "data2 = [(1,\"dinesh\", \"H R\", \"13-04-1995\",\"KA\",\"INDIA\"),\n",
    "(2,\"bharath\", \"kumar\", \"12-06-1999\", \"KA\", \"INDIA\"),\n",
    "(3,\"shiva\", \"kumar\", \"15-02-1993\", \"KA\", \"INDIA\"),\n",
    "(5,\"dhivesh\", \"CTS\", \"22-05-1999\", \"KA\", \"INDIA\"),\n",
    "(6,\"santhosh\", \"CTS\", \"08-08-1998\", \"KA\", \"INDIA\"),\n",
    "(8,\"bassappa\", \"CTS\", \"07-07-2000\",\"KA\", \"INDIA\")]\n",
    "\n",
    "employee = spark.createDataFrame(data1, schema=schema1)\n",
    "employee.show()\n",
    "\n",
    "employee_details = spark.createDataFrame(data2, schema=schema2)\n",
    "employee_details.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d1fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+---------+----------+-----+-------+\n",
      "|employee_id|department|salary|first_name|last_name|       DOB|state|country|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+\n",
      "|          1|       cse| 25000|    dinesh|      H R|13-04-1995|   KA|  INDIA|\n",
      "|          2|       ece| 40000|   bharath|    kumar|12-06-1999|   KA|  INDIA|\n",
      "|          3|       cse| 29000|     shiva|    kumar|15-02-1993|   KA|  INDIA|\n",
      "|          5|       ece| 22000|   dhivesh|      CTS|22-05-1999|   KA|  INDIA|\n",
      "|          6|        me| 12000|  santhosh|      CTS|08-08-1998|   KA|  INDIA|\n",
      "|          8|       ece| 32000|  bassappa|      CTS|07-07-2000|   KA|  INDIA|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = employee.join(employee_details, employee.employee_id == employee_details.employee_id, \"inner\").drop(employee_details.employee_id)\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93ac623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+---------+----------+-----+-------+\n",
      "|employee_id|department|salary|first_name|last_name|       DOB|state|country|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+\n",
      "|          1|       cse| 25000|    dinesh|      H R|1995-04-13|   KA|  INDIA|\n",
      "|          2|       ece| 40000|   bharath|    kumar|1999-06-12|   KA|  INDIA|\n",
      "|          3|       cse| 29000|     shiva|    kumar|1993-02-15|   KA|  INDIA|\n",
      "|          5|       ece| 22000|   dhivesh|      CTS|1999-05-22|   KA|  INDIA|\n",
      "|          6|        me| 12000|  santhosh|      CTS|1998-08-08|   KA|  INDIA|\n",
      "|          8|       ece| 32000|  bassappa|      CTS|2000-07-07|   KA|  INDIA|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+\n",
      "\n",
      "root\n",
      " |-- employee_id: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- DOB: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_data = joined_data.withColumn(\"DOB\", to_date(col(\"DOB\"), \"dd-MM-yyyy\"))\n",
    "joined_data.show()\n",
    "joined_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a5eab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+---------+----------+-----+-------+---+\n",
      "|employee_id|department|salary|first_name|last_name|       DOB|state|country|age|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+---+\n",
      "|          1|       cse| 25000|    dinesh|      H R|1995-04-13|   KA|  INDIA| 28|\n",
      "|          2|       ece| 40000|   bharath|    kumar|1999-06-12|   KA|  INDIA| 24|\n",
      "|          3|       cse| 29000|     shiva|    kumar|1993-02-15|   KA|  INDIA| 30|\n",
      "|          5|       ece| 22000|   dhivesh|      CTS|1999-05-22|   KA|  INDIA| 24|\n",
      "|          6|        me| 12000|  santhosh|      CTS|1998-08-08|   KA|  INDIA| 25|\n",
      "|          8|       ece| 32000|  bassappa|      CTS|2000-07-07|   KA|  INDIA| 23|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# agedata = joined_data.withColumn('age', datediff(current_date(),col(\"DOB\")))\n",
    "\n",
    "agedata = joined_data.withColumn('age', floor(datediff(current_date(), to_date(col(\"DOB\")))/365))\n",
    "agedata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6602bc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n"
     ]
    }
   ],
   "source": [
    "max_salary = agedata.select(max(\"salary\")).collect()[0][0]\n",
    "print(max_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e75d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+----------+---------+----------+-----+-------+---+------------------------+\n",
      "|employee_id|department|salary|first_name|last_name|       DOB|state|country|age|salary_diff_to_reach_max|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+---+------------------------+\n",
      "|          1|       cse| 25000|    dinesh|      H R|1995-04-13|   KA|  INDIA| 28|                   15000|\n",
      "|          2|       ece| 40000|   bharath|    kumar|1999-06-12|   KA|  INDIA| 24|                       0|\n",
      "|          3|       cse| 29000|     shiva|    kumar|1993-02-15|   KA|  INDIA| 30|                   11000|\n",
      "|          5|       ece| 22000|   dhivesh|      CTS|1999-05-22|   KA|  INDIA| 24|                   18000|\n",
      "|          6|        me| 12000|  santhosh|      CTS|1998-08-08|   KA|  INDIA| 25|                   28000|\n",
      "|          8|       ece| 32000|  bassappa|      CTS|2000-07-07|   KA|  INDIA| 23|                    8000|\n",
      "+-----------+----------+------+----------+---------+----------+-----+-------+---+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_diff = agedata.withColumn(\"salary_diff_to_reach_max\", max_salary- col(\"salary\"))\n",
    "salary_diff.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e9d07d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|employee_id|employee_full_name|\n",
      "+-----------+------------------+\n",
      "|          1|        dinesh H R|\n",
      "|          2|     bharath kumar|\n",
      "|          3|       shiva kumar|\n",
      "|          5|       dhivesh CTS|\n",
      "|          6|      santhosh CTS|\n",
      "|          8|      bassappa CTS|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first_last = salary_diff.withColumn(\"full_name\", concat_ws(sep = \" \", col(\"first_name\"), col(\"last_name\")))\n",
    "first_last = salary_diff.withColumn(\"employee_full_name\", concat_ws(\" \", salary_diff.first_name ,salary_diff.last_name))\n",
    "first_last.select(\"employee_id\", \"employee_full_name\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c277c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------+------+--------------------------------+----------+-----+-------+----+\n",
      "|employee_id|employee_full_name|department|salary|Salary_Diff_to_reach_highest_sal|       DOB|state|country| age|\n",
      "+-----------+------------------+----------+------+--------------------------------+----------+-----+-------+----+\n",
      "|          1|        dinesh,H R|       cse| 25000|                           15000|13-04-1995|   KA|  INDIA|null|\n",
      "|          2|     bharath,kumar|       ece| 40000|                               0|12-06-1999|   KA|  INDIA|null|\n",
      "|          3|       shiva,kumar|       cse| 29000|                           11000|15-02-1993|   KA|  INDIA|null|\n",
      "|          5|       dhivesh,CTS|       ece| 22000|                           18000|22-05-1999|   KA|  INDIA|null|\n",
      "|          6|      santhosh,CTS|        me| 12000|                           28000|08-08-1998|   KA|  INDIA|null|\n",
      "|          8|      bassappa,CTS|       ece| 32000|                            8000|07-07-2000|   KA|  INDIA|null|\n",
      "+-----------+------------------+----------+------+--------------------------------+----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Everything in one thing.\n",
    "\n",
    "resultDF = employee\\\n",
    ".join(employee_details, employee.employee_id==employee_details.employee_id, \"inner\").drop(employee.employee_id)\\\n",
    ".withColumn(\"employee_full_name\", concat_ws(\",\", col(\"first_name\"), col(\"last_name\")))\\\n",
    ".withColumn(\"Salary_Diff_to_reach_highest_sal\", lit(max_salary) - col(\"salary\"))\\\n",
    ".withColumn(\"age\", floor(datediff(current_date(), to_date(col(\"DOB\")))/365))\\\n",
    ".select(\"employee_id\",\n",
    "     \"employee_full_name\",\n",
    "     \"department\",\n",
    "     \"salary\",\n",
    "     \"Salary_Diff_to_reach_highest_sal\",\n",
    "     \"DOB\",\n",
    "     \"state\",\n",
    "     \"country\",\n",
    "     \"age\")\n",
    "\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "872fef20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|PERSON| TYPE|AGE|\n",
      "+------+-----+---+\n",
      "|    A1|ADULT| 54|\n",
      "|    A2|ADULT| 53|\n",
      "|    A3|ADULT| 52|\n",
      "|    A4|ADULT| 58|\n",
      "|    A5|ADULT| 54|\n",
      "|    C1|CHILD| 20|\n",
      "|    C2|CHILD| 19|\n",
      "|    C3|CHILD| 22|\n",
      "|    C4|CHILD| 15|\n",
      "+------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/ankur-bhattacharya-10055877_dataengineering-jpmorganinterview-datajobs-activity-7114127851586502656-mXl7?utm_source=share&utm_medium=member_desktop&trk=public_post_comment-text\n",
    "\n",
    "schema_ = [\"PERSON\" ,\"TYPE\" ,\"AGE\"]\n",
    "\n",
    "data = [(\"A1\" ,\"ADULT\" ,54),\n",
    "(\"A2\" ,\"ADULT\" ,53),\n",
    "(\"A3\" ,\"ADULT\" ,52),\n",
    "(\"A4\" ,\"ADULT\" ,58),\n",
    "(\"A5\" ,\"ADULT\" ,54),\n",
    "(\"C1\" ,\"CHILD\" ,20),\n",
    "(\"C2\" ,\"CHILD\" ,19),\n",
    "(\"C3\" ,\"CHILD\" ,22),\n",
    "(\"C4\" ,\"CHILD\" ,15)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c1bceb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|PERSON| TYPE|AGE|\n",
      "+------+-----+---+\n",
      "|    A4|ADULT| 58|\n",
      "|    A1|ADULT| 54|\n",
      "|    A5|ADULT| 54|\n",
      "|    A2|ADULT| 53|\n",
      "|    A3|ADULT| 52|\n",
      "+------+-----+---+\n",
      "\n",
      "+------+-----+---+\n",
      "|PERSON| TYPE|AGE|\n",
      "+------+-----+---+\n",
      "|    C4|CHILD| 15|\n",
      "|    C2|CHILD| 19|\n",
      "|    C1|CHILD| 20|\n",
      "|    C3|CHILD| 22|\n",
      "+------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adult = df.filter(col(\"TYPE\") == \"ADULT\").sort(desc(col(\"AGE\")))\n",
    "adult.show()\n",
    "\n",
    "child = df.filter(col(\"TYPE\") == \"CHILD\").sort(asc(col(\"AGE\")))\n",
    "child.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8bdaa5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+----------+\n",
      "|PERSON| TYPE|AGE|row_number|\n",
      "+------+-----+---+----------+\n",
      "|    A4|ADULT| 58|         1|\n",
      "|    A1|ADULT| 54|         2|\n",
      "|    A5|ADULT| 54|         3|\n",
      "|    A2|ADULT| 53|         4|\n",
      "|    A3|ADULT| 52|         5|\n",
      "+------+-----+---+----------+\n",
      "\n",
      "+-----+-----+---+----------+\n",
      "|CHILD| TYPE|AGE|row_number|\n",
      "+-----+-----+---+----------+\n",
      "|   C4|CHILD| 15|         1|\n",
      "|   C2|CHILD| 19|         2|\n",
      "|   C1|CHILD| 20|         3|\n",
      "|   C3|CHILD| 22|         4|\n",
      "+-----+-----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(desc(\"AGE\"))\n",
    "window_spec1 = Window.orderBy(\"AGE\")\n",
    "\n",
    "adult_row = adult.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "adult_row.show()\n",
    "\n",
    "child_row = child.withColumn(\"row_number\", row_number().over(window_spec1))\\\n",
    "            .withColumnRenamed(\"PERSON\",\"CHILD\")\n",
    "child_row.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7de2eb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+----------+-----+-----+----+----------+\n",
      "|PERSON| TYPE|AGE|row_number|CHILD| TYPE| AGE|row_number|\n",
      "+------+-----+---+----------+-----+-----+----+----------+\n",
      "|    A4|ADULT| 58|         1|   C4|CHILD|  15|         1|\n",
      "|    A1|ADULT| 54|         2|   C2|CHILD|  19|         2|\n",
      "|    A5|ADULT| 54|         3|   C1|CHILD|  20|         3|\n",
      "|    A2|ADULT| 53|         4|   C3|CHILD|  22|         4|\n",
      "|    A3|ADULT| 52|         5| null| null|null|      null|\n",
      "+------+-----+---+----------+-----+-----+----+----------+\n",
      "\n",
      "+-----+-----+\n",
      "|ADULT|CHILD|\n",
      "+-----+-----+\n",
      "|   A4|   C4|\n",
      "|   A1|   C2|\n",
      "|   A5|   C1|\n",
      "|   A2|   C3|\n",
      "|   A3| null|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finaldf = adult_row.join(child_row, adult_row.row_number == child_row.row_number, \"full\")\n",
    "finaldf.show()\n",
    "                        \n",
    "finaldf.selectExpr(\"PERSON as ADULT\", \"CHILD\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6d6019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|PERSON| TYPE|AGE|\n",
      "+------+-----+---+\n",
      "|    A1|ADULT| 54|\n",
      "|    A2|ADULT| 53|\n",
      "|    A3|ADULT| 52|\n",
      "|    A4|ADULT| 58|\n",
      "|    A5|ADULT| 54|\n",
      "|    C1|CHILD| 20|\n",
      "|    C2|CHILD| 19|\n",
      "|    C3|CHILD| 22|\n",
      "|    C4|CHILD| 15|\n",
      "+------+-----+---+\n",
      "\n",
      "+------+-----+---+----+\n",
      "|PERSON| TYPE|AGE|rank|\n",
      "+------+-----+---+----+\n",
      "|    A4|ADULT| 58|   1|\n",
      "|    A1|ADULT| 54|   2|\n",
      "|    A5|ADULT| 54|   3|\n",
      "|    A2|ADULT| 53|   4|\n",
      "|    A3|ADULT| 52|   5|\n",
      "+------+-----+---+----+\n",
      "\n",
      "+------+-----+---+----+\n",
      "|PERSON| TYPE|AGE|rank|\n",
      "+------+-----+---+----+\n",
      "|    C4|CHILD| 15|   1|\n",
      "|    C2|CHILD| 19|   2|\n",
      "|    C1|CHILD| 20|   3|\n",
      "|    C3|CHILD| 22|   4|\n",
      "+------+-----+---+----+\n",
      "\n",
      "+------------+-----+---+----+\n",
      "|child_person| type|age|rank|\n",
      "+------------+-----+---+----+\n",
      "|          C4|CHILD| 15|   1|\n",
      "|          C2|CHILD| 19|   2|\n",
      "|          C1|CHILD| 20|   3|\n",
      "|          C3|CHILD| 22|   4|\n",
      "+------------+-----+---+----+\n",
      "\n",
      "+----+------+-----+---+------------+-----+----+\n",
      "|rank|PERSON| TYPE|AGE|child_person| type| age|\n",
      "+----+------+-----+---+------------+-----+----+\n",
      "|   1|    A4|ADULT| 58|          C4|CHILD|  15|\n",
      "|   3|    A5|ADULT| 54|          C1|CHILD|  20|\n",
      "|   5|    A3|ADULT| 52|        null| null|null|\n",
      "|   4|    A2|ADULT| 53|          C3|CHILD|  22|\n",
      "|   2|    A1|ADULT| 54|          C2|CHILD|  19|\n",
      "+----+------+-----+---+------------+-----+----+\n",
      "\n",
      "+-----+-----+\n",
      "|ADULT|CHILD|\n",
      "+-----+-----+\n",
      "|   A4|   C4|\n",
      "|   A5|   C1|\n",
      "|   A3| null|\n",
      "|   A2|   C3|\n",
      "|   A1|   C2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PySpark Solution: \n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()\n",
    "\n",
    "wind_spec=Window.partitionBy(col(\"type\")).orderBy(col(\"Age\").desc())\n",
    "\n",
    "adult=df.withColumn(\"rank\",row_number().over(wind_spec)).filter(col(\"type\")==\"ADULT\")\n",
    "# adult.show()\n",
    "\n",
    "child=df.withColumn(\"rank\", row_number().over(Window.partitionBy(col(\"type\")).\n",
    "orderBy(col(\"Age\").asc()))).filter(col(\"type\")==\"CHILD\")\n",
    "# child.show()\n",
    "\n",
    "\n",
    "child=child.select(col(\"person\").alias(\"child_person\"),\"type\",\"age\",\"rank\")\n",
    "# child.show()\n",
    "\n",
    "result=adult.join(child,\"rank\",\"left\")\n",
    "# result.show()\n",
    "\n",
    "result.select(col(\"person\").alias(\"ADULT\"),col(\"child_person\").alias(\"CHILD\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02422fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|PERSON| TYPE|AGE|\n",
      "+------+-----+---+\n",
      "|    A1|ADULT| 54|\n",
      "|    A2|ADULT| 53|\n",
      "|    A3|ADULT| 52|\n",
      "|    A4|ADULT| 58|\n",
      "|    A5|ADULT| 54|\n",
      "|    C1|CHILD| 20|\n",
      "|    C2|CHILD| 19|\n",
      "|    C3|CHILD| 22|\n",
      "|    C4|CHILD| 15|\n",
      "+------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7114127851586502656/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114127851586502656%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"PERSON\" ,\"TYPE\" ,\"AGE\"]\n",
    "\n",
    "data = [(\"A1\" ,\"ADULT\" ,54),\n",
    "(\"A2\" ,\"ADULT\" ,53),\n",
    "(\"A3\" ,\"ADULT\" ,52),\n",
    "(\"A4\" ,\"ADULT\" ,58),\n",
    "(\"A5\" ,\"ADULT\" ,54),\n",
    "(\"C1\" ,\"CHILD\" ,20),\n",
    "(\"C2\" ,\"CHILD\" ,19),\n",
    "(\"C3\" ,\"CHILD\" ,22),\n",
    "(\"C4\" ,\"CHILD\" ,15)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bdad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+\n",
      "|PERSON| TYPE|AGE|\n",
      "+------+-----+---+\n",
      "|    A1|ADULT| 54|\n",
      "|    A2|ADULT| 53|\n",
      "|    A3|ADULT| 52|\n",
      "|    A4|ADULT| 58|\n",
      "|    A5|ADULT| 54|\n",
      "|    C1|CHILD| 20|\n",
      "|    C2|CHILD| 19|\n",
      "|    C3|CHILD| 22|\n",
      "|    C4|CHILD| 15|\n",
      "+------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"persons\")\n",
    "spark.sql(\"select * from persons\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe852983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"with cte1 as (\\\n",
    "# select *, case \\\n",
    "# when type='adult' then dense_rank() over (partition by type order by age desc)\\\n",
    "# when type='child' then dense_rank() over (partition by type order by age)\\\n",
    "# end as rn from person)\\\n",
    "# select p1.person as adult, p2.person as child from cte1 p1 join cte1 p2 on p1.rn=p2.rn \\\n",
    "# and p1.type<>p2.type and p1.type='adult'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73548d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+---------+\n",
      "|first_name| last_name|designation|   salary|\n",
      "+----------+----------+-----------+---------+\n",
      "|       Pat|     Given|     Junior| 32720.55|\n",
      "|      Remy|      Esch|     Senior|569929.25|\n",
      "|  Rafaello|   Worrall|    Manager|622487.93|\n",
      "|  Gherardo|Chadbourne|       Lead| 425927.7|\n",
      "+----------+----------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7114120072947757056/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114120072947757056%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# The salary is incremented by following criteria:\n",
    "# Managers -> 20%\n",
    "# Leads -> 15%\n",
    "# Senior -> 10%\n",
    "# Junior -> 20%\n",
    "\n",
    "schema_ = [\"first_name\",\"last_name\",\"designation\",\"salary\"]\n",
    "\n",
    "data = [(\"Pat\",\"Given\",\"Junior\",32720.55),\n",
    "(\"Remy\",\"Esch\",\"Senior\",569929.25),\n",
    "(\"Rafaello\",\"Worrall\",\"Manager\",622487.93),\n",
    "(\"Gherardo\",\"Chadbourne\",\"Lead\",425927.70)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e934c621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------+---------+----------+\n",
      "|first_name| last_name|designation|   salary|salary_inc|\n",
      "+----------+----------+-----------+---------+----------+\n",
      "|       Pat|     Given|     Junior| 32720.55|  39264.66|\n",
      "|      Remy|      Esch|     Senior|569929.25| 626922.18|\n",
      "|  Rafaello|   Worrall|    Manager|622487.93| 746985.52|\n",
      "|  Gherardo|Chadbourne|       Lead| 425927.7| 489816.86|\n",
      "+----------+----------+-----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salary_incr = df.withColumn(\"salary_inc\", expr(\"case when designation == 'Manager' then round(salary + salary * (20/100), 2) \\\n",
    "              when designation == 'Lead' then round(salary + salary * (15/100), 2)\\\n",
    "              when designation == 'Senior' then round(salary + salary * (10/100), 2)\\\n",
    "              when designation == 'Junior' then round(salary + salary * (20/100), 2)\\\n",
    "                else 0 end\"))\n",
    "salary_incr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d062745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-------+-------+-------+\n",
      "|year|Wimbledon|Fr_Open|US_Open|Au_Open|\n",
      "+----+---------+-------+-------+-------+\n",
      "|2017|        2|      1|      1|      2|\n",
      "|2018|        3|      1|      3|      2|\n",
      "|2019|        3|      1|      1|      3|\n",
      "+----+---------+-------+-------+-------+\n",
      "\n",
      "+---------+-----------+\n",
      "|Player_ID|player_Name|\n",
      "+---------+-----------+\n",
      "|        1|      Nadal|\n",
      "|        2|    Federer|\n",
      "|        3|      Novak|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/hasnain-motagamwala_players-tournament-win-results-code-activity-7123234638797373440-1Obc\n",
    "\n",
    "# Matches Data\n",
    "matches_list = [(2017, 2, 1, 1, 2),(2018, 3, 1, 3, 2),(2019, 3, 1, 1, 3)]\n",
    "matches_header = [\"year\", \"Wimbledon\", \"Fr_Open\", \"US_Open\", \"Au_Open\"]\n",
    "\n",
    "# Player Data\n",
    "players_list = [(1, \"Nadal\"),(2, \"Federer\"),(3, \"Novak\")]\n",
    "players_header = [\"Player_ID\", \"player_Name\"]\n",
    "\n",
    "matches = spark.createDataFrame(matches_list, matches_header)\n",
    "matches.show()\n",
    "\n",
    "players = spark.createDataFrame(players_list, players_header)\n",
    "players.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aaeb2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|PlayerID|count|\n",
      "+--------+-----+\n",
      "|       2|    1|\n",
      "|       3|    2|\n",
      "|       1|    3|\n",
      "|       1|    2|\n",
      "|       3|    1|\n",
      "|       2|    2|\n",
      "|       3|    1|\n",
      "+--------+-----+\n",
      "\n",
      "+--------+----------------+\n",
      "|PlayerID|No_of_GrandSlams|\n",
      "+--------+----------------+\n",
      "|       3|               4|\n",
      "|       2|               3|\n",
      "|       1|               5|\n",
      "+--------+----------------+\n",
      "\n",
      "+---------+-----------+----------------+\n",
      "|Player_ID|player_Name|No_of_GrandSlams|\n",
      "+---------+-----------+----------------+\n",
      "|        1|      Nadal|               5|\n",
      "|        2|    Federer|               3|\n",
      "|        3|      Novak|               4|\n",
      "+---------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Used GroupedCount\n",
    "inter_res= matches.groupBy(col(\"Wimbledon\").alias(\"PlayerID\")).count().union(\n",
    "matches.groupBy(col(\"Fr_Open\").alias(\"PlayerID\")).count()).union(\n",
    "matches.groupBy(col(\"US_Open\").alias(\"PlayerID\")).count()).union(\n",
    "matches.groupBy(col(\"Au_Open\").alias(\"PlayerID\")).count())\n",
    "inter_res.show()\n",
    "\n",
    "#Getting the No_of_GrandSlams per player\n",
    "res = inter_res.groupBy(\"PlayerID\").agg(sum(\"count\").alias(\"No_of_GrandSlams\"))\n",
    "res.show()\n",
    "\n",
    "#Player name with player ID\n",
    "res.join(players, res.PlayerID==players.Player_ID, \"inner\")\\\n",
    "    .select(col(\"Player_ID\"), col(\"player_Name\"), col(\"No_of_GrandSlams\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1d7424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----+\n",
      "|student_name|exam_score|rank|\n",
      "+------------+----------+----+\n",
      "|       Salma|        97|   1|\n",
      "|       Akbar|        92|   2|\n",
      "|        Amar|        85|   3|\n",
      "|     Anthony|        78|   4|\n",
      "|     Lakshmi|        78|   4|\n",
      "|       Jenny|        70|   6|\n",
      "+------------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7114435642473820160/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114435642473820160%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Sample student data\n",
    "data = [(\"Amar\", 85),(\"Akbar\", 92),(\"Anthony\", 78),(\"Lakshmi\", 78),(\"Salma\", 97),(\"Jenny\", 70)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"student_name\", \"exam_score\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.orderBy(col(\"exam_score\").desc())\n",
    "\n",
    "# Apply the PySpark RANK function\n",
    "df_with_rank = df.withColumn(\"rank\", rank().over(window_spec))\n",
    "df_with_rank.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abdf6e39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:503\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msendall(command\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:506\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    505\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending or receiving.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mSCD Type 2 is a common technique used in data warehousing to track changes in dimension data over time. It involves maintaining historical versions of records when a dimension attribute's value changes. In PySpark, you can implement SCD Type 2 by managing historical records and handling updates. Here's a step-by-step demonstration of how to implement SCD Type 2 using PySpark:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mAssuming you have two DataFrames: `currentData` representing the current dimension data and `newData` representing the incoming data that might have changes.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Sample data for demonstration\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m currentData \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m     15\u001b[0m   (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-01-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m999-123-4567\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     16\u001b[0m   (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlice\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-02-01\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m888-987-6543\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_from\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     19\u001b[0m newData \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([\n\u001b[1;32m     20\u001b[0m   (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJohn Doe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-06-15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m999-123-4567\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     21\u001b[0m   (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBob\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2021-05-10\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m777-555-1234\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m ], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid_from\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphone\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Add necessary columns to the DataFrames\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/session.py:1222\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1220\u001b[0m SparkSession\u001b[38;5;241m.\u001b[39m_activeSession \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSparkSession\u001b[38;5;241m.\u001b[39msetActiveSession(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is already a DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(\n\u001b[1;32m   1713\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFLECTION_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1714\u001b[0m     proto\u001b[38;5;241m.\u001b[39mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1715\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART)\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1053\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(retry, connection, pne):\n\u001b[1;32m   1052\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1053\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_command(command, binary\u001b[38;5;241m=\u001b[39mbinary)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     logging\u001b[38;5;241m.\u001b[39mexception(\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7114489959322939392/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114489959322939392%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Slowly Changing Dimension Type 2 (SCD Type 2) Using PySpark:\n",
    "\n",
    "# SCD Type 2 is a common technique used in data warehousing to track changes in dimension data over time. \n",
    "# It involves maintaining historical versions of records when a dimension attribute's value changes. \n",
    "# In PySpark, you can implement SCD Type 2 by managing historical records and handling updates. \n",
    "# Here's a step-by-step demonstration of how to implement SCD Type 2 using PySpark:\n",
    "\n",
    "# Assuming you have two DataFrames: `currentData` representing the current dimension data and `newData` \n",
    "# representing the incoming data that might have changes.\n",
    "\n",
    "\n",
    "# Sample data for demonstration\n",
    "currentData = spark.createDataFrame([\n",
    "  (1, \"John\", \"2021-01-01\", \"999-123-4567\"),\n",
    "  (2, \"Alice\", \"2021-02-01\", \"888-987-6543\")\n",
    "], [\"id\", \"name\", \"valid_from\", \"phone\"])\n",
    "\n",
    "newData = spark.createDataFrame([\n",
    "  (1, \"John Doe\", \"2021-06-15\", \"999-123-4567\"),\n",
    "  (3, \"Bob\", \"2021-05-10\", \"777-555-1234\")\n",
    "], [\"id\", \"name\", \"valid_from\", \"phone\"])\n",
    "\n",
    "# Add necessary columns to the DataFrames\n",
    "currentData = currentData.withColumn(\"valid_to\", current_date())\n",
    "currentData.show()\n",
    "\n",
    "newData = newData.withColumn(\"valid_from\", newData[\"valid_from\"].cast(\"date\"))\n",
    "newData = newData.withColumn(\"valid_to\", lit(None).cast(\"date\"))\n",
    "newData.show()\n",
    "\n",
    "# Define a union of the currentData and newData\n",
    "unionedData = currentData.union(newData)\n",
    "unionedData.show()\n",
    "\n",
    "# Implement SCD Type 2 logic\n",
    "\n",
    "# Create a window specification for partitioning and ordering\n",
    "windowSpec = Window.partitionBy(\"id\").orderBy(\"valid_from\")\n",
    "\n",
    "# Add a new column to track the change sequence\n",
    "changeSequence = unionedData.withColumn(\"change_sequence\", row_number().over(windowSpec))\n",
    "changeSequence.show()\n",
    "\n",
    "# Filter out records where change_sequence is greater than 1 (i.e., duplicates)\n",
    "scd2Data = changeSequence.filter(changeSequence.change_sequence == 1)\n",
    "\n",
    "# Show the SCD Type 2 result\n",
    "scd2Data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e314d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------------+\n",
      "|user_id|date_searched|filter_room_types       |\n",
      "+-------+-------------+------------------------+\n",
      "|1      |2022-01-01   |entire home,private room|\n",
      "|2      |2022-01-02   |entire home,shared room |\n",
      "|3      |2022-01-02   |private room,shared room|\n",
      "|4      |2022-01-03   |private room            |\n",
      "+-------+-------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7114576934088376320/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114576934088376320%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»: Find the date on which maximum number of rooms were searched along with their count.\n",
    "\n",
    "schema_ = [\"user_id\", \"date_searched\", \"filter_room_types\"]\n",
    "\n",
    "data = [(1 , \"2022-01-01\" , \"entire home,private room\"),\n",
    "(2 , \"2022-01-02\" , \"entire home,shared room\"),\n",
    "(3 , \"2022-01-02\" , \"private room,shared room\"),\n",
    "(4 , \"2022-01-03\" , \"private room\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975643d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+\n",
      "|user_id|date_searched|filter_room_types|\n",
      "+-------+-------------+-----------------+\n",
      "|      1|   2022-01-01|      entire home|\n",
      "|      1|   2022-01-01|     private room|\n",
      "|      2|   2022-01-02|      entire home|\n",
      "|      2|   2022-01-02|      shared room|\n",
      "|      3|   2022-01-02|     private room|\n",
      "|      3|   2022-01-02|      shared room|\n",
      "|      4|   2022-01-03|     private room|\n",
      "+-------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "split_data = df.withColumn(\"filter_room_types\", explode(split(df[\"filter_room_types\"], \",\")))\n",
    "split_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "065cc499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+\n",
      "|date_searched|           All Rooms|count|\n",
      "+-------------+--------------------+-----+\n",
      "|   2022-01-02|[entire home, sha...|    4|\n",
      "|   2022-01-01|[entire home, pri...|    2|\n",
      "|   2022-01-03|      [private room]|    1|\n",
      "+-------------+--------------------+-----+\n",
      "\n",
      "+-------------+-----------------------------------------------------+-----+\n",
      "|date_searched|All Rooms                                            |count|\n",
      "+-------------+-----------------------------------------------------+-----+\n",
      "|2022-01-02   |[entire home, shared room, private room, shared room]|4    |\n",
      "+-------------+-----------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupedData = split_data.groupBy(\"date_searched\").agg(\n",
    "    collect_list(\"filter_room_types\").alias(\"All Rooms\"), \n",
    "    count(\"filter_room_types\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
    "groupedData.show()\n",
    "\n",
    "resultDf.limit(1).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a97119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e102c9f",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7114549577948528640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114549577948528640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "## Topic : ğŸš€ Harnessing the Power of Lazy Evaluation in Apache Spark! ğŸŒŸ\n",
    "\n",
    "\n",
    "Topic : ğŸš€ Harnessing the Power of Lazy Evaluation in Apache Spark! ğŸŒŸ\n",
    "\n",
    "Lazy evaluation in Apache Spark is like a smart assistant for data processing. Imagine you have a magical helper who doesn't do any work until it's really needed, making things faster and giving you lots of options.\n",
    "\n",
    "For example, when working with lots of data, instead of doing everything at once, this helper only does each task when you say, \"I need this now!\" This way, you save time and resources because you're not doing unnecessary work.\n",
    "\n",
    "### Sample data\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22), (\"David\", 28), (\"Eve\", 35)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "### Transformation 1: Group by Age\n",
    "grouped_data = df.groupBy(\"Age\").count()\n",
    "\n",
    "### Transformation 2: Filter Ages above 25\n",
    "filtered_data = grouped_data.filter(grouped_data.Age > 25)\n",
    "\n",
    "### Action 1: Show the grouped data (Lazy Evaluation)\n",
    "print(\"Grouped Data (Lazy):\")\n",
    "\n",
    "grouped_data.show()\n",
    "\n",
    "### Action 2: Show the filtered data (Lazy Evaluation)\n",
    "print(\"Filtered Data (Lazy):\")\n",
    "\n",
    "filtered_data.show()\n",
    "\n",
    "In this code:\n",
    "\n",
    "1.We create a Spark session.\n",
    "\n",
    "2.We create a DataFrame df with sample data containing names and ages.\n",
    "\n",
    "3.Transformation 1 groups the data by age and counts the occurrences but does not execute it immediately.\n",
    "\n",
    "4.Transformation 2 filters the grouped data to select ages above 25 but also does not execute it immediately.\n",
    "\n",
    "5.Action 1 displays the grouped data using .show(), triggering the execution of the grouping transformation.\n",
    "\n",
    "6.Action 2 displays the filtered data using .show(), triggering the execution of both the grouping and filtering transformations.\n",
    "\n",
    "The use of groupBy and filtering with filter demonstrates how different transformations can be applied lazily, and execution is deferred until an action is triggered.\n",
    "\n",
    "âœ¨ Why Lazy Evaluation Matters âœ¨\n",
    "\n",
    "ğŸ¯ Optimization: Spark fine-tunes the execution plan based on the entire sequence of transformations, avoiding unnecessary work and improving efficiency.\n",
    "\n",
    "ğŸš€ Efficiency: It minimizes data shuffling, cuts down on disk reads, and supercharges overall performance.\n",
    "\n",
    "ğŸ¤ Flexibility: You can craft intricate data pipelines without prematurely incurring computational costs, a true game-changer in the world of big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e27a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d976611",
   "metadata": {},
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115146007184609280/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115146007184609280%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "PySpark Interview Questions ğŸ‰\n",
    "\n",
    "Question 16 : What is difference between narrow and wide transforamtion in PySpark ?\n",
    "\n",
    "In PySpark, transformations are categorized into two types: narrow transformations and wide transformations. These categories are based on how they impact the execution plan and data shuffling in a Spark job.\n",
    "\n",
    "Narrow Transformations:\n",
    "Narrow transformations are those transformations where each output partition depends on a single input partition.\n",
    "They do not require data shuffling or data movement across partitions, making them more efficient.\n",
    "Examples of narrow transformations include map, filter, and union.\n",
    "\n",
    "### Sample DataFrame\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 22), (\"David\", 28), (\"Eve\", 35)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "### Narrow Transformation: Filtering ages above 25\n",
    "\n",
    "filtered_df = df.filter(df.Age > 25)\n",
    "\n",
    "Wide Transformations:\n",
    "Wide transformations are those transformations where each output partition depends on multiple input partitions.\n",
    "They require data shuffling or redistribution across partitions, which can be resource-intensive and time-consuming.\n",
    "Examples of wide transformations include groupByKey and join.\n",
    "\n",
    "### Sample DataFrames\n",
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")], [\"ID\", \"Name\"])\n",
    "\n",
    "df2 = spark.createDataFrame([(1, \"Math\"), (2, \"Science\"), (3, \"History\")], [\"ID\", \"Subject\"])\n",
    "\n",
    "### Wide Transformation: Joining two DataFrames\n",
    "joined_df = df1.join(df2, \"ID\")\n",
    "\n",
    "Understanding these transformation types helps us optimize our PySpark jobs for blazing-fast data processing! ğŸ”¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e2b8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0efab556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+\n",
      "| emp_name|dep_id|salary|\n",
      "+---------+------+------+\n",
      "|   Genece|     2| 75000|\n",
      "|   Jaimin|     2| 80000|\n",
      "|   Pankaj|     2| 80000|\n",
      "| Tarvares|     2| 70000|\n",
      "| Marlania|     4| 70000|\n",
      "|   Briana|     4| 85000|\n",
      "| Kimberli|     4| 55000|\n",
      "|Gabriella|     4| 55000|\n",
      "|   Lakken|     5| 60000|\n",
      "| Latoynia|     5| 65000|\n",
      "+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115214890214002688/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115214890214002688%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_ = [\"emp_name\",\"dep_id\",\"salary\"]\n",
    "\n",
    "data = [(\"Genece\", 2, 75000),\n",
    " (\"Jaimin\", 2, 80000),\n",
    " (\"Pankaj\", 2, 80000),\n",
    " (\"Tarvares\", 2, 70000),\n",
    " (\"Marlania\", 4, 70000),\n",
    " (\"Briana\", 4, 85000),\n",
    " (\"Kimberli\", 4, 55000),\n",
    " (\"Gabriella\", 4, 55000),\n",
    " (\"Lakken\", 5, 60000),\n",
    " (\"Latoynia\", 5, 65000)]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a6671ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+----------+\n",
      "| emp_name|dep_id|salary|salaryRank|\n",
      "+---------+------+------+----------+\n",
      "|   Jaimin|     2| 80000|         1|\n",
      "|   Pankaj|     2| 80000|         1|\n",
      "|   Genece|     2| 75000|         3|\n",
      "| Tarvares|     2| 70000|         4|\n",
      "|   Briana|     4| 85000|         1|\n",
      "| Marlania|     4| 70000|         2|\n",
      "| Kimberli|     4| 55000|         3|\n",
      "|Gabriella|     4| 55000|         3|\n",
      "| Latoynia|     5| 65000|         1|\n",
      "|   Lakken|     5| 60000|         2|\n",
      "+---------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"dep_id\").orderBy(desc(\"salary\"))\n",
    "\n",
    "df1 = df.withColumn(\"salaryRank\", rank().over(window_spec))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad7b3aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+----------+--------+\n",
      "| emp_name|dep_id|salary|salaryRank|lastRank|\n",
      "+---------+------+------+----------+--------+\n",
      "|   Jaimin|     2| 80000|         1|       4|\n",
      "|   Pankaj|     2| 80000|         1|       4|\n",
      "|   Genece|     2| 75000|         3|       4|\n",
      "| Tarvares|     2| 70000|         4|       4|\n",
      "|   Briana|     4| 85000|         1|       3|\n",
      "| Marlania|     4| 70000|         2|       3|\n",
      "| Kimberli|     4| 55000|         3|       3|\n",
      "|Gabriella|     4| 55000|         3|       3|\n",
      "| Latoynia|     5| 65000|         1|       2|\n",
      "|   Lakken|     5| 60000|         2|       2|\n",
      "+---------+------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn(\"lastRank\", max(\"salaryRank\").over(Window.partitionBy(\"dep_id\")))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "197344af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+----------+--------+\n",
      "| emp_name|dep_id|salary|salaryRank|lastRank|\n",
      "+---------+------+------+----------+--------+\n",
      "|   Jaimin|     2| 80000|         1|       4|\n",
      "|   Pankaj|     2| 80000|         1|       4|\n",
      "| Tarvares|     2| 70000|         4|       4|\n",
      "|   Briana|     4| 85000|         1|       3|\n",
      "| Kimberli|     4| 55000|         3|       3|\n",
      "|Gabriella|     4| 55000|         3|       3|\n",
      "| Latoynia|     5| 65000|         1|       2|\n",
      "|   Lakken|     5| 60000|         2|       2|\n",
      "+---------+------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FilterData = df2.filter((df2.salaryRank == 1) | (df2.salaryRank == df2.lastRank))\n",
    "FilterData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70707974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------------------+\n",
      "|dep_id|salary|emp_names            |\n",
      "+------+------+---------------------+\n",
      "|2     |80000 |[Pankaj, Jaimin]     |\n",
      "|2     |70000 |[Tarvares]           |\n",
      "|4     |85000 |[Briana]             |\n",
      "|4     |55000 |[Gabriella, Kimberli]|\n",
      "|5     |65000 |[Latoynia]           |\n",
      "|5     |60000 |[Lakken]             |\n",
      "+------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = FilterData.groupBy(\"dep_id\", \"salary\").agg(collect_set(\"emp_name\").alias(\"emp_names\"))\n",
    "final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59dab591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+\n",
      "| emp_name|dep_id|salary|\n",
      "+---------+------+------+\n",
      "| Marlania|     1| 92643|\n",
      "|   Briana|     1| 87202|\n",
      "| Kimberli|     2| 51407|\n",
      "|   Lakken|     2| 88933|\n",
      "|  Micaila|     2| 82145|\n",
      "| Tarvares|     3| 82979|\n",
      "| Latoynia|     3| 55729|\n",
      "|Gabriella|     4| 74132|\n",
      "|   Medusa|     4| 72551|\n",
      "|   Genece|     5| 98877|\n",
      "|  Thurley|     5| 65628|\n",
      "+---------+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/hasnain-motagamwala_key-takeaways-approach-activity-7114867617609203713-UAMz?utm_source=share&utm_medium=member_desktop\n",
    "\n",
    "# Sample input data\n",
    "data = [(\"Marlania\", 1, 92643),\n",
    "       (\"Briana\", 1, 87202),\n",
    "       (\"Kimberli\", 2, 51407),\n",
    "       (\"Lakken\", 2, 88933),\n",
    "       (\"Micaila\", 2, 82145),\n",
    "       (\"Tarvares\", 3, 82979),\n",
    "       (\"Latoynia\", 3, 55729),\n",
    "       (\"Gabriella\", 4, 74132),\n",
    "       (\"Medusa\", 4, 72551),\n",
    "       (\"Genece\", 5, 98877),\n",
    "       (\"Thurley\", 5, 65628)]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"emp_name\", \"dep_id\", \"salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10d30cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|dep_id|maxSalary|minSalary|\n",
      "+------+---------+---------+\n",
      "|     1|    92643|    87202|\n",
      "|     2|    88933|    51407|\n",
      "|     3|    82979|    55729|\n",
      "|     4|    74132|    72551|\n",
      "|     5|    98877|    65628|\n",
      "+------+---------+---------+\n",
      "\n",
      "+---------+-------+------+------+---------+---------+\n",
      "| emp_name|dept_id|salary|dep_id|maxSalary|minSalary|\n",
      "+---------+-------+------+------+---------+---------+\n",
      "| Marlania|      1| 92643|     1|    92643|    87202|\n",
      "|   Briana|      1| 87202|     1|    92643|    87202|\n",
      "| Kimberli|      2| 51407|     2|    88933|    51407|\n",
      "|   Lakken|      2| 88933|     2|    88933|    51407|\n",
      "|  Micaila|      2| 82145|     2|    88933|    51407|\n",
      "| Tarvares|      3| 82979|     3|    82979|    55729|\n",
      "| Latoynia|      3| 55729|     3|    82979|    55729|\n",
      "|Gabriella|      4| 74132|     4|    74132|    72551|\n",
      "|   Medusa|      4| 72551|     4|    74132|    72551|\n",
      "|   Genece|      5| 98877|     5|    98877|    65628|\n",
      "|  Thurley|      5| 65628|     5|    98877|    65628|\n",
      "+---------+-------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_min = df.groupBy(\"dep_id\").agg(max(\"salary\").alias(\"maxSalary\"), min(\"salary\").alias(\"minSalary\"))\n",
    "max_min.show()\n",
    "\n",
    "joinedData = df.withColumnRenamed(\"dep_id\", \"dept_id\")\n",
    "joinedData = joinedData.join(max_min, joinedData.dept_id == max_min.dep_id, \"inner\") \n",
    "joinedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c5a18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+\n",
      "|dep_id|  max_emp| min_emp|\n",
      "+------+---------+--------+\n",
      "|     1| Marlania|  Briana|\n",
      "|     2|   Lakken|Kimberli|\n",
      "|     3| Tarvares|Latoynia|\n",
      "|     4|Gabriella|  Medusa|\n",
      "|     5|   Genece| Thurley|\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF = joinedData.groupBy(\"dep_id\").agg(\n",
    "    max(when(col(\"salary\") == col(\"maxSalary\"), col(\"emp_name\")))\\\n",
    "    .alias(\"max_emp\"),\n",
    "    min(when(col(\"salary\") == col(\"minSalary\"), col(\"emp_name\"))\\\n",
    "        .otherwise(\"null\")).alias(\"min_emp\"))\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0ce887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+\n",
      "|dept_id|  max_emp| min_emp|\n",
      "+-------+---------+--------+\n",
      "|      1| Marlania|    null|\n",
      "|      1|     null|  Briana|\n",
      "|      2|     null|Kimberli|\n",
      "|      2|   Lakken|    null|\n",
      "|      2|     null|    null|\n",
      "|      3| Tarvares|    null|\n",
      "|      3|     null|Latoynia|\n",
      "|      4|Gabriella|    null|\n",
      "|      4|     null|  Medusa|\n",
      "|      5|   Genece|    null|\n",
      "|      5|     null| Thurley|\n",
      "+-------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Second Approach\n",
    "\n",
    "fullData = joinedData.withColumn(\"max_emp\", expr(\"case when salary == maxSalary then emp_name else null end\"))\\\n",
    ".withColumn(\"min_emp\", \n",
    "        expr(\"case when salary == minSalary then emp_name else null end\"))\n",
    "# fullData.show()\n",
    "\n",
    "res = fullData.select(\"dept_id\", \"max_emp\", \"min_emp\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5dcbce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_id|  max_emp|\n",
      "+-------+---------+\n",
      "|      1| Marlania|\n",
      "|      2|   Lakken|\n",
      "|      3| Tarvares|\n",
      "|      4|Gabriella|\n",
      "|      5|   Genece|\n",
      "+-------+---------+\n",
      "\n",
      "+------+--------+\n",
      "|dep_id| min_emp|\n",
      "+------+--------+\n",
      "|     1|  Briana|\n",
      "|     2|Kimberli|\n",
      "|     3|Latoynia|\n",
      "|     4|  Medusa|\n",
      "|     5| Thurley|\n",
      "+------+--------+\n",
      "\n",
      "+-------+---------+--------+\n",
      "|dept_id|  max_emp| min_emp|\n",
      "+-------+---------+--------+\n",
      "|      1| Marlania|  Briana|\n",
      "|      2|   Lakken|Kimberli|\n",
      "|      3| Tarvares|Latoynia|\n",
      "|      4|Gabriella|  Medusa|\n",
      "|      5|   Genece| Thurley|\n",
      "+-------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1 = res.filter(res.max_emp != \"null\").drop(\"min_emp\")\n",
    "res1.show()\n",
    "\n",
    "res2 = res.filter(res.min_emp != \"null\").withColumnRenamed(\"dept_id\", \"dep_id\")\\\n",
    "        .drop(\"max_emp\")\n",
    "res2.show()\n",
    "\n",
    "final = res1.join(res2, res1.dept_id == res2.dep_id, \"inner\").drop(\"dep_id\")\n",
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "010e121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+----+------+\n",
      "|EmpName|EmpID|Department|Year|Rating|\n",
      "+-------+-----+----------+----+------+\n",
      "|  Peter|E5489| Marketing|2020|   8.0|\n",
      "|  Peter|E5489| Marketing|2019|   7.0|\n",
      "|  Peter|E5489| Marketing|2018|   6.5|\n",
      "|  Peter|E5489| Marketing|2017|   8.5|\n",
      "|  Peter|E5489| Marketing|2016|   7.0|\n",
      "| Philip|E1027|        HR|2020|   8.0|\n",
      "| Philip|E1027|        HR|2019|   8.5|\n",
      "|Shirley|E0589|   Finance|2020|   7.0|\n",
      "|Shirley|E0589|   Finance|2019|   5.0|\n",
      "|Shirley|E0589|   Finance|2018|   6.0|\n",
      "+-------+-----+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115269785868795904/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115269785868795904%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# Challenge : \n",
    "# Write a PySpark code for getting difference of average rating and second highest rating for the given dataset. \n",
    "\n",
    "Schema = StructType([\n",
    "    StructField(\"EmpName\", StringType(), True),\n",
    "    StructField(\"EmpID\", StringType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True),\n",
    "    StructField(\"Rating\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "Data = [\n",
    "    (\"Peter\", \"E5489\", \"Marketing\", 2020, 8.0),\n",
    "    (\"Peter\", \"E5489\", \"Marketing\", 2019, 7.0),\n",
    "    (\"Peter\", \"E5489\", \"Marketing\", 2018, 6.5),\n",
    "    (\"Peter\", \"E5489\", \"Marketing\", 2017, 8.5),\n",
    "    (\"Peter\", \"E5489\", \"Marketing\", 2016, 7.0),\n",
    "    (\"Philip\", \"E1027\", \"HR\", 2020, 8.0),\n",
    "    (\"Philip\", \"E1027\", \"HR\", 2019, 8.5),\n",
    "    (\"Shirley\", \"E0589\", \"Finance\", 2020, 7.0),\n",
    "    (\"Shirley\", \"E0589\", \"Finance\", 2019, 5.0),\n",
    "    (\"Shirley\", \"E0589\", \"Finance\", 2018, 6.0)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(Data, Schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beb50ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|empid|avgRating|\n",
      "+-----+---------+\n",
      "|E5489|      7.4|\n",
      "|E1027|     8.25|\n",
      "|E0589|      6.0|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average rating for each user\n",
    "avg_rating = df.groupBy(\"empid\").agg(avg(\"rating\").alias(\"avgRating\"))\n",
    "avg_rating.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30adfaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+----+------+----------+\n",
      "|EmpName|EmpID|Department|Year|Rating|dense_rank|\n",
      "+-------+-----+----------+----+------+----------+\n",
      "|Shirley|E0589|   Finance|2020|   7.0|         1|\n",
      "|Shirley|E0589|   Finance|2018|   6.0|         2|\n",
      "|Shirley|E0589|   Finance|2019|   5.0|         3|\n",
      "| Philip|E1027|        HR|2019|   8.5|         1|\n",
      "| Philip|E1027|        HR|2020|   8.0|         2|\n",
      "|  Peter|E5489| Marketing|2017|   8.5|         1|\n",
      "|  Peter|E5489| Marketing|2020|   8.0|         2|\n",
      "|  Peter|E5489| Marketing|2019|   7.0|         3|\n",
      "|  Peter|E5489| Marketing|2016|   7.0|         3|\n",
      "|  Peter|E5489| Marketing|2018|   6.5|         4|\n",
      "+-------+-----+----------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Applying Dense_rank function over Employee_ID\n",
    "\n",
    "window_spec = Window.partitionBy(\"empid\").orderBy(desc(\"rating\"))\n",
    "\n",
    "# Calculate the dense rank of the ratings for each user\n",
    "\n",
    "dense_rank = df.withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "dense_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ab5aecc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+----+------+----------+\n",
      "|EmpName|EmpID|Department|Year|Rating|dense_rank|\n",
      "+-------+-----+----------+----+------+----------+\n",
      "|Shirley|E0589|   Finance|2018|   6.0|         2|\n",
      "| Philip|E1027|        HR|2020|   8.0|         2|\n",
      "|  Peter|E5489| Marketing|2020|   8.0|         2|\n",
      "+-------+-----+----------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame to only include the second highest rating for each user\n",
    "filtered_data = dense_rank.filter(col(\"dense_rank\") == 2)\n",
    "filtered_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79e0cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+----------+----+------+----------+\n",
      "|empid|avgRating|EmpName|Department|Year|Rating|dense_rank|\n",
      "+-----+---------+-------+----------+----+------+----------+\n",
      "|E0589|      6.0|Shirley|   Finance|2020|   7.0|         1|\n",
      "|E0589|      6.0|Shirley|   Finance|2018|   6.0|         2|\n",
      "|E0589|      6.0|Shirley|   Finance|2019|   5.0|         3|\n",
      "|E1027|     8.25| Philip|        HR|2019|   8.5|         1|\n",
      "|E1027|     8.25| Philip|        HR|2020|   8.0|         2|\n",
      "|E5489|      7.4|  Peter| Marketing|2017|   8.5|         1|\n",
      "|E5489|      7.4|  Peter| Marketing|2020|   8.0|         2|\n",
      "|E5489|      7.4|  Peter| Marketing|2019|   7.0|         3|\n",
      "|E5489|      7.4|  Peter| Marketing|2016|   7.0|         3|\n",
      "|E5489|      7.4|  Peter| Marketing|2018|   6.5|         4|\n",
      "+-----+---------+-------+----------+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference between the average rating and the second highest rating for each user\n",
    "\n",
    "join_df = avg_rating.join(dense_rank, on=\"empid\", how=\"inner\")\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2555883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+\n",
      "|empid|Rating|avgRating|diff|\n",
      "+-----+------+---------+----+\n",
      "|E5489|   8.0|      7.4| 0.6|\n",
      "|E5489|   7.0|      7.4| 0.4|\n",
      "|E5489|   6.5|      7.4| 0.9|\n",
      "|E5489|   8.5|      7.4| 1.1|\n",
      "|E5489|   7.0|      7.4| 0.4|\n",
      "|E1027|   8.0|     8.25|0.25|\n",
      "|E1027|   8.5|     8.25|0.25|\n",
      "|E0589|   7.0|      6.0| 1.0|\n",
      "|E0589|   5.0|      6.0| 1.0|\n",
      "|E0589|   6.0|      6.0| 0.0|\n",
      "+-----+------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "difference = join_df.withColumn(\"diff\", round(abs(col(\"Rating\") - col(\"avgRating\")), 3))\n",
    "difference.select(\"empid\", \"Rating\", \"avgRating\", \"diff\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115266803274055680/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115266803274055680%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "# There are two files existing_data.csv and new_data.csv for that we have following pyspark code.\n",
    "\n",
    "# Load existing transaction data\n",
    "existing_data_df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(\"dbfs:/FileStore/raw/existing_data.csv\")\n",
    "\n",
    "# Load new transaction data\n",
    "new_data_df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(\"dbfs:/FileStore/raw/new_data.csv\")\n",
    "\n",
    "existing_data_df.show()\n",
    "\n",
    "# Identify new records based on timestamp\n",
    "max_timestamp_existing = existing_data_df.selectExpr(\"max(timestamp)\").collect()[0][0]\n",
    "new_records_df = new_data_df.filter(new_data_df.timestamp > max_timestamp_existing)\n",
    "\n",
    "# Append new records to existing data\n",
    "updated_data_df = existing_data_df.union(new_records_df)\n",
    "\n",
    "# Show the updated dataset\n",
    "updated_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25b8c63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|     LCPHONE|2000-01-16|\n",
      "|      2|     LCPhone|2000-01-17|\n",
      "|      3|     LcPhOnE|2000-02-18|\n",
      "|      4|  LCKeyCHAiN|2000-02-19|\n",
      "|      5|  LCKeyChain|2000-02-28|\n",
      "|      6|  Matryoshka|2000-03-31|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7113498071069540352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7113498071069540352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "schema_=[\"sale_id\",\"product_name\",\"sale_date\"]\n",
    "\n",
    "data=[\n",
    "(1,\"LCPHONE\",\"2000-01-16\"),\n",
    "(2,\"LCPhone\",\"2000-01-17\"),\n",
    "(3,\"LcPhOnE\",\"2000-02-18\"),\n",
    "(4,\"LCKeyCHAiN\",\"2000-02-19\"),\n",
    "(5,\"LCKeyChain\",\"2000-02-28\"),\n",
    "(6,\"Matryoshka\",\"2000-03-31\")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd7c4cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+\n",
      "|sale_id|product_name| sale_date|\n",
      "+-------+------------+----------+\n",
      "|      1|     lcphone|2000-01-16|\n",
      "|      2|     lcphone|2000-01-17|\n",
      "|      3|     lcphone|2000-02-18|\n",
      "|      4|  lckeychain|2000-02-19|\n",
      "|      5|  lckeychain|2000-02-28|\n",
      "|      6|  matryoshka|2000-03-31|\n",
      "+-------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lowerData = df.withColumn(\"product_name\", lower(trim(col(\"product_name\"))))\n",
    "lowerData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0f2323e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+----------+----------+\n",
      "|sale_id|product_name| sale_date|sale_month|\n",
      "+-------+------------+----------+----------+\n",
      "|      1|     lcphone|2000-01-16|   2000-01|\n",
      "|      2|     lcphone|2000-01-17|   2000-01|\n",
      "|      3|     lcphone|2000-02-18|   2000-02|\n",
      "|      4|  lckeychain|2000-02-19|   2000-02|\n",
      "|      5|  lckeychain|2000-02-28|   2000-02|\n",
      "|      6|  matryoshka|2000-03-31|   2000-03|\n",
      "+-------+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthData = lowerData.withColumn(\"sale_month\", date_format(col(\"sale_date\"), \"yyyy-MM\"))\n",
    "monthData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "646355f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----+\n",
      "|sale_month|product_name|total|\n",
      "+----------+------------+-----+\n",
      "|   2000-02|  lckeychain|    2|\n",
      "|   2000-01|     lcphone|    2|\n",
      "|   2000-02|     lcphone|    1|\n",
      "|   2000-03|  matryoshka|    1|\n",
      "+----------+------------+-----+\n",
      "\n",
      "+------------+----------+-----+\n",
      "|product_name|sale_month|total|\n",
      "+------------+----------+-----+\n",
      "|  lckeychain|   2000-02|    2|\n",
      "|     lcphone|   2000-01|    2|\n",
      "|     lcphone|   2000-02|    1|\n",
      "|  matryoshka|   2000-03|    1|\n",
      "+------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grData = monthData.groupBy(\"sale_month\", \"product_name\")\\\n",
    "        .agg(count(\"sale_month\").alias(\"total\"))\\\n",
    "        .orderBy(\"product_name\", \"sale_month\")\n",
    "grData.show()\n",
    "\n",
    "finalData = grData.select(\"product_name\", \"sale_month\", \"total\")\n",
    "finalData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77d0276e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+------+------+\n",
      "|emp_id| emp_name|salary|dep_id|\n",
      "+------+---------+------+------+\n",
      "|  1001| Marlania| 92643|     1|\n",
      "|  1002|   Briana| 87202|     1|\n",
      "|  1003|   Maysha| 70545|     1|\n",
      "|  1004|  Jamacia| 65285|     1|\n",
      "|  1005| Kimberli| 51407|     2|\n",
      "|  1006|   Lakken| 88933|     2|\n",
      "|  1007|  Micaila| 82145|     2|\n",
      "|  1008|     Gion| 66187|     2|\n",
      "|  1009| Latoynia| 55729|     3|\n",
      "|  1010| Shaquria| 52111|     3|\n",
      "|  1011| Tarvares| 82979|     3|\n",
      "|  1012|Gabriella| 74132|     4|\n",
      "|  1013|   Medusa| 72551|     4|\n",
      "|  1014|    Kubra| 55170|     4|\n",
      "+------+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115260055872032769/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115260055872032769%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "# Write a SQL query to identify ğ’•ğ’‰ğ’† ğ‘¯ğ’Šğ’ˆğ’‰ğ’†ğ’”ğ’• & ğ‘³ğ’ğ’˜ğ’†ğ’”ğ’• ğ‘ºğ’‚ğ’ğ’‚ğ’“ğ’Šğ’†ğ’… ğ‘¬ğ’ğ’‘ğ’ğ’ğ’šğ’†ğ’† ğ’Šğ’ ğ’†ğ’‚ğ’„ğ’‰ ğ‘«ğ’†ğ’‘ğ’‚ğ’“ğ’•ğ’ğ’†ğ’ğ’•.\n",
    "\n",
    "data = [(1001, 'Marlania', 92643, 1),\n",
    "(1002, 'Briana', 87202, 1),\n",
    "(1003, 'Maysha', 70545, 1),\n",
    "(1004, 'Jamacia', 65285, 1),\n",
    "(1005, 'Kimberli', 51407, 2),\n",
    "(1006, 'Lakken', 88933, 2),\n",
    "(1007, 'Micaila', 82145, 2),\n",
    "(1008, 'Gion', 66187, 2),\n",
    "(1009, 'Latoynia', 55729, 3),\n",
    "(1010, 'Shaquria', 52111, 3),\n",
    "(1011, 'Tarvares', 82979, 3),\n",
    "(1012, 'Gabriella', 74132, 4),\n",
    "(1013, 'Medusa', 72551, 4),\n",
    "(1014, 'Kubra', 55170, 4)]\n",
    "\n",
    "schema_ = [\"emp_id\", \"emp_name\" ,\"salary\", \"dep_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97b68943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|dep_id|maxSalary|minSalary|\n",
      "+------+---------+---------+\n",
      "|     1|    92643|    65285|\n",
      "|     2|    88933|    51407|\n",
      "|     3|    82979|    52111|\n",
      "|     4|    74132|    55170|\n",
      "+------+---------+---------+\n",
      "\n",
      "+------+---------+------+-------+------+---------+---------+\n",
      "|emp_id| emp_name|salary|dept_id|dep_id|maxSalary|minSalary|\n",
      "+------+---------+------+-------+------+---------+---------+\n",
      "|  1001| Marlania| 92643|      1|     1|    92643|    65285|\n",
      "|  1002|   Briana| 87202|      1|     1|    92643|    65285|\n",
      "|  1003|   Maysha| 70545|      1|     1|    92643|    65285|\n",
      "|  1004|  Jamacia| 65285|      1|     1|    92643|    65285|\n",
      "|  1005| Kimberli| 51407|      2|     2|    88933|    51407|\n",
      "|  1006|   Lakken| 88933|      2|     2|    88933|    51407|\n",
      "|  1007|  Micaila| 82145|      2|     2|    88933|    51407|\n",
      "|  1008|     Gion| 66187|      2|     2|    88933|    51407|\n",
      "|  1009| Latoynia| 55729|      3|     3|    82979|    52111|\n",
      "|  1010| Shaquria| 52111|      3|     3|    82979|    52111|\n",
      "|  1011| Tarvares| 82979|      3|     3|    82979|    52111|\n",
      "|  1012|Gabriella| 74132|      4|     4|    74132|    55170|\n",
      "|  1013|   Medusa| 72551|      4|     4|    74132|    55170|\n",
      "|  1014|    Kubra| 55170|      4|     4|    74132|    55170|\n",
      "+------+---------+------+-------+------+---------+---------+\n",
      "\n",
      "+------+---------+--------+\n",
      "|dep_id|  max_emp| min_emp|\n",
      "+------+---------+--------+\n",
      "|     1| Marlania| Jamacia|\n",
      "|     2|   Lakken|Kimberli|\n",
      "|     3| Tarvares|Shaquria|\n",
      "|     4|Gabriella|   Kubra|\n",
      "+------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_min = df.groupBy(\"dep_id\").agg(max(\"salary\").alias(\"maxSalary\"), min(\"salary\").alias(\"minSalary\"))\n",
    "max_min.show()\n",
    "\n",
    "joinedData = df.withColumnRenamed(\"dep_id\", \"dept_id\")\n",
    "joinedData = joinedData.join(max_min, joinedData.dept_id == max_min.dep_id, \"inner\") \n",
    "joinedData.show()\n",
    "\n",
    "resultDF = joinedData.groupBy(\"dep_id\").agg(\n",
    "    max(when(col(\"salary\") == col(\"maxSalary\"), col(\"emp_name\")))\\\n",
    "    .alias(\"max_emp\"),\n",
    "    min(when(col(\"salary\") == col(\"minSalary\"), col(\"emp_name\"))\\\n",
    "        .otherwise(\"null\")).alias(\"min_emp\"))\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4534a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24321468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported\n",
      "imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/29 19:56:34 WARN Utils: Your hostname, DINESHs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.100 instead (on interface en0)\n",
      "23/12/29 19:56:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/29 19:56:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported All\n"
     ]
    }
   ],
   "source": [
    "# Normal Imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import col,struct,when, lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "print(\"imported\")\n",
    "# Agg\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
    "from pyspark.sql.functions import collect_set, sum, avg, max, countDistinct, count\n",
    "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness \n",
    "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
    "from pyspark.sql.functions import variance, var_samp, var_pop\n",
    "\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.sql.types import MapType, StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "print(\"imported\")\n",
    "# spark context\n",
    "conf = SparkConf().setAppName(\"Spark\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf = conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Usage of config()\n",
    "# spark = SparkSession.builder \\\n",
    "#       .master(\"local[1]\") \\\n",
    "#       .appName(\"LearnSpark\") \\\n",
    "#       .config(\"spark.some.config.option\", \"config-value\") \\\n",
    "#       .getOrCreate()\n",
    "\n",
    "print(\"imported All\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c83685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "402b475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/posts/mahaboob-pathan_snowflake-snowsql-sql-activity-7132574458640228352-AqzH\n",
    "\n",
    "# see all the previous posta nd do in sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdabbecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|student|\n",
      "+---+-------+\n",
      "|  1|      A|\n",
      "|  2|      D|\n",
      "|  3|      E|\n",
      "|  4|      G|\n",
      "|  5|      J|\n",
      "+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# https://www.linkedin.com/posts/mahaboob-pathan_namastesql-sqlquery-sqlchallenges-activity-7113771061086744576-xfnO\n",
    "\n",
    "schema_ = [\"id\", \"student\"]\n",
    "\n",
    "data = [(1, 'A'),\n",
    "(2, 'D'),\n",
    "(3, 'E'),\n",
    "(4, 'G'),\n",
    "(5, 'J')]\n",
    "\n",
    "df = spark.createDataFrame(data, schema= schema_)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6d7c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----+\n",
      "| id|student|lead| lag|\n",
      "+---+-------+----+----+\n",
      "|  1|      A|   D|null|\n",
      "|  2|      D|   E|   A|\n",
      "|  3|      E|   G|   D|\n",
      "|  4|      G|   J|   E|\n",
      "|  5|      J|null|   G|\n",
      "+---+-------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lead_fun = Window.orderBy(asc(\"id\"))\n",
    "\n",
    "lead_lag = df.withColumn(\"lead\", lead(\"student\").over(lead_fun))\\\n",
    "        .withColumn(\"lag\", lag(\"student\").over(lead_fun))\n",
    "lead_lag.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9d9523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----+----+\n",
      "| id|student|lead| lag| res|\n",
      "+---+-------+----+----+----+\n",
      "|  1|      A|   D|null|   D|\n",
      "|  2|      D|   E|   A|   A|\n",
      "|  3|      E|   G|   D|   G|\n",
      "|  4|      G|   J|   E|   E|\n",
      "|  5|      J|null|   G|null|\n",
      "+---+-------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lead_lag.withColumn(\"res\", \n",
    "        when(col(\"id\")%2 != 0, when(col('lead') == \"null\", \n",
    "                col(\"student\")).otherwise(col(\"lead\")))\\\n",
    "                    .when(col('id') % 2 == 0, col(\"lag\"))\n",
    "                   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb0af7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| ID|STUDENT|\n",
      "+---+-------+\n",
      "|  1|      D|\n",
      "|  2|      A|\n",
      "|  3|      G|\n",
      "|  4|      E|\n",
      "|  5|      J|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"seats_tbl\")\n",
    "spark.sql(\"select \\\n",
    "            case when mod(ID,2) = 0 then ID-1 \\\n",
    "                  when mod(ID,2) <> 0 and ID <> (select Max(ID) \\\n",
    "                from seats_tbl) then ID+1 else ID end as ID, \\\n",
    "                  STUDENT from seats_tbl order by 1;\"\n",
    "         ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.linkedin.com/posts/mahaboob-pathan_namastesql-sqlquery-sqlchallenges-activity-7113004314930933760-gXk8\n",
    "\n",
    "# Write a query to get ğ’•ğ’‰ğ’† ğ’ğ’ ğ’ğ’‡ ğ’–ğ’ğ’Šğ’’ğ’–ğ’† ğ’ğ’‚ğ’ğ’†ğ’”, ğ’ğ’ ğ’ğ’‡ ğ’•ğ’“ğ’‚ğ’ğ’”ğ’‚ğ’„ğ’•ğ’Šğ’ğ’ğ’”, ğ’‚ğ’ğ’… ğ’•ğ’‰ğ’† ğ’…ğ’Šğ’‡ğ’‡ğ’†ğ’“ğ’†ğ’ğ’„ğ’† ğ’ƒğ’†ğ’•ğ’˜ğ’†ğ’†ğ’ ğ’•ğ’‰ğ’† ğ’‡ğ’Šğ’“ğ’”ğ’• & ğ’ğ’‚ğ’”ğ’• ğ’•ğ’“ğ’‚ğ’ğ’”ğ’‚ğ’„ğ’•ğ’Šğ’ğ’ ğ’ğ’„ğ’„ğ’–ğ’“ğ’†ğ’… ğ’ğ’ 02-01-2023.\n",
    "\n",
    "# https://www.linkedin.com/feed/update/urn:li:activity:7115260055872032769/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115260055872032769%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45307304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d442dc8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3209194749.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[18], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://www.linkedin.com/feed/update/urn:li:activity:7114235881678417920/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114235881678417920%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\u001b[0m\n\u001b[0m                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7114235881678417920/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114235881678417920%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56d8d78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1727679684.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[20], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://www.linkedin.com/feed/update/urn:li:activity:7114530651709554688/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114530651709554688%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\u001b[0m\n\u001b[0m                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7114530651709554688/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114530651709554688%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bcc734",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7110481458405785600/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110481458405785600%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "ğ‡ğ¨ğ° ğ’ğ©ğšğ«ğ¤ ğğğœğ¢ğğğ¬ ğ¨ğ§ ğ­ğ¡ğ ğ‰ğ¨ğ¢ğ§ ğ’ğ­ğ«ğšğ­ğğ ğ²?\n",
    "\n",
    "Spark uses a cost-based optimizer to choose the most efficient join strategy based on the size of the datasets, the type of join, the join condition, the distribution of the data, and the availability of resources.\n",
    "\n",
    "The three most common join strategies in Spark are:\n",
    "1. Broadcast join\n",
    "2. Shuffle hash join\n",
    "3. Sort merge join\n",
    "\n",
    "Read my blog to learn more about how Apache Spark decides on the join strategy and how to use join hints effectively: https://lnkd.in/g3EvZZfC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a920d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7110903214325059585/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7110903214325059585%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c6d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7114818781520048128/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7114818781520048128%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d353a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resources to Prepare:\n",
    "1. DataLemur (Really good questions from product based companies but edge cases not covered). [https://datalemur.com/]\n",
    "2. LeetCode (the classic). [https://leetcode.com/]\n",
    "3. 8 week SQL challenge. [https://lnkd.in/g9KxBFem]\n",
    "4. This cult classic problem:\n",
    "Table A: 1,1,2,Null,3,5,Null\n",
    "Table B: 1,2,2,Null,4,5,Null,6\n",
    "What will be the result of Inner, Left, Right, Full, Cross Join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77797bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115546144084037633/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115546144084037633%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115266803274055680/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115266803274055680%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115588607486156801/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115588607486156801%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d647270",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115644296111783936/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115644296111783936%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115697833059491840/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115697833059491840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2144c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115373323517145088/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115373323517145088%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f6c039",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115546153542172672/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115546153542172672%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a14000",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116437104812851200/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116437104812851200%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5706a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116106222910550016/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116106222910550016%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a6d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116006679380443136/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116006679380443136%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3b321",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7115897323930460160/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7115897323930460160%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f864e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117297691230957568/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117297691230957568%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab15591",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117038244063510528/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117038244063510528%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116973047328206848/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116973047328206848%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ed1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116830873060069376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116830873060069376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b18cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116565359666610176/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116565359666610176%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e186ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116608449236353024/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116608449236353024%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d148104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116565359335268352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116565359335268352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "https://github.com/imadhaka/PyLeetProblems/blob/master/spark/PairedRows.py\n",
    "    \n",
    "https://github.com/imadhaka/PyLeetProblems/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116437104812851200/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116437104812851200%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c30311",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7116445887807074304/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7116445887807074304%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652fb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117350533186695168/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117350533186695168%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6b0803",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117377810200952832/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117377810200952832%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f0f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117103379687821313/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117103379687821313%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117667622749421568/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117667622749421568%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1dd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118105404118593538/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118105404118593538%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118037578146070528/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118037578146070528%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118305823767883776/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118305823767883776%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7117841840699052032/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7117841840699052032%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118075350764445697/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118075350764445697%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7111bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118772247946797056/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118772247946797056%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118472512916643840/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118472512916643840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8efd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118426384892379136/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118426384892379136%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac7afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118905100168777729/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118905100168777729%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ae6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7119913279933603840/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119913279933603840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82af6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7119548123739226112/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119548123739226112%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac31c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7119597076719509504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119597076719509504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7119912119998840832/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119912119998840832%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa4a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7119585361919418368/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7119585361919418368%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a48aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120995100997939200/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120995100997939200%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "ğŸ“ŒQuestion: Given one list of events like below example.\n",
    "list1 = ['user1 click 06:04:00', 'user2 log ERROR', 'user1 click 04:09:08', 'user1 click 00:00:01']\n",
    "\n",
    "ğŸ¯show me the distinct number of users.\n",
    "ğŸ¯show me the how many number of events.\n",
    "ğŸ¯show me the which users having most events.\n",
    "\n",
    "ğŸ€For the above tasks, we can easly write the code in normal programming, but we need to write in the RDD form.                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120651975347597312/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120651975347597312%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920eb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120618783588687872/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120618783588687872%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef18f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120110212866306048/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120110212866306048%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fd2680",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120256705694736385/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120256705694736385%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344cf786",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7121112905302863873/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121112905302863873%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118496721755648000/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118496721755648000%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7121905553978593280/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121905553978593280%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122282162531241984/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122282162531241984%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dc331",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122199508280606720/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122199508280606720%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74479f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122128627739086848/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122128627739086848%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122001177667395585/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122001177667395585%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "https://github.com/imadhaka/PyLeetProblems/blob/master/spark/StudentAttendance.py                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7121832036981501952/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121832036981501952%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/4602256280705183/69616450320427/8093088653480119/latest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120677975829753856/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120677975829753856%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2332a34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7121343994546053120/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121343994546053120%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082e0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120678141882179585/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120678141882179585%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122827934645563393/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122827934645563393%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8aefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122634055266803712/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122634055266803712%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "do all the questins in this.                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263216d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/mattmartin14/dream_machine/tree/main/deltalake/docker/dl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2392133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7122938682243772416/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7122938682243772416%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7121361196137484288/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7121361196137484288%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7205b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123234638797373440/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123234638797373440%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7175ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123345666528935936/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123345666528935936%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66645fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123353474204852224/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123353474204852224%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "Sample data:\n",
    "product_id,product_name,quantity_sold,order_date\n",
    "1,Product A,50,2023-07-01\n",
    "2,Product B,75,2023-07-05\n",
    "3,Product C,30,2023-07-12\n",
    "4,Product A,45,2023-07-15\n",
    "5,Product D,60,2023-07-20\n",
    "6,Product B,70,2023-07-25\n",
    "7,Product C,35,2023-07-29\n",
    "8,Product D,55,2023-07-30\n",
    "9,Product E,40,2023-07-01\n",
    "10,Product F,65,2023-07-10\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ab6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120956804745826304/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120956804745826304%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123284641020579843/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123284641020579843%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123527963072970752/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123527963072970752%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598af201",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123522447164776448/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123522447164776448%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7124085577792073728/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124085577792073728%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00bb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123558824749236224/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123558824749236224%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff574a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7124256858676035584/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124256858676035584%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7123896977603387392/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7123896977603387392%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a20bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7124450661517131776/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124450661517131776%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ea0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7124446283817201664/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7124446283817201664%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cda393",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7125009569696673794/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125009569696673794%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68ce2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7125014408900083713/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125014408900083713%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "watch all the questions                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5049e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7125185637636333568/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125185637636333568%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29edbd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7126894271420387328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7126894271420387328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f92880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127097269199245312/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127097269199245312%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5f27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7126623558360780800/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7126623558360780800%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3397b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7126445296313593856/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7126445296313593856%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5632803",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7126262755782799360/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7126262755782799360%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abfa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7125706674941607936/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125706674941607936%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7125538528997892096/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7125538528997892096%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e298ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127574447221260288/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127574447221260288%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077a2ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127284282821070848/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127284282821070848%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821dde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127518232948654080/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127518232948654080%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50997d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127187913859104768/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127187913859104768%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca459ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127270902743896064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127270902743896064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79412c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127500819242876928/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127500819242876928%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28943d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127343624580739072/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127343624580739072%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e382ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127648844800008193/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127648844800008193%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5264308",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127242879789330432/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127242879789330432%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53e5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127575459453648896/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127575459453648896%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefad102",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127656206487142400/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127656206487142400%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7127867344248291328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7127867344248291328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874ac90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7128353648228675584/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7128353648228675584%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a07a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7128290390381592576/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7128290390381592576%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7128161770971496448/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7128161770971496448%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6059551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130131940963143681/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130131940963143681%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Okay! As a Data Engineer, how often would you have written the below piece of code?\n",
    "\n",
    "Spark = SparkSession.builder\n",
    ".master()\n",
    ".appName()\n",
    ".config()\n",
    ".getOrCreate()\n",
    "\n",
    "I would say you have used it more often. But ever wondered why we write this?\n",
    "\n",
    "âœ… A SparkSession is the entry point to programming Spark with the DataFrame and SQL API. It is the unified interface that combines the functionalities of SQLContext, HiveContext, and StreamingContext\n",
    "\n",
    "âœ… builder is an attribute within SparkSession class which initiates Builder class to construct :class:`SparkSession` instances. The Builder class contains methods that allow you to set various configuration options for the SparkSession before creating it.\n",
    "\n",
    "Below are some of the methods within Builder class,\n",
    "\n",
    "â master(String master) :- Used to specify the cluster manager URL, determining where the Spark application will run. You can set this value to connect to a standalone cluster, YARN, Mesos, or Kubernetes, depending on your infrastructure\n",
    "\n",
    "â appName(String name) :- Sets a name for the application, which will be shown in the Spark web UI. If no application name is set, a randomly generated name will be used.\n",
    "\n",
    "â config(SparkConf conf) :- Used to set configuration options for your Spark application. It allows you to specify various Spark properties and control the behavior of your Spark job.\n",
    "\n",
    "â getOrCreate() :- Used to either retrieve an existing SparkSession with the specified configuration or create a new one if it doesn't exist.\n",
    "\n",
    "Edited:- GetorCreate ensure instance of Spark session remains one per spark application. In Apache Spark, it is possible to create and use multiple Spark sessions within a single application. Multiple Spark sessions within the same application are typically used in scenarios where you want to isolate different sets of Spark functionality or configurations.\n",
    "\n",
    "â— Note:- While it's possible to have multiple Spark sessions in a single application, doing so comes with certain trade-offs and potential drawbacks. Each Spark session consumes resources, including memory and CPU. Creating multiple sessions within the same application can lead to increased resource overhead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721903cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7129973716570017792/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7129973716570017792%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf18d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7129811895481475072/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7129811895481475072%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f73b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130188087300636672/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130188087300636672%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130094886023761920/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130094886023761920%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69c690",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130590802245779458/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130590802245779458%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f86dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130053191857045504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130053191857045504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "1) How do You execute your Spark Job ?\n",
    "Spark-submit\n",
    "It has many parameters such as -\n",
    ">>spark-submit \\\n",
    "- - master yarn \\       resource manager is Yarn\n",
    "- - deploy-mode cluster \\     deploying the spark application in cluster mode\n",
    "- - driver-memory 8g \\         memory to be used by driver jvm\n",
    "- - driver-cores 1 \\    cores to be used by driver jvm\n",
    "- - num-executors 5\\ The total number of executors to be used\n",
    "- - executor-memory 16g \\  Amount of memory to be used ach executors\n",
    "- - executor-cores 2 \\ Number of cores to be used by each executors\n",
    "- - conf \"spark.sql.shuffle.partitions=20000\" \\       \n",
    "- - conf \"spark.executor.memoryOverhead=5244\" \\\n",
    "- - conf \"spark.memory.fraction=0.8\" \\\n",
    "- - conf \"spark.memory.storageFraction=0.2\" \\\n",
    "- - conf \"spark.serializer=org.apache.spark.serializer.KryoSerializer\" \\\n",
    "- - conf \"spark.sql.files.maxPartitionBytes=168435456\" \\\n",
    "- - conf \"spark.dynamicAllocation.minExecutors=1\" \\\n",
    "- - conf \"spark.dynamicAllocation.maxExecutors=200\" \\\n",
    "- - conf \"spark.dynamicAllocation.enabled=true\" \\\n",
    "- - conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" \\ \n",
    "- - jars dependency1.jar, dependency2.jar \\\n",
    "- - class \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc05d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130220537133301760/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130220537133301760%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62538326",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130826889874604034/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130826889874604034%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0b2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131430811504963585/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131430811504963585%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d25f47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131150748666523648/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131150748666523648%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9563c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esrever ot ecnetnes elpmas a si sihT\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"This is a sample sentence to reverse\"\n",
    "print(input_sentence[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95f9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130904493835120640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130904493835120640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7107f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131849051863400448/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131849051863400448%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131286673409114112/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131286673409114112%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131589365658517504/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131589365658517504%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be86ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131308221222293505/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131308221222293505%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927ee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132352789942255617/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132352789942255617%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98aa7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132377874984022016/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132377874984022016%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132233433853886465/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132233433853886465%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972206b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131250476611768321/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131250476611768321%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88923e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132210496119734273/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132210496119734273%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d627d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131800751231696897/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131800751231696897%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-spark-activity-7131308221222293505-Kt1i/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131677453747429376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131677453747429376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133439078947037184/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133439078947037184%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133300632303587328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133300632303587328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133650440394616832/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133650440394616832%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf2ada4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133038914390392835/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133038914390392835%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13328f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132887915352784896/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132887915352784896%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132784170765885440/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132784170765885440%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cec99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7132872813538910208/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7132872813538910208%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133612690916184064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133612690916184064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec83729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133257864789327873/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133257864789327873%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133272955505033218/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133272955505033218%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-spark-activity-7132377874984022016-m2kx/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d99125",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133125938602491904/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133125938602491904%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135466730939682816/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135466730939682816%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "ğ’ğ¡ğšğ«ğğ ğ•ğšğ«ğ¢ğšğ›ğ¥ğğ¬ ğ¢ğ§ ğ’ğ©ğšğ«ğ¤\n",
    "=================================\n",
    "There are 2 type of shared variables\n",
    "i)Broadcast variable\n",
    "ii) Accumulator variable\n",
    "\n",
    "ğğ«ğ¨ğšğğœğšğ¬ğ­ ğ•ğšğ«ğ¢ğšğ›ğ¥ğ :\n",
    "-> Broadcast variables are read-only, distributed data structures used to efficiently share data across worker nodes in a Spark cluster\n",
    "-> Broadcast Variables are used to cache a value or object on each node in the cluster, which can then be used across multiple tasks.\n",
    "-> They are loaded onto every worker node so that they can access them.\n",
    "\n",
    " ğ”ğ¬ğ ğœğšğ¬ğ : You can use broadcast to distribute lookup tables or reference data to each worker node, used in a join operation or filter operation.\n",
    "Ex : val joinedDf=largeDf.join(broadcast(smallDf))\n",
    "=======================================\n",
    "ğ€ğœğœğ®ğ¦ğ®ğ¥ğšğ­ğ¨ğ« :\n",
    "->Accumulator variable is a shared copy kept in driver node and all the executors will have access to it.\n",
    "->Executors update the variable in Driver node.\n",
    "->Executors only have an access to update the variable they do not have access to read the variable.\n",
    "\n",
    "ğ”ğ¬ğ ğœğšğ¬ğ :\n",
    "you can use an accumulator to keep track of the count of a particular event, sum of values, or any other kind of accumulation that you need to do across all the worker nodes\n",
    "Ex : val accum = sc.longAccumulator(\"Sum Accumulator\")\n",
    "val rdd=sc.parallelize(Array(1, 2, 3)).foreach(x => accum.add(x))\n",
    "println(accum.value)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130236033287274497/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130236033287274497%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40e59aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134163826815418368/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134163826815418368%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133831650236411904/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133831650236411904%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f34d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_databricks-pyspark-spark-activity-7134594743274995712-4LwC/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab284f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134960321202446336/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134960321202446336%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134964193274843139/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134964193274843139%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134766136801263617/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134766136801263617%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a211764",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7134424091536572416/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7134424091536572416%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b3d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133878542810951680/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133878542810951680%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5da2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136989323773493248/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136989323773493248%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7133310348861861888/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7133310348861861888%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135618539335389185/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135618539335389185%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7564f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135657443992248321/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135657443992248321%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1136b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136950876237144064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136950876237144064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4c0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136722175230689280/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136722175230689280%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "project: https://github.com/Pavanpawar2705/Ingesting-Real-time-Logistics-Data-in-MongoDB-with-Kafka-and-Python                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73883921",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136421513045434368/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136421513045434368%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7137026016887140352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7137026016887140352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d236c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135663369507803136/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135663369507803136%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2411f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7138387159870296064/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7138387159870296064%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7135475274451664896/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7135475274451664896%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2733eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7137478380425285632/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7137478380425285632%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea458e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7138499356214665218/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7138499356214665218%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "https://www.montecarlodata.com/blog-pyspark-data-quality-checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7138583044248035328/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7138583044248035328%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55674be",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7141326742601863168/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7141326742601863168%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "Deloitte Data Engineer Interview question and Round :\n",
    "soultuion:\n",
    "Q:- How would you rename 100 column in pyspark?\n",
    "Ans:-\n",
    "from functools import reduce\n",
    "def rename_cols(df, old_columns, new_columns):\n",
    "  for old_col, new_col in zip(old_columns, new_columns):\n",
    "    df = df.withColumnRenamed(old_col, new_col)\n",
    "  return df\n",
    "\n",
    "old_columns = ['old_name1', 'old_name2', ..., 'old_name100']\n",
    "new_columns = ['new_name1', 'new_name2', ..., 'new_name100']\n",
    "\n",
    "df_renamed = rename_cols(df, old_columns, new_columns)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee759415",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140921123524837376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140921123524837376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "ğŸ›‘ Features of PySpark :\n",
    "===================\n",
    "\n",
    "1. In memory computation\n",
    "2. Distributed processing using parallelism\n",
    "3. Fault - tolerance\n",
    "4. Immutable\n",
    "5. Lazy Evaluation\n",
    "6. Cache & Persistence\n",
    "7. Inbuild optimization using DataFrame\n",
    "\n",
    "ğŸ›‘ Advantage of PySpark\n",
    "==================\n",
    "\n",
    "1. It is a general purpose in-memory distributed processing engine that allows to process data efficiently in a distributed manner.\n",
    "\n",
    "2. Application running on spark is 100x faster than traditional one (i.e Hadoop ecosystem)\n",
    "\n",
    "3. We can process data from external data resource like HDFS, AWS, S3 & many more file system.\n",
    "\n",
    "4. It has Mlib & graph libraries as well for processing.\n",
    "\n",
    "5. By spark we can process real-time data through streaming & Kafka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcbfdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140407880188428288/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140407880188428288%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "Data Engineering Interview Question:\n",
    "===========================\n",
    "\n",
    "Spark map() vs mapPartitions():\n",
    "========================\n",
    "\n",
    "map() â€“\n",
    "\n",
    "Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.\n",
    "\n",
    "\n",
    "mapPartitions() â€“\n",
    "\n",
    "This is exactly the same as map(); the difference being, Spark mapPartitions() provides a facility to do heavy initializations (for example Database connection) once for each partition instead of doing it on every DataFrame row. This helps the performance of the job when you dealing with heavy-weighted initialization on larger datasets.\n",
    "\n",
    "map vs flatmap :\n",
    "=============\n",
    "\n",
    "The key difference between map and flatMap is map returns only one row/element for every input, while flatMap() can return a list of rows/elements.                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10a0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140257445376937984/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140257445376937984%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "\n",
    "\n",
    "Data Engineering Interview question:\n",
    "===========================\n",
    "\n",
    "distinct() vs dropDuplicates() :\n",
    "\n",
    "\n",
    "distinct() and dropDuplicates() in Spark are used to remove duplicate rows, but there is a subtle difference. distinct() considers all columns when identifying duplicates, while dropDuplicates() allowing you to specify a subset of columns to determine uniqueness.\n",
    "\n",
    "Please comment on which one is better if we consider we need to remove duplicate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676b9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7140395946852429824/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7140395946852429824%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7141653171134787584/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7141653171134787584%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143547646807355393/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143547646807355393%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7142294899235696640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7142294899235696640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1d58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143112135022133249/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143112135022133249%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87baa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143210451340804097/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143210451340804097%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981ba493",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143268521010008067/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143268521010008067%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd12411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144726008561041408/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144726008561041408%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cdddba",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143508594397724672/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143508594397724672%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8f3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144663579512791040/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144663579512791040%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc5a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143656501138673667/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143656501138673667%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc812953",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144167235375280128/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144167235375280128%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b480e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7142271906786725888/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7142271906786725888%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144922987287240704/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144922987287240704%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b83367",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7143818058900324352/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7143818058900324352%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "\n",
    "see the pdf and see                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415b048",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7145061670602678272/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7145061670602678272%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "This is the best explanation to differentiateğŸ¤¯.. Letâ€™s learnâ€¦\n",
    "\n",
    "1ï¸âƒ£ ğ——ğ—®ğ˜ğ—® ğ—ªğ—®ğ—¿ğ—²ğ—µğ—¼ğ˜‚ğ˜€ğ—²: A data warehouse is a centralized repository of data that is used for analysis and reporting. It is typically used to store historical data, but can also store real-time data. Data warehouses are typically structured, meaning that the data is organized in a way that makes it easy to query and analyze.ğŸ¢\n",
    "\n",
    "2ï¸âƒ£ ğ——ğ—®ğ˜ğ—® ğ— ğ—®ğ—¿ğ˜: A data mart is a subset of a data warehouse. It's designed to cater to the needs of a specific business unit or team. Think of it as a department store within the larger shopping mall (the data warehouse).ğŸª\n",
    "\n",
    "3ï¸âƒ£ ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—²: A data lake is a repository for all of an organization's data, both structured and unstructured. It is a more flexible and scalable solution than a data warehouse, as it can store any type of data, regardless of its structure. Data lakes are typically used for big data analytics, as they can store and process large amounts of data quickly and efficiently..ğŸŒŠ\n",
    "\n",
    "4ï¸âƒ£ ğ——ğ—²ğ—¹ğ˜ğ—® ğ—Ÿğ—®ğ—¸ğ—²: Delta Lake is a data lake storage layer that provides a unified view of data across multiple data lakes. It is a relatively new technology, but it has quickly become popular due to its ability to provide a consistent view of data in a data lake.\n",
    "\n",
    "â€¢ ACID transactions\n",
    "â€¢ Data versioning\n",
    "â€¢ Automatic compactionğŸ› ï¸\n",
    "\n",
    "5ï¸âƒ£ ğ——ğ—®ğ˜ğ—® ğ—£ğ—¶ğ—½ğ—²ğ—¹ğ—¶ğ—»ğ—²: A data pipeline is the process of moving data from one location to another, often involving steps for extracting, transforming, and loading data (ETL). It's like the conveyer belt in a factory, ensuring that the data gets to where it's needed in a usable form.ğŸš€\n",
    "\n",
    "6ï¸âƒ£ ğ——ğ—®ğ˜ğ—® ğ— ğ—²ğ˜€ğ—µ: Data Mesh is an architectural paradigm that treats data as a product. It decentralizes data ownership and architecture, allowing teams to develop, govern, and operate their own data domains. It's a shift from monolithic, centralized data management to a more distributed approach.ğŸ•¸ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dddc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7144217757201358849/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7144217757201358849%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146341572584816640/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146341572584816640%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3469c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146286978206949376/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146286978206949376%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146478784010006529/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146478784010006529%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c42c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc27a301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dde7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f15dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9adbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920d60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f883046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b643f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13232141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00af250",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-databricks-activity-7116845920289234944-9Zvn/?utm_source=share&utm_medium=member_desktop\n",
    "    \n",
    "links to previous questions    \n",
    "\n",
    "do all the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7118563608191533056/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7118563608191533056%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "do in last        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85edf680",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7120828695124074497/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7120828695124074497%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e19c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/posts/priyam-jain-0946ab199_pyspark-pyspark-spark-activity-7126262755782799360-zsjS/?utm_source=share&utm_medium=member_desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a48c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "40\n",
    "\n",
    "https://www.linkedin.com/feed/update/urn:li:activity:7131308221222293505/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131308221222293505%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc69a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7131113063897800705/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7131113063897800705%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8739af",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7130992927283052544/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7130992927283052544%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6970411",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7136270124465999872/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7136270124465999872%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "try all this                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8641cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project\n",
    "\n",
    "https://github.com/MrSachinGoyal/gcp-dataproc-pyspark-analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddceb49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7145633755364229120/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7145633755364229120%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa8956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.linkedin.com/feed/update/urn:li:activity:7146764370142724097/?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7146764370142724097%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29\n",
    "                \n",
    "5 SQL projects to build your strong Analytics portfolio ğŸš€\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d63df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
